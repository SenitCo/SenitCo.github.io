<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DL," />





  <link rel="alternate" href="/atom.xml" title="Senit_Co" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="&amp;emsp;&amp;emsp;Normalization可理解为归一化、标准化或者规范化，广泛应用于诸多领域。整体来讲，Normalization扮演着对数据分布重新调整的角色。在图像处理领域，不同形式的归一化可以改变图像的灰度、对比度信息；在机器学习和神经网络中，Normalization可用于对数据去相关，加速模型训练，提高模型的泛化能力。">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning -- Normalization">
<meta property="og:url" content="https://senitco.github.io/2017/09/12/deep-learning-normalization/index.html">
<meta property="og:site_name" content="Senit_Co">
<meta property="og:description" content="&amp;emsp;&amp;emsp;Normalization可理解为归一化、标准化或者规范化，广泛应用于诸多领域。整体来讲，Normalization扮演着对数据分布重新调整的角色。在图像处理领域，不同形式的归一化可以改变图像的灰度、对比度信息；在机器学习和神经网络中，Normalization可用于对数据去相关，加速模型训练，提高模型的泛化能力。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://i.loli.net/2017/09/14/59ba2ec2b1f8d.png">
<meta property="og:image" content="https://i.loli.net/2017/09/14/59ba31a97d72d.jpg">
<meta property="og:image" content="https://i.loli.net/2017/09/14/59ba31ae6663c.jpg">
<meta property="og:image" content="https://i.loli.net/2017/09/14/59ba46a4b1b6c.jpg">
<meta property="og:image" content="https://i.loli.net/2017/09/16/59bce0018d695.jpg">
<meta property="og:image" content="https://i.loli.net/2017/09/16/59bce41fc966a.jpg">
<meta property="og:image" content="https://i.loli.net/2017/09/16/59bce867e253a.jpg">
<meta property="og:updated_time" content="2017-09-18T04:15:45.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning -- Normalization">
<meta name="twitter:description" content="&amp;emsp;&amp;emsp;Normalization可理解为归一化、标准化或者规范化，广泛应用于诸多领域。整体来讲，Normalization扮演着对数据分布重新调整的角色。在图像处理领域，不同形式的归一化可以改变图像的灰度、对比度信息；在机器学习和神经网络中，Normalization可用于对数据去相关，加速模型训练，提高模型的泛化能力。">
<meta name="twitter:image" content="https://i.loli.net/2017/09/14/59ba2ec2b1f8d.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 6392863219974669000,
      author: 'Author'
    }
  };
</script>

  <title> Deep Learning -- Normalization | Senit_Co </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Senit_Co</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Deep Learning -- Normalization
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-09-12T00:00:00+08:00" content="2017-09-12">
              2017-09-12
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Algorithm/" itemprop="url" rel="index">
                    <span itemprop="name">Algorithm</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/09/12/deep-learning-normalization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/09/12/deep-learning-normalization/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
              &nbsp; | &nbsp;
              <span class="page-pv"><i class="fa fa-file-o"></i>
              <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
              </span>
          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>&emsp;&emsp;Normalization可理解为归一化、标准化或者规范化，广泛应用于诸多领域。整体来讲，Normalization扮演着对数据分布重新调整的角色。在图像处理领域，不同形式的归一化可以改变图像的灰度、对比度信息；在机器学习和神经网络中，Normalization可用于对数据去相关，加速模型训练，提高模型的泛化能力。<br><a id="more"></a></p>
<h3 id="Normalization-min-max"><a href="#Normalization-min-max" class="headerlink" title="Normalization(min-max)"></a>Normalization(min-max)</h3><p>&emsp;&emsp;通常意义上的归一化，也是使用较为频繁的一种是对数据按比例缩放，使之分布在一个特定的区间。例如将数据映射到[0, 1]区间，这在数据挖掘和机器学习中较为常见，在原始数据集中，不同维的数据（特征）往往具有不同的量纲或范围，会导致不同特征呈现不同的重要性，为了消除这种影响，将不同维的特征归一化到特定区间，使所有指标处于同一数量级，将归一化的数据应用于后续的算法模型，也会更加高效便捷。归一化的数学表达式如下：<br>$$x = \dfrac{x-x_{min}}{x_{max}-x_{min}}$$<br>如果是映射到特定区间[a,b]，公式如下：<br>$$x = \dfrac{x-x_{min}}{x_{max}-x_{min}} (b-a) + a$$<br>在图像处理中，经常会将像素灰度值映射到区间[0,255]。此外，还有直方图均衡化(Histogram Equalization)、按特定幂函数对图像进行对比度拉伸(Contrast Stretching)等图像预处理操作。</p>
<h3 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h3><p>&emsp;&emsp;z-score标准化方法使数据符合均值为0、标准差为1的分布，数学变换式如下：<br>$$\widehat x = \dfrac{x - \mu}{\sigma}$$<br>式中，$\mu、\sigma$分别为原始数据的均值和标准差。这种标准化方法在一定程度上可以降低数据之间的相关性，并加速训练过程。就图像和视觉领域而言，通常只需要减去均值即可。对于所有图像样本，对相同坐标的像素求取均值，这样便可得到一个均值模板图像，然后让所有图像减去这个均值模板，这是一种比较通用的处理方法；还有一种是对所有图像的所有像素在RGB三个通道求取均值，然后让所有像素的三通道值分别减去对应通道的均值，这种方法有在Faster R-CNN等系列目标检测领域中用到。</p>
<h3 id="ZCA-Whitening"><a href="#ZCA-Whitening" class="headerlink" title="ZCA Whitening"></a>ZCA Whitening</h3><p>&emsp;&emsp;白化(Whitening)的目的是降低数据之间的相关性，对于图像数据而言，相邻像素之间具有很强的相关性，也就是说数据是冗余的。通过白化过程，希望数据具备如下两点性质：</p>
<ul>
<li>特征之间的相关性降低</li>
<li>所有特征具有相同的方差</li>
</ul>
<p>&emsp;&emsp;主成分分析(PCA)一般用于数据降维，其思路是计算数据样本集$X$协方差矩阵的特征值和特征向量，通过选取前k个特征值对应的特征向量组成投影矩阵，将原始数据映射到新的特征空间，这样便去除了数据之间的相关性（具体内容可参考<a href="https://senitco.github.io/2017/05/10/data-dimensionality-reduction/">博文</a>）。白化的第一步操作就是PCA，如下图所示，假设由特征向量组成的变换矩阵为$U$（既可以保留所有特征向量，也可以只取前k个），那么变换后的数据为<br>$$X_{PCA}=U^T X$$</p>
<p><img src="https://i.loli.net/2017/09/14/59ba2ec2b1f8d.png" alt="pca.png"></p>
<p>第二步PCA白化对变换后数据的每一维特征做标准差归一化处理，使得不同特征的方差相同（单位方差），变换式如下：<br>$$X_{PCAwhite} = \dfrac{X_{PCA}}{std(X_{PCA})}$$<br>也可直接采用下式：<br>$$X_{PCAwhite} = \dfrac{X_{PCA}}{\sqrt{\lambda_i + \varepsilon}}$$<br>$\lambda_i$是PCA得到的特征值，$\varepsilon$是正则化项，避免除数为0，一般取值为$\varepsilon \approx 10^{-5}$。PCA白化后数据分布如下图所示：</p>
<p><img src="https://i.loli.net/2017/09/14/59ba31a97d72d.jpg" alt="pca whitening.jpg" title="PCA Whitening"></p>
<p>第三步ZCA白化是在PCA白化的基础上，将数据变换回原来的特征空间，使得白化后的数据尽可能地接近原始数据。<br>$$X_{ZCAwhite} = U X_{PCAwhite}$$</p>
<p><img src="https://i.loli.net/2017/09/14/59ba31ae6663c.jpg" alt="zca whitening.jpg" title="ZCA Whitening"></p>
<p>不同于 PCA白化，当使用ZCA白化时，通常保留数据的全部n个维度，不尝试去降低它的维数。关于图像的协方差矩阵和ZCA白化可参考<a href="http://218.199.87.242/cache/12/03/www.cs.toronto.edu/cb468cfcbdb22c46e3e49e2ba3c72198/learning-features-2009-TR.pdf" target="_blank" rel="external">Learning Multiple Layers of Features from Tiny Images</a></p>
<h3 id="Global-Contrast-Normalization-GCN"><a href="#Global-Contrast-Normalization-GCN" class="headerlink" title="Global Contrast Normalization(GCN)"></a>Global Contrast Normalization(GCN)</h3><p>&emsp;&emsp;全局对比度标准化(GCN)就是计算图像中所有像素的均值和标准差，然后每个像素分别减去权值并除以标准差，GCN和ZCA Whitening通常一起使用。</p>
<h3 id="Local-Contrast-Normalization-LCN"><a href="#Local-Contrast-Normalization-LCN" class="headerlink" title="Local Contrast Normalization(LCN)"></a>Local Contrast Normalization(LCN)</h3><p>&emsp;&emsp;局部对比度标准化(LCN)是通过计算图像中局部邻域的均值和标准差来进行标准化，可参考论文<a href="http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf" target="_blank" rel="external">What is the Best Multi-Stage Architecture for Object Recognition</a>，具体流程如下：</p>
<ul>
<li>对图像中的像素，考虑一个$3 \times 3$的邻域</li>
<li>计算邻域窗口内所有像素的均值(mean)和标准差(std)，并让中心像素减去均值</li>
<li>比较标准差与1的大小，如果大于1，则中心像素值（减去均值后）除以标准差作为标准化后的最终值；否则，直接将减去均值后的中心像素值作为最终值</li>
<li>遍历图像中的所有像素，按照上述流程分别进行局部标准化</li>
</ul>
<p>在实际处理过程中，可以考虑不同窗口大小的邻域，也可利用加权的方式求取邻域像素的均值。如果在卷积神经网络中，存在多个特征图(feature map)的情况，可以同时考虑空间邻域和特征邻域，例如相邻两个特征图的$3 \times 3$邻域，则每个像素有$3 \times 3 \times 3 -1 = 26$个邻域像素点。LCN可以增强某些特征图的特征表达能力，而限制另外一些特征图的特征表达(?)。</p>
<h3 id="Local-Response-Normalization-LRN"><a href="#Local-Response-Normalization-LRN" class="headerlink" title="Local Response Normalization(LRN)"></a>Local Response Normalization(LRN)</h3><p>&emsp;&emsp;局部响应标准化(LRN)这一概念是在<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">AlexNet</a>中提出的，受到LCN的启发，通过考虑相邻特征图进行标准化，数学表达式如下：<br>$$b_{x,y}^i = a_{x,y}^i / (k + \alpha \Sigma_{j=max(0,i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2)^\beta$$<br>$a_{x,y}^i$是第$i$个卷积核在特征图中坐标$(x,y)$处的激活输出，$b_{x,y}^i$为对应的LRN输出值，$N$是卷积核或者特征图的数量，$n$是考虑的邻域特征图个数，论文中取值为5，$k、\alpha、\beta$是超参数，分别取值为$k=2,\alpha=10^{-4},\beta=0,75$。LRN的流程图如下：</p>
<p><img src="https://i.loli.net/2017/09/14/59ba46a4b1b6c.jpg" alt="local_response_normalization_process.jpg" title="LRN流程图"></p>
<p>为了使LRN输入输出的特征图数量一致，需要对特征图进行扩充(padding)，两端分别增加$n/2$个特征图，可直接拷贝相邻特征图。后续有研究表明，LRN在实际模型中发挥的作用不大，因而使用逐渐变少。</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>&emsp;&emsp;在训练神经网络模型时，由于网络参数在不断更新，每一层的输入分布也在不断变化，这迫使网络设置更小的学习率，导致更慢的训练速度，而且对参数初始化的要求也较高，这种现象称为Internal Convariate Shift。而Batch Normalization通过对每层的输入进行归一化，可以有效地缓解这个问题，网络可以使用较大的学习率，参数初值也无需刻意设置，并且可以起到正则化(Regularization)的作用，减弱了对Dropout的需求，保证了更快的训练速度和更精确的网络模型。</p>
<h4 id="Why-need-BN"><a href="#Why-need-BN" class="headerlink" title="Why need BN"></a>Why need BN</h4><p>&emsp;&emsp;在神经网络中，最基本的参数优化方法就是随机梯度下降(SGD)，通过最小化代价函数来求得一组模型参数<br>$$\theta = \underset{\theta}{\mathrm{arg min}} \dfrac{1}{N} \sum_{i=1}^N{\mathcal L\left(x_i, \theta\right)}$$<br>在实际训练过程中，通常采用minibatch的方法，即每次迭代训练一批数据，梯度计算如下：<br>$$\dfrac{1}{m}\sum \dfrac{\partial{\mathcal L \left(x_i, \theta\right)}}{\partial \theta}$$<br>minibatch可以近似整个训练集的梯度，并且可以并行地计算m个样本，比单独计算每个样本要快。但SGD仍然存在一些固有缺陷：</p>
<ul>
<li>学习率比较难设置，为了尽量避免梯度弥散(Gradiant Vanish)，只能设置较小的学习率，导致训练缓慢</li>
<li>对权值参数的初始值要求较高，否则会造成收敛困难</li>
<li>每层的输入受到前面参数的影响，前面很小的变动，随着网络层数增加会不断被放大</li>
</ul>
<p>&emsp;&emsp;参数的更新会导致各层输入数据分布的变化，每次迭代，网络中的每一层需要学习拟合不同的数据分布，这样会显著降低网络的训练速度。当一个学习系统的输入分布是变化的，例如训练集和测试集的样本分布不一致，训练的模型就很难有较好的泛化能力。通常在将原始数据输入到网络中训练时，都会有一个归一化的预处理，也就是使得数据分布的均值为0、方差为1的近似白化过程。而Batch Normalization正是将这种归一化的方法应用到网络的每一层中，这样每一层的输入分布都不再受到前面参数变化的影响，该层网络也不需要适应输入分布的变化。</p>
<p>&emsp;&emsp;考虑到网络使用饱和非线性的激活函数例如Sigmoid，参数的变化可能会导致输入大量分布于饱和区域，局部梯度趋近于0，在反向传播的时候可能会出现梯度弥散的情况，进一步导致前面参数更新较慢或者停止更新，模型收敛困难。ReLU激活函数、合适的初值以及较小的学习率可以在一定程度上解决梯度消失的问题，而采用Batch Normalization的方法使每一层的输入分布更稳定，哪怕是使用Sigmoid激活函数，输入也不太可能分布于饱和区域，降低了模型对激活函数选择的依赖。</p>
<p>&emsp;&emsp;在传统的深度网络中，过高的学习率会导致梯度爆炸或消失(explode or vanish)，以及陷入局部极值，BN可以防止参数变化的影响逐层放大，避免陷入饱和区域。而且BN使得训练对参数的尺度(scale)更加鲁棒，一般高学习率会增加参数的scale，在反向传播中放大梯度，导致模型爆炸。在进行BN之后，<br>$$BN(wu) = BN((\alpha w)u)$$<br>$$\frac{\partial BN((\alpha w)u)}{\partial u} = \frac{\partial BN(wu)}{\partial u}$$<br>$$\frac{\partial BN((\alpha w)u)}{\partial (\alpha w)} = \frac{1}{\alpha} \frac{\partial BN(wu)}{\partial w}$$<br>可以看出较大的$w$将获得较小的梯度，也就是说较大的权值更新较小，较小的权值更新较大。因此，BN使得权重的更新更加稳健，反向传播的梯度不受参数scale的影响，解决了梯度爆炸或梯度消失的问题。</p>
<h4 id="What-is-BN"><a href="#What-is-BN" class="headerlink" title="What is BN"></a>What is BN</h4><p>&emsp;&emsp;Batch Normalization作用在每一层的输入，也就是激活函数（非线性变换）之前，而不是前一层的输出，BN的算法流程如下：</p>
<p><img src="https://i.loli.net/2017/09/16/59bce0018d695.jpg" alt="batch-norm.jpg" title="Batch Normalizing Transform"></p>
<p>BN Transform在对输入数据做0均值、单位方差的归一化处理后，又进行了一次线性变换，引入了一对可学习的参数$\gamma、\beta$，提升了模型的容纳能力(capacity)，如果$\gamma=\sqrt{\sigma_{\beta}^2}、\beta=\mu_{\beta}$，这样$y_i$就恢复成原来的输入分布$x_i$。 至于是否需要对$y_i$进行还原，则由网络模型自动从训练数据中学习决定。BN层的梯度反向传播公式如下：</p>
<p><img src="https://i.loli.net/2017/09/16/59bce41fc966a.jpg" alt="bn-bp.jpg" title="bn-bp.jpg"></p>
<p>训练结束将模型用于测试时，均值$\mu$和标准差$\sigma$是固定的，均值为训练阶段所有Batch均值的统计平均，方差为所有训练Batch方差的无偏估计，公式如下：<br>$$E[x]=E_{B}[\mu _B],&emsp;Var[x]=\dfrac{m}{m-1}E_{B}[\sigma_B^2]$$<br>在测试阶段，BN变换式为<br>$$y=\dfrac{\gamma}{\sqrt{Var[x]+\varepsilon}}x+(\beta-\dfrac{\gamma E[x]}{\sqrt{Var[x]+\varepsilon}})$$</p>
<p>整个训练和测试流程如下：</p>
<p><img src="https://i.loli.net/2017/09/16/59bce867e253a.jpg" alt="bn-inference.jpg" title="Training and Inference BN Network"></p>
<p>BN变换置于网络激活函数层的前面，对于普通前馈神经网络，BN是对一层的每个神经元进行归一化处理；而在卷积神经网络中，BN则是对一层的每个特征图进行处理，类似于权值共享，一个特征图只有一对可学习参数$\gamma、\beta$，如果某一层的输入张量为$[batch\_size, height, width, channels]$，则BN变换是按照channels进行的，minibatch的大小相当于$batch\_size \times height \times width$。</p>
<h4 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h4><p>总体来说，Batch Normalization主要有如下优点：</p>
<ul>
<li>保证网络各层输入的均值和方差稳定（数据分布不一定一致？），减弱Internal Convariate Shift的影响</li>
<li>减小参数scale和初值对梯度的影响，解决了梯度爆炸或梯度消失的问题</li>
<li>在某种程度上实现了正则化，减弱对L2-norm、Dropout的依赖</li>
<li>让输入分布于饱和区域(Sigmoid)的概率降低</li>
<li>降低超参数选择对模型的影响，可使用较大的学习率加速训练</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">Paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf" target="_blank" rel="external">Paper: Local Contrast Normalization: What is the Best Multi-Stage Architecture for Object Recognition?</a></li>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">Paper: ImageNet Classification with Deep Convolutional Neural Networks</a></li>
<li><a href="http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/" target="_blank" rel="external">http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/</a></li>
<li><a href="https://calculatedcontent.com/2017/06/16/normalization-in-deep-learning/" target="_blank" rel="external">https://calculatedcontent.com/2017/06/16/normalization-in-deep-learning/</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96</a></li>
<li><a href="http://www.csuldw.com/2015/11/15/2015-11-15%20normalization/" target="_blank" rel="external">http://www.csuldw.com/2015/11/15/2015-11-15%20normalization/</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="external">http://blog.csdn.net/hjimce/article/details/50866313</a></li>
<li><a href="https://stats.stackexchange.com/questions/117427/what-is-the-difference-between-zca-whitening-and-pca-whitening" target="_blank" rel="external">https://stats.stackexchange.com/questions/117427/what-is-the-difference-between-zca-whitening-and-pca-whitening</a></li>
<li><a href="http://shuokay.com/2016/05/28/batch-norm/" target="_blank" rel="external">http://shuokay.com/2016/05/28/batch-norm/</a></li>
<li><a href="http://jiangqh.info/Batch-Normalization%E8%AF%A6%E8%A7%A3/" target="_blank" rel="external">http://jiangqh.info/Batch-Normalization%E8%AF%A6%E8%A7%A3/</a></li>
<li><a href="http://blog.csdn.net/u012816943/article/details/51691868" target="_blank" rel="external">http://blog.csdn.net/u012816943/article/details/51691868</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="external">http://blog.csdn.net/hjimce/article/details/50866313</a></li>
</ul>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag">#DL</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/10/deep-learning-regularization/" rel="next" title="Deep Learning -- Regularization">
                <i class="fa fa-chevron-left"></i> Deep Learning -- Regularization
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/15/deep-learning-optimization/" rel="prev" title="Deep Learning -- Optimization">
                Deep Learning -- Optimization <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/my.jpg"
               alt="Senit_Co" />
          <p class="site-author-name" itemprop="name">Senit_Co</p>
          <p class="site-description motion-element" itemprop="description">Senit_Co's Blogs</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">30</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/senitco" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/hustershen/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/hustershen/activities" target="_blank" title="Zhihu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Zhihu
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalization-min-max"><span class="nav-number">1.</span> <span class="nav-text">Normalization(min-max)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Standardization"><span class="nav-number">2.</span> <span class="nav-text">Standardization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ZCA-Whitening"><span class="nav-number">3.</span> <span class="nav-text">ZCA Whitening</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Global-Contrast-Normalization-GCN"><span class="nav-number">4.</span> <span class="nav-text">Global Contrast Normalization(GCN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Local-Contrast-Normalization-LCN"><span class="nav-number">5.</span> <span class="nav-text">Local Contrast Normalization(LCN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Local-Response-Normalization-LRN"><span class="nav-number">6.</span> <span class="nav-text">Local Response Normalization(LRN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">7.</span> <span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Why-need-BN"><span class="nav-number">7.1.</span> <span class="nav-text">Why need BN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What-is-BN"><span class="nav-number">7.2.</span> <span class="nav-text">What is BN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Advantage"><span class="nav-number">7.3.</span> <span class="nav-text">Advantage</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reference"><span class="nav-number">8.</span> <span class="nav-text">reference</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Senit_Co</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'SenitCo';
      var disqus_identifier = '2017/09/12/deep-learning-normalization/';
      var disqus_title = 'Deep Learning -- Normalization';
      var disqus_url = 'https://senitco.github.io/2017/09/12/deep-learning-normalization/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  



  
  
  

  

  

</body>
</html>
