---
title: 集成学习之GBDT
date: 2017-11-12
categories: Machine Learning
tags: [ML、Ensemble、Boosting]
---
集成学习中Boosting族的经典模型：Boosting Tree(提升树)、GBDT(Gradient Boosting Decision Tree)、XGBoost(eXtreme Gradient Boosting)。首先介绍基础算法回归树，然后是提升树——以决策树为基学习器的提升方法，接着介绍Gradient Boosting，即梯度提升，最后导出GBDT算法。

### 回归树
#### 回归树简介
决策树分为分类树和回归树，分类树常用于分类标签值，如用户性别、用户是否购买、网页是否为垃圾页面等；而回归树常用于预测连续实值，如用户年龄、点击概率等。

回归树总体流程类似于分类树，区别在于回归树的每一个结点都会得到一个预测值，该预测值等于属于这个结点的所有样本输出的平均值。在属性（特征）划分时，穷举每一个特征的每一个阈值寻找最优切分变量和最优切分点，一般情况下都是先得到每一个特征的最优划分点，再比较得到最优划分特征。但衡量准则不再是分类树中的信息增益、增益率以及基尼指数，而是回归损失中常用的平方误差，通过最小化平方误差找到最优的分支依据。分支知道每个叶子结点上所有样本的输出都一致或者达到预设的终止条件如叶子结点数量上限，若最终叶子结点上样本输出不一致，则以该结点上所有样本输出的均值作为该结点的预测值。

Boosting族方法中，GBDT的核心思想是累加所有树的输出作为最终结果，分类树得到的离散分类结果对于预测分类不是那么容易直观叠加（例如“是否”这样的二值分类结果累加是没有意义的），而基于回归树所得到的连续实值进行加减是有意义的。GBDT算法运用了回归树的这个性质，将所有的树的累加结果作为最终的预测输出。所以GDBT中的基学习器都是回归树，只能用来做回归预测，当然调整之后也能做分类。值得注意的是，在AdaBoost中也是对每个基学习器（分类器，可以是分类树）采用加权求和得到最终输出，但每个基学习器的输出都是+1/-1，最后根据加权求和结果的符号进行二值分类，其累加求和没有数学上的意义，这点需要与GBDT区分。因此标准的AdaBoost算法只能用于二分类情况，多分类需要拓展。

#### 回归树生成
假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集
$$D=\{(x_1,y_1),(x_2,y_2),···,(x_N,y_N)\}$$
利用最小二乘回归树生成算法来生成回归树$f(x)$，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树，步骤如下：
- 选择最优切分变量$j$与切分点$s$，求解
$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\left(j,s\right)}{\left(y_i-c_1\right)^2}+\min_{c_2}\sum_{x_i\in R_2\left(j,s\right)}{\left(y_i-c_2\right)^2}\right]$$
遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值对$j,s$
- 用选定的对$(j,s)$划分区域并决定相应的输出值：
$$R_1\left(j,s\right)=\left\{x|x^{\left(j\right)}\le s\right\}\ ,\ R_2\left(j,s\right)=\left\{x|x^{\left(j\right)}>s\right\}$$
$$\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_2\left(j,s\right)}{y_i}\ ,\ x\in R_m\ ,\ m=1,2$$
- 继续对两个子区域调用步骤（1），（2），直至满足停止条件。
- 将输入空间划分为$M$个区域$R_1,R_2,⋅⋅⋅,R_M$，在每个单元$Rm$上有一个固定的输出值$c_m$，生成决策树：
$$f\left(x\right)=\sum_{m=1}^M{\hat{c}_m\textrm{I}\left(\textrm{x}\in\textrm{R}_{\textrm{m}}\right)}$$

### 提升树(Boosting Decision Tree)
#### 提升树模型
提升方法（Boosting族）采用加法模型（即基学习器的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（Boosting tree）。包括采用指数损失函数的分类提升树，采用平方误差损失函数的回归提升树。对于二分类问题，提升树是AdaBoost算法的特例，只需将基学习器限定为二分类树即可。对于回归问题，提升树是迭代得到多棵回归树来共同决策，当采用平方误差损失函数时，每次迭代都是拟合当前模型的残差，最终累加所有回归树的输出，加法模型可表示为
$$f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}$$
其中$T(x;Θm)$表示决策树，$Θm$为决策树的参数，$M$为树的个数。
#### 提升树算法
对回归问题的提升树算法来说，给定当前模型 fm−1(x)只需要简单地拟合当前模型的残差。算法流程如下：
- 初始化$f_0(x)=0$
- 对$m=1,2,⋅⋅⋅,M$
a）计算残差
$$r_{mi}=y_i−f_{m−1}(x_i) , i=1,2,⋅⋅⋅,N$$
b）拟合残差$r_{mi}$学习一个回归树，得到 $T(x;Θm)$
c）更新$f_m(x)=f_{m−1}(x)+T(x;Θm)$
- 得到回归问题提升树
$$f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}$$

关于回归树和提升树的详细公式推导及具体实例可参考统计学习方法中决策树和提升方法这两章。


### 梯度提升(Gradient Boosting)
提升树是利用加法模型和前向分步算法实现学习的优化过程，当损失函数是平方损失（拟合当前模型残差）或指数损失（基于当前样本分布训练分类器）时，每一步优化都比较简单。但对一般损失函数而言，往往每一轮迭代的优化并不容易，如绝对值损失函数和Huber损失函数。针对这一问题，Freidman提出了梯度提升(Gradient Boosting)算法，这是利用最速下降法（梯度下降法）的近似方法，利用损失函数的负梯度在当前模型的值
$$-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}$$
作为回归问题提升树算法中的残差近似值，拟合这一轮迭代中的回归树。与其说负梯度是残差的近似值，不如说残差是负梯度的一种特例，在采用平方误差损失函数的情况下，残差等于负梯度。
常见的损失函数及其梯度如下表所示：
![Alt text](./1522030967991.png)

### GBDT
梯度提升决策树(Gradient Boosting Decision Tree, GBDT)是一种迭代的决策树算法，又叫 MART(Multiple Additive Regression Tree)，通过构造一组弱的学习器（回归树），并把多颗决策树的结果累加起来作为最终的预测输出。在GBDT的迭代中，假设当前的集成模型为$f_{t−1}(x)$，对应的损失函数则为$L(y,f_{t−1}(x))$。因此新一轮迭代的目的就是找到一个弱分类器$ht(x)$，使得损失函数$L(y,f_{t−1}(x)+ht(x))$达到最小。问题的关键就在于对损失函数的度量。

#### 算法流程
- 初始化弱分类器，估计使损失函数极小化的一个常数值，此时树仅有一个根结点
$$f_0\left(x\right)=arg\min_c\sum_{i=1}^N{L\left(y_i,c\right)}$$
- 对迭代轮数$m = 1,2,⋅⋅⋅,M$
（a）对$i=1,2,⋅⋅⋅,N$，计算损失函数的负梯度值在当前模型的值，将它作为残差的估计。即
$$r_{mi}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}$$
对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。
（b）对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R{mj}$，$j=1,2,⋅⋅⋅,J$
（c）对$j=1,2,⋅⋅⋅,J$计算
$$c_{mj}=arg\min_c\sum_{x_i\in R_{mj}}{L\left(y_i,f_{m-1}\left(x_i\right)+c\right)}$$
上面（b）（c）可以合为一步，在划分得到每个区域时，即可得到相应的预测值。
（d）更新回归树
$$f_m\left(x\right)=f_{m-1}\left(x\right)+\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}$$
- 得到最终输出模型
$$\hat{f}\left(x\right)=f_M\left(x\right)=\sum_{m=1}^M{\sum_{j=1}^J{c_{mj}I\left(x\in R_{mj}\right)}}$$

### QA
（1）对于一颗原始回归树和基于多颗回归树集成得到的提升树（也可以是GBDT），可能在训练集上的拟合程度差异不大，那提升树的优势在哪？

模型泛化能力是一个关键因素，对于原始回归树，为了尽可能准确地拟合训练数据，可能会使用到非常多的特征，使得树的深度较深，造成过拟合的现象；而对于提升树或者GBDT而言，可能所有回归树所使用的特征数之和更少，在具备较强拟合能力的基础上，模型的泛化性能更好。

（2）GBDT/XGBoost在调参时为什么树的深度很少就能达到很高的精度？（用GBDT、XGBoost在在调参的时候把树的最大深度调成6就有很高的精度了，但是用DecisionTree、RandomForest的时候需要把树的深度调到15或更高。）[Source](https://www.zhihu.com/question/45487317)

Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。（周志华《机器学习》）

随机森林(random forest)和GBDT都是属于集成学习(ensemble learning)的范畴。集成学习下有两个重要的策略Bagging和Boosting。Bagging族算法中，每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，再把这些分类器组合起来，分类问题可采用投票法（随机森林），回归问题可采用取均值。在Boosting族算法中，则是通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关，其代表算法是AdaBoost, GBDT。

就机器学习算法来说，其泛化误差可以分解为两部分，偏差（Bias）和方差（variance），公式表示为$D(X)=E(X^2)-[E(X)]^2$。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。在训练模型时，需要兼顾偏差和方差。
![Alt text](./1522033211494.png)

对于Bagging算法来说，并行地训练很多不同的基学习器，目的就是降低模型方差(variance) ，提升模型的泛化能力。对每个基学习器来说，就应该着重关注如何降低偏差（bias），所以一般会采用深度很深甚至不剪枝的决策树。对于Boosting算法来说，每一步都是在上一轮的基础上更加拟合原数据，可以保证低偏差（bias）,对每个基学习器而言，则应关注降低方差，即使用更简单的学习器，所以选择深度很浅的决策树。

### reference
- [plushunter.github.io](https://plushunter.github.io/2017/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9AGBDT/)
- [geosmart.github.io](http://geosmart.github.io/2017/07/26/GBDT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/)
- https://blog.csdn.net/sb19931201/article/details/52506157

