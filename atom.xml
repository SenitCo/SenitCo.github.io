<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Senit_Co</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://senitco.github.io/"/>
  <updated>2018-03-20T04:42:28.029Z</updated>
  <id>https://senitco.github.io/</id>
  
  <author>
    <name>Senit_Co</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>C++智能指针</title>
    <link href="https://senitco.github.io/2017/10/10/cplusplus-smart-pointer/"/>
    <id>https://senitco.github.io/2017/10/10/cplusplus-smart-pointer/</id>
    <published>2017-10-09T16:00:00.000Z</published>
    <updated>2018-03-20T04:42:28.029Z</updated>
    
    <content type="html"><![CDATA[<p>C++中智能指针（auto_ptr、unique_ptr、shared_ptr）的简单总结。<br><a id="more"></a></p>
<h3 id="智能指针的设计思想"><a href="#智能指针的设计思想" class="headerlink" title="智能指针的设计思想"></a>智能指针的设计思想</h3><pre><code>先看一个简单的例子：
</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> &amp; str)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> * ps = <span class="keyword">new</span> <span class="built_in">std</span>::<span class="built_in">string</span>(str);</div><div class="line">    ...</div><div class="line">    <span class="keyword">if</span> (weird_thing())</div><div class="line">        <span class="keyword">throw</span> exception();</div><div class="line">    str = *ps; </div><div class="line">    <span class="keyword">delete</span> ps;</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当出现异常时（weird_thing()返回true），delete将不被执行，因此将导致内存泄露。如何避免这种问题？直接在throw exception();之前加上delete ps;。这样确实可行，但更多时候开发者都会忘记在适当的地方加上delete语句。换个角度想，如果func()函数终止（不管是正常终止还是异常终止），局部变量都会自动从栈内存中删除，因此指针ps占据的内存将被释放，如果ps指向的内存也能被自动释放，这样就不会出现内存泄漏的问题，析构函数确实有这个功能，如果ps有一个析构函数，该析构函数将在ps过期时自动释放它指向的内存。但是问题在于ps只是一个常规指针，不是有析构凼数的类对象指针。因此引出了智能指针的设计思想：将常规指针封装成类（类模板，以适应不同类型的需求），并在析构函数里编写delete语句删除指针指向的内存空间。这样就无需程序员显式调用delete语句释放内存，也不会因为程序异常退出导致内存泄漏。</p>
<h3 id="智能指针的简单介绍"><a href="#智能指针的简单介绍" class="headerlink" title="智能指针的简单介绍"></a>智能指针的简单介绍</h3><p>&emsp;&emsp;C++一共提供了四种智能指针：auto_ptr、unique_ptr、shared_ptr和weak_ptr（此处只介绍前三种）。auto_ptr是C++98提供的解决方案，C+11已将将其摒弃，并提供了另外两种解决方案。然而，虽然auto_ptr被摒弃，但已使用了多年：同时，如果你所用的编译器不支持其他两种解决力案，auto_ptr将是唯一的选择。</p>
<ul>
<li><p>所有的智能指针都有一个explicit构造函数，以指针作为参数，因此不能自动将指针转换为智能指针对象，必须显示调用</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">double</span>&gt; pd; </div><div class="line"><span class="keyword">double</span> *p_reg = <span class="keyword">new</span> <span class="keyword">double</span>;</div><div class="line">pd = p_reg;                               <span class="comment">// not allowed (implicit conversion)</span></div><div class="line">pd = <span class="built_in">shared_ptr</span>&lt;<span class="keyword">double</span>&gt;(p_reg);           <span class="comment">// allowed (explicit conversion)</span></div><div class="line"></div><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">double</span>&gt; pshared = p_reg;       <span class="comment">// not allowed (implicit conversion)</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">double</span>&gt; pshared(p_reg);        <span class="comment">// allowed (explicit conversion)</span></div><div class="line"></div><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">double</span>&gt; ps = <span class="keyword">new</span> <span class="keyword">double</span>(<span class="number">10.0</span>);   <span class="comment">// not allowed (implicit conversion)</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">double</span>&gt; ps(<span class="keyword">new</span> <span class="keyword">double</span>(<span class="number">10.0</span>));    <span class="comment">// allowed (explicit conversion)</span></div></pre></td></tr></table></figure>
</li>
<li><p>应避免将存储在栈区的普通变量或对象的地址作为智能指针的初始化参数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> number = <span class="number">10</span>;</div><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; pvac(&amp;number);   <span class="comment">// pvac过期时，程序将把delete运算符用于非堆内存，这是错误的</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="auto-ptr"><a href="#auto-ptr" class="headerlink" title="auto_ptr"></a>auto_ptr</h3><p>&emsp;&emsp;对于下面的赋值语句，可以看出，两个常规指针指向同一个对象，这样会导致错误的结果，因为程序试图销毁同一个对象两次，除非程序员特别注意只delete一个指针，但这个过程是不可控的，也应尽量避免在程序中留下这种隐患。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">string</span> *p1 = <span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"Hello World!"</span>);</div><div class="line"><span class="built_in">string</span> *p2 = p1;</div><div class="line">...</div><div class="line"><span class="keyword">delete</span> p1;</div><div class="line"><span class="keyword">delete</span> p2;</div></pre></td></tr></table></figure></p>
<p>要避免这种问题，有多种方法：</p>
<ul>
<li>在复制指针的时候采用深拷贝。这样两个指针将指向不同的对象，其中一个对象时另一个对象的副本。缺点时浪费空间，所有智能指针都未采用此方案。</li>
<li>建立所有权概念。对于特定的对象，只有一个智能指针可拥有，在进行复制操作时，会转让对象的所有权，这样只有拥有该对象的智能指针在生命周期结束时会自动销毁对象。这就是auto_ptr和unique_ptr的策略，但unique_ptr的策略更严格。</li>
<li>创建智能程度更高的指针，跟踪引用（指向）特定对象的智能指针数，称为引用计数。例如赋值时引用计数加一，而指针过期时计数减一，当减为0时才调用delete。这时shared_ptr采用的策略。</li>
</ul>
<p>同样的策略适用于复制运算符（重载）和复制构造函数。但是auto_ptr存在一定的弊端，这也是在后面的版本中将其摒弃的原因。举个例子：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;	//包含智能指针的头文件</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">	<span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; str[<span class="number">5</span>] = &#123;</div><div class="line">		<span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; (<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"What"</span>));</div><div class="line">		<span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; (<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"Why"</span>));</div><div class="line">		<span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; (<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"Where"</span>));</div><div class="line">		<span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; (<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"when"</span>));</div><div class="line">		<span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; (<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"How"</span>));</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="built_in">auto_ptr</span>&lt;<span class="keyword">int</span>&gt; s;</div><div class="line">	s = str[<span class="number">2</span>];	<span class="comment">//str[2]所指对象的所有权转让给了s</span></div><div class="line">	<span class="keyword">if</span>(str[<span class="number">2</span>].get() == <span class="literal">NULL</span>)	<span class="comment">//此处判断条件为真</span></div><div class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"The pointer is null!"</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line"></div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)</div><div class="line">		<span class="built_in">cout</span> &lt;&lt; *str[i]&lt;&lt; <span class="built_in">endl</span>;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>上面程序运行会崩溃，因为str[2]转让所有权后（对应的裸指针）变成空指针了，因此输出访问空指针必然会崩溃。如果将auto_ptr换成shared_ptr或unique_ptr后，程序不会崩溃。</p>
<ul>
<li>使用shared_ptr时运行正常，因为shared_ptr采用引用计数，s与str[2]指向同一块内存，同一个对象，在释放空间时会先判断引用计数值的大小，不会出现多次删除同一个对象的情况。</li>
<li>使用unique_ptr时编译出错，与auto_ptr一样，unique_ptr也采用所有权模型，但在使用unique时，程序不会等待运行阶段崩溃，而会在编译期就报错。<br>之所以要摒弃auto_ptr，就是要避免潜在的内存崩溃问题。</li>
</ul>
<h3 id="unique-ptr"><a href="#unique-ptr" class="headerlink" title="unique_ptr"></a>unique_ptr</h3><p>&emsp;&emsp;unique_ptr和auto_ptr一样，一个对象只能被一个智能指针所占有，这样可防止多个指针试图销毁销毁同一个对象。但在进行复制操作后，auto_ptr可能存在潜在的内存崩溃问题，如果访问失去对象所有权的指针的话，而unique_ptr则会在编译期提前报错。此外，unique_ptr还有其他优势，例如下面的例子<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt; func(<span class="keyword">const</span> <span class="keyword">char</span>* s)</div><div class="line">&#123;</div><div class="line">	<span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt; temp(<span class="keyword">new</span> <span class="built_in">string</span>(s));</div><div class="line">	<span class="keyword">return</span> temp;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt; ps = func(<span class="string">"Hello World"</span>);</div></pre></td></tr></table></figure></p>
<p>func()返回一个临时的unique_ptr，ps接管了临时unique_ptr所指的对象，而返回时该临时unique_ptr被销毁，也就是说后面没有机会通过该指针来访问无效的数据。也就是说这种赋值是没有问题的，实际上，编译器也允许unique_ptr的这种赋值。<br>简单总结，当程序试图将一个unique_ptr赋值给另一个时，如果源unique_ptr是个临时右值，编译器运行这么做；如果源unique_ptr将存在一段时间，编译器将禁止这种做法。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt; ps1(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"Hello World"</span>));	</div><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt; ps2 = ps1;	<span class="comment">//调用拷贝构造函数 #1 not allowed</span></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">unique_ptr&lt;string&gt; ps2;</span></div><div class="line"><span class="comment">ps2 = ps1；	//赋值运算符重载（operator=）</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt; ps3;		<span class="comment">//调用默认（无参）构造函数</span></div><div class="line">ps3 = <span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"Hello World"</span>));	<span class="comment">//先调用有参构造函数创建临时对象，再调用赋值运算符	#2 allowed</span></div></pre></td></tr></table></figure></p>
<p>前面已经提到，第一种情况#1编译器是会报错的，转让对象所有权留下了空指针，而第二种情况#2是先创建可一个unique_ptr临时对象，然后将所指对象的所有权转让给ps3。这种随情况而异的行为表明，unique_ptr从安全性和实用性方面都优于auto_ptr。<br>如果想对unique_ptr对象执行普通的赋值操作，可借助移动语义实现（将左值转换为右值，std::move()）,能够直接将一个unique_ptr赋给另一个，使用move()后，原来的指针转让所有权成为空指针，可对其重新赋值后再使用。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="built_in">string</span>&gt; ps1 = unique&lt;<span class="built_in">string</span>&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">"Hello World!"</span>));	<span class="comment">//等价于unique_ptr&lt;string&gt; ps1(new string("Hello world!"))，直接调用有参构造函数</span></div><div class="line">uinque_ptr&lt;<span class="built_in">string</span>&gt; ps2 = <span class="built_in">std</span>::move(ps1)</div><div class="line"><span class="keyword">if</span>(ps == <span class="literal">NULL</span>)	<span class="comment">//此处判断条件为真，与auto_ptr必须通过get()成员函数返回指针略有不同，ps可直接和指针进行比较（应该是重载了operator==），也可通过get()函数返回指针</span></div><div class="line">	<span class="built_in">cout</span> &lt;&lt; <span class="string">"The pointer is null!"</span> &lt;&lt; <span class="built_in">endl</span>;</div></pre></td></tr></table></figure></p>
<h3 id="shared-ptr"><a href="#shared-ptr" class="headerlink" title="shared_ptr"></a>shared_ptr</h3><p>&emsp;&emsp;shared_ptr是通过引用计数来共享同一个对象的智能指针，也就是说多个shared_ptr指针可指向同一个对象，通过引用计数的方式可避免多次释放内存。但在使用shared_ptr指针时也存在一些陷阱。见下面例子：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// #1 正确用法</span></div><div class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; ps1(<span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">0</span>));</div><div class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; ps2 = ps1;</div><div class="line"></div><div class="line"><span class="comment">// #2 错误用法</span></div><div class="line"><span class="keyword">int</span> *p = <span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">0</span>);</div><div class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; ps3(p);</div><div class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; ps4(p);</div></pre></td></tr></table></figure></p>
<p>第一个例子中是两个shared_ptr指针共享一个对象，ps1和ps2是有关联的，引用计数为2，最后一个负责释放new的变量；第二个例子中是两个独立的shared_ptr指针指向了同一个对象，ps3和ps4是没有关联的，他们并不知道对方的存在，因此会争相去释放p指针，导致重复释放。一个裸指针只能用来初始化一个shared_ptr指针，如果要让对象在多个指针间共享，需通过shared_ptr指针之间的拷贝，而不能直接拷贝原始指针。<br>&emsp;&emsp;使用智能指针难以避免的场景之一就是需要把 this 当成一个 shared_ptr 传递到其他函数中去，不能简单粗暴的用 this 指针构造一个 shared_ptr，因为那样做会导致两个独立的 shared_ptr 指向同一份内存资源，就像第二个例子那样<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span> </span></div><div class="line"><span class="class">&#123;</span></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; GetPtr() </div><div class="line">	&#123;</div><div class="line">		<span class="keyword">return</span> <span class="built_in">shared_ptr</span>&lt;Widget&gt;(<span class="keyword">this</span>); <span class="comment">// 错误</span></div><div class="line">	&#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">	<span class="keyword">auto</span> widget = <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt;();</div><div class="line">	widget-&gt;GetPtr();</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>正确的做法是继承模板类 std::enable_shared_from_this，然后调用它的 shared_from_this 成员函数，这种把自己作为基类的模板参数的做法称为——奇异递归模板模式<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span> :</span> <span class="keyword">public</span> <span class="built_in">std</span>::enable_shared_from_this&lt;Widget&gt; </div><div class="line">&#123;</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; GetPtr()</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">return</span> shared_from_this();</div><div class="line">	&#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">	<span class="keyword">auto</span> widget = <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt;();</div><div class="line">	widget-&gt;GetPtr();</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;在裸指针被初始化给多个shared_ptr的异常场景下，shared_from_this返回的对象将会增加哪个shared_ptr的引用计数呢？ 对于这种未定义的行为通常答案是由编译器决定。可以看出裸指针通过shared_from_this返回的对象与最近一个初始化的share_ptr相关联。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>:</span> <span class="keyword">public</span> <span class="built_in">std</span>::enable_shared_from_this&lt;Widget&gt;</div><div class="line">&#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; xxx()</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">return</span> shared_from_this();</div><div class="line">	&#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">	Widget *p = <span class="keyword">new</span> Widget();</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; one1(p);</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; one2(one1);</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; one3(one1);</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; one1.use_count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">//3</span></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; one2.use_count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">//3</span></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; one3.use_count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">//3</span></div><div class="line"></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; two1(p);    <span class="comment">//这种用一个裸指针初始化多个shared_ptr指针的行为实际上是不允许的</span></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; two1.use_count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">//1</span></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; guess = p-&gt;xxx();</div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; one1.use_count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">//3</span></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; two1.use_count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">//2</span></div><div class="line">	<span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; guess.use_count() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">//2</span></div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>; <span class="comment">// crash at end</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="如何选择智能指针"><a href="#如何选择智能指针" class="headerlink" title="如何选择智能指针"></a>如何选择智能指针</h3><ul>
<li>如果程序要使用多个指向同一个对象的指针，应该选择shared_ptr。例如STL容器中包含指针，很多STL算法都只会复制和赋值操作，这些操作可用于shared_ptr，而不能用于unique_ptr（编译器warning警告）和auto_ptr（行为不确定）。</li>
<li>如果程序不需要多个指向同一个对象的指针，则可使用unique_ptr，可省去显式调用delete销毁对象的过程；而且unique_ptr可通过传引用的方式作为函数的参数，传值则不允许。</li>
<li>尽量不要使用auto_ptr指针。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; make_int(<span class="keyword">int</span> n)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">return</span> <span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt;(<span class="keyword">new</span> <span class="keyword">int</span>(n));	</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">(<span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; &amp;p)</span>	<span class="comment">//形参为引用是允许的，传值则不允许</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="built_in">cout</span> &lt;&lt; *a &lt;&lt; <span class="string">' '</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    ...</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; &gt; vp(size);</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; vp.size(); i++)</div><div class="line">        vp[i] = make_int(rand() % <span class="number">1000</span>);       <span class="comment">// copy temporary unique_ptr</span></div><div class="line">    vp.push_back(make_int(rand() % <span class="number">1000</span>));     <span class="comment">// ok because arg is temporary</span></div><div class="line">    for_each(vp.begin(), vp.end(), show);      <span class="comment">// use for_each()</span></div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中push_back调用没有问题，因为它返回一个临时unique_ptr，该unique_ptr被赋给vp中的一个unique_ptr。另外，如果按值而不是按引用给show()传递对象，for_each()将非法，因为这将导致使用一个来自vp的非临时unique_ptr初始化p，而这是不允许的，编译器会报错。</p>
<p>在unique_ptr为右值时，可将其赋给shared_ptr，这与将一个unique_ptr赋给另一个需要满足的条件相同。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; p1(make_int(rand() % <span class="number">1000</span>));   <span class="comment">// ok</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; p2(p1);                        <span class="comment">// not allowed, p1 as lvalue</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt; p3(make_int(rand() % <span class="number">1000</span>));   <span class="comment">// ok</span></div></pre></td></tr></table></figure></p>
<p>模板shared_ptr包含一个显式构造函数，可用于将右值unique_ptr转换为shared_ptr。shared_ptr将接管原来归unique_ptr所有的对象。在满足unique_ptr要求的条件时，也可使用auto_ptr，但unique_ptr是更好的选择。如果你的编译器没有unique_ptr，可考虑使用Boost库提供的scoped_ptr，它与unique_ptr类似。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://yizhi.ren/2016/11/14/sharedptr/" target="_blank" rel="external">http://yizhi.ren/2016/11/14/sharedptr/</a></li>
<li><a href="http://blog.guorongfei.com/2017/01/25/enbale-shared-from-this-implementaion/" target="_blank" rel="external">http://blog.guorongfei.com/2017/01/25/enbale-shared-from-this-implementaion/</a></li>
<li><a href="http://www.cnblogs.com/lanxuezaipiao/p/4132096.html" target="_blank" rel="external">http://www.cnblogs.com/lanxuezaipiao/p/4132096.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;C++中智能指针（auto_ptr、unique_ptr、shared_ptr）的简单总结。&lt;br&gt;
    
    </summary>
    
      <category term="C++" scheme="https://senitco.github.io/categories/C/"/>
    
    
      <category term="C++" scheme="https://senitco.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning -- Optimization</title>
    <link href="https://senitco.github.io/2017/09/15/deep-learning-optimization/"/>
    <id>https://senitco.github.io/2017/09/15/deep-learning-optimization/</id>
    <published>2017-09-14T16:00:00.000Z</published>
    <updated>2017-09-18T04:15:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;在机器学习中，模型优化通常会定义一个代价函数(Loss Function)，然后通过最小化代价函数，求得一组参数，例如Logistic Regression、SVM以及神经网络等都属于这类问题，而这类模型往往使用迭代法求解，其中梯度下降法(Gradient Descent)应用最为广泛。<br><a id="more"></a></p>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>&emsp;&emsp;由一组模型参数$\theta$定义的目标函数$J(\theta)$，梯度下降法通过计算目标函数对所有参数的梯度$\nabla _{\theta}J(\theta)$，然后往梯度相反的方向更新参数，使得目标函数逐步下降，最终趋于（局部）极小值。<br>$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta)$$<br>学习率$\eta$决定每次更新的步长。梯度下降法有三种变体：Batch gradient descent、Stochastic gradient descent、Mini-batch gradient descent。不同之处在于每次迭代训练使用的样本数。</p>
<h4 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h4><p>&emsp;&emsp;BGD每次迭代计算所有训练样本的代价函数（均值），并求其梯度用于更新参数，这种方法在较小的学习率下能够保证收敛到全局最小（凸函数）或者局部最小（非凸函数）。但每次更新参数需要计算整个训练集的梯度，造成训练缓慢；样本数据较多的情况下无法全部放入内存；并且不能在线(Online)更新。</p>
<h4 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h4><p>&emsp;&emsp;SGD和BGD相反，每次迭代训练只计算一个样本的梯度，对参数进行更新。相比于Batch gradient descent对大数据集执行冗余计算，每次更新参数之前都要重复计算相似样本的梯度，SGD通过每次计算单个样本可以消除这种冗余，参数更新较快，可用于在线学习。而且SGD在迭代的过程中波动较大，可能从一个局部最优跳到另一个更好的局部最优，甚至是全局最优。但SGD没法利用矩阵运算加速计算过程。</p>
<h4 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h4><p>&emsp;&emsp;Mini-batch Gradient Descent相当于是BGD和SGD的折衷，每次迭代训练从数据集中选取一部分样本(minibatch)计算梯度值。这种方法减少了参数更新时的波动，使代价函数整体上呈下降趋势，达到了平稳收敛的效果；而且可以有效利用矩阵计算工具加速训练，通常minibatch的取值范围为[50, 256]。由于minibatch的方法使用较为广泛，因此现在的SGD一般是指Mini-batch Gradient Descent。</p>
<h4 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h4><p>&emsp;&emsp;上述几种梯度下降法并不能保证模型达到最优的效果，还有一些比较有挑战性的问题需要解决：</p>
<ul>
<li>选取合适的学习率是困难的。学习率较小导致训练缓慢；学习率较大造成收敛困难，代价函数在最小值附近波动甚至发散</li>
<li>通过Schedule的方式在训练过程中调整学习率，例如迭代到一定次数后降低学习率，但这些都需要提前设置好，不能适应训练数据的特性</li>
<li>所有参数更新的学习率是相同的，如果训练数据是稀疏的，而且特征出现的频率不一样，比较合适的做法是对出现频率低的特征采用较大的学习率更新</li>
<li>对于非凸函数，可能会陷入鞍点(saddle points)而导致训练困难，在鞍点区域某些维度呈上升趋势，而另外一些呈下降趋势，鞍点的梯度在所有维度都趋近于0，导致代价函数陷入其中，参数更新困难</li>
</ul>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>&emsp;&emsp;SGD存在一个缺陷就是，当某一个方向(维度)梯度较大、另一个方向梯度较小时，代价函数可能会在大梯度方向以大步长来回振荡，而在小梯度方向缓慢前进，造成收敛缓慢。Momentum受到物理学中动量的启发，能够在相关方向加速SGD，并抑制振荡，加快收敛<br><img src="https://i.loli.net/2017/09/17/59be150c4a4d4.jpg" alt="momentum.jpg" title="SGD without momentum(left) and with momentum(right)"></p>
<p>\begin{split}<br>v_t &amp;= \gamma v_{t-1} + \eta \nabla_\theta J(\theta) \cr<br>\theta &amp;= \theta - v_t<br>\end{split}</p>
<p>由上式可以看到，当前梯度的方向要与之前的梯度方向进行加权平均，$\gamma$一般取值为0.9。在梯度方向不变的维度，momentum项增加；在梯度方向频繁改变的维度，momentum项更新减少。经过一段时间动量项的累加，可以在一定程度上抑制振荡，加速收敛。</p>
<h3 id="Nesterov-accelerated-gradient-NAG"><a href="#Nesterov-accelerated-gradient-NAG" class="headerlink" title="Nesterov accelerated gradient(NAG)"></a>Nesterov accelerated gradient(NAG)</h3><p>&emsp;&emsp;在梯度下降更新参数的过程中，有时候希望提前知道下一步到达位置的梯度，以便及时调整参数更新方向。NAG实现了这一想法，利用Momentum预测下一步的梯度，而不是直接使用当前的$\theta$</p>
<p>\begin{split}<br>v_t &amp;= \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \cr<br>\theta &amp;= \theta - v_t<br>\end{split}</p>
<p>动量项系数$\gamma$取值为0.9。在上面提到的Momentum方法中，首先计算当前参数(位置)的梯度，然后更新动量项，并沿着动量项（更新后）的方向更新参数；而在NAG方法中，首先沿着之前累积的动量项的方向更新参数，然后基于更新后的参数计算梯度，并对参数进行调整。两种方法的对比如下图所示：<br><img src="https://i.loli.net/2017/09/17/59be22509c283.jpeg" alt="nesterov.jpeg" title="Momentum and NAG"><br>详细介绍可参考<a href="http://cs231n.github.io/neural-networks-3/#sgd-and-bells-and-whistles" target="_blank" rel="external">cs231n-sgd</a></p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>&emsp;&emsp;Adagrad是一种在学习过程中自动调整学习率的参数优化方法，不同参数采用不同的学习率进行更新。出现频率低的参数更新较大，频率高的参数更新较小，因此特别适用于处理稀疏数据，而且Adagrad提高了SGD的鲁棒性</p>
<p>\begin{split}<br>g_t &amp;= \nabla_\theta J( \theta) \cr<br>G_t &amp;= G_{t-1} + g_t^2 \cr<br>\theta_{t+1} &amp;= \theta_t -  \dfrac{\eta}{\sqrt{G_t + \varepsilon}} \cdot g_t<br>\end{split}</p>
<p>$t$表示迭代次数，$G_t是每个参数的梯度平方的累加和$，使用$\sqrt{G_t + \varepsilon}$作为约束项自动调整每个参数在训练过程中的学习率。对于梯度较大的参数，有效学习率会降低；而梯度较小的参数，有效学习率则相对会增大。但对每个参数而言，由于梯度平方和的累加，学习率实际上一直是递减的，这也是Adagrad的一个缺陷，可能会导致学习率降低至0，过早结束训练过程。</p>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p>&emsp;&emsp;Adagrad方法比较激进，会过早结束训练过程。在对参数进行调整时，Adagrad使用的是每次迭代的梯度累加和，而在Adadelta中则通过衰减平均(decaying average)的方式计算梯度平方的加权均值<br>$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t$$<br>$\gamma$可取值为0.9(0.95)，使用$E[g^2]_t$调整学习率<br>$$\Delta \theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}$$<br>考虑到分母为梯度的均方根(root mean squared, RMS)，公式可替换为<br>$$\Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t,&emsp;RMS[g]_t=\sqrt{E\left[g^2\right]_t+\epsilon}$$<br>将学习率$\eta$替换为$RMS[\Delta \theta]_{t-1}$，利用之前的步长估计下一步的步长，公式如下：</p>
<p>\begin{split}<br>\Delta \theta_t &amp;= - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t} \\<br>\theta_{t+1} &amp;= \theta_t + \Delta \theta_t<br>\end{split}</p>
<p>式中，$RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon},E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t$，在Adadelta中，不需要设置默认学习率，其自身可通过迭代计算得到。详细讲解参考原论文<a href="https://arxiv.org/pdf/1212.5701v1.pdf" target="_blank" rel="external">Adadelta</a></p>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>&emsp;&emsp;RMSprop同样是用来解决Adagrad学习率趋于0、过早结束训练的方法，来源于Hinton大神的<a href="http://218.199.87.242/cache/9/03/www.cs.toronto.edu/a1dce074ab79c3b6a179c02438d76249/lecture_slides_lec6.pdf" target="_blank" rel="external">slide</a>，而且思想基本和Adadelta一致，公式就是上面Adadelta方法中的前两项：</p>
<p>\begin{split}<br>E[g^2]_t &amp;= \gamma E[g^2]_{t-1} + (1-\gamma) g^2_t \\<br>\theta_{t+1} &amp;= \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}<br>\end{split}</p>
<p>$\gamma$为权重衰减率，一般可设置为0.9、0.99、0.999，$\eta$为学习率，可取值为0.001。</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>&emsp;&emsp;Adaptive Moment Estimation(Adam)是一种结合了Momentum和RMSprop优点的方法，利用指数衰减平均的方法(exponentially decaying average)计算梯度平方的加权均值$v_t$，类似于Adadelta和RMSprop，并且基于同样的方法计算梯度值的加权均值$m_t$，相当于引入动量项</p>
<p>\begin{split}<br>m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\<br>v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2<br>\end{split}</p>
<p>$m_t$和$v_t$分别是梯度的一阶矩（均值）和二阶矩（方差）估计，$\beta_1$和$\beta_2$为衰减率，一般取值为$\beta_1=0.9$、$\beta_2=0.999$。由于$m_t$和$v_t$在初始几步的取值较小，趋近于0，Adam对此进行了偏差校正(Bias Correction)：</p>
<p>\begin{split}<br>\hat{m}_t &amp;= \dfrac{m_t}{1 - \beta^t_1} \\<br>\hat{v}_t &amp;= \dfrac{v_t}{1 - \beta^t_2}<br>\end{split}</p>
<p>参数更新规则类似于RMSprop：<br>$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$<br>Adam集成了多种优化方法的优点，既适用于处理稀疏数据，又能够达到平稳快速收敛的效果，应用十分广泛。</p>
<h3 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h3><p>&emsp;&emsp;AdaMax是Adam的一种变体，在Adam中计算$v_t$时考虑的是$\ell_2$范数(l2-norm)，AdaMax将这种约束推广到$\ell_p$范数：<br>$$v_t = \beta_2^p v_{t-1} + (1 - \beta_2^p) |g_t|^p$$<br>$\ell_\infty$能够取得较为稳定的结果，因此在AdaMax中被采纳</p>
<p>\begin{split}<br>u_t &amp;= \beta_2^\infty v_{t-1} + (1 - \beta_2^\infty) |g_t|^\infty\\<br>              &amp; = \max(\beta_2 \cdot v_{t-1}, |g_t|)\\<br>              \theta_{t+1} &amp;= \theta_{t} - \dfrac{\eta}{u_t} \hat{m}_t<br>\end{split}</p>
<p>$u_t$取两项中的较大值，不需要再执行偏差校正操作，$\hat{m}_t$仍然和Adam中一样，超参数取值分别为$\eta = 0.002,\beta_1=0.9,\beta_2=0.999$。</p>
<h3 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h3><p>&emsp;&emsp;Nadam类似于带有Nesterov动量项的Adam，相当于NAG和Adam的组合，在NAG中，参数更新规则为：</p>
<p>\begin{split}<br>g_t &amp;= \nabla_{\theta_t}J(\theta_t - \gamma m_{t-1})\\<br>m_t &amp;= \gamma m_{t-1} + \eta g_t\\<br>\theta_{t+1} &amp;= \theta_t - m_t<br>\end{split}</p>
<p>在NAG中既要更新梯度$g_t$，又要更新参数$\theta_{t+1}$，Nadam作者Dozat直接应用look-ahead momentum更新当前参数</p>
<p>\begin{split}<br>g_t &amp;= \nabla_{\theta_t}J(\theta_t)\\<br>m_t &amp;= \gamma m_{t-1} + \eta g_t\\<br>\theta_{t+1} &amp;= \theta_t - (\gamma m_t + \eta g_t)<br>\end{split}</p>
<p>注意到在更新参数时，用的不是previous momentum $m_{t-1}$，而是current momentum $m_t$。Adam的参数更新规则为</p>
<p>\begin{split}<br>m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t\\<br>\hat{m}_t &amp; = \frac{m_t}{1 - \beta^t_1}\\<br>\theta_{t+1} &amp;= \theta_{t} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t<br>\end{split}</p>
<p>为了将Nesterov momentum加入Adam中，需要扩展参数更新式：</p>
<p>\begin{split}<br>\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} (\dfrac{\beta_1 m_{t-1}}{1 - \beta^t_1} + \dfrac{(1 - \beta_1) g_t}{1 - \beta^t_1}) \<br>&amp;= \theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} (\beta_1 \hat{m}_{t-1} + \dfrac{(1 - \beta_1) g_t}{1 - \beta^t_1})<br>\end{split}</p>
<p>将$\hat{m}_{t-1}$替换为$\hat{m}_t$，即可得到Nadam的参数更新规则<br>$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} (\beta_1 \hat{m}_t + \dfrac{(1 - \beta_1) g_t}{1 - \beta^t_1})$$<br>详细内容参考论文<a href="http://cs229.stanford.edu/proj2015/054_report.pdf" target="_blank" rel="external">Nadam</a></p>
<h3 id="Visualization-of-Algorithms"><a href="#Visualization-of-Algorithms" class="headerlink" title="Visualization of Algorithms"></a>Visualization of Algorithms</h3><p>&emsp;&emsp;下面两幅图直观地展示了各种优化方法的性能，从左图中可以看出，Adagrad、Adadelta、RMSprop能够朝着正确的方向更新参数，而Momentum和NAG则在一开始偏离了正确路线，但最后均完成了收敛，而且NAG可以较为迅速地校正参数更新方向。右图展示了各种方法在鞍点处的表现，可以看出SGD、Momentum和NAG很难打破鞍点处的平衡(symmetry)，不过Momentum和NAG最终还是能够逃逸出来，而Adagrad、Adadelta、RMSprop则能够迅速朝代价函数的下降方向更新参数。</p>
<p><table><br>  <tr><br>    <td style="padding:1px"><br>      <figure><br>      <img src="https://i.loli.net/2017/09/17/59be6494370fd.gif" style="width: 100%; height: 100%" title="SGD optimization on loss surface contours"><br></figure><br>    </td><br>    <td style="padding:1px"><br>      <figure><br>      <img src="https://i.loli.net/2017/09/17/59be6493e3d8b.gif" style="width: 100%; height: 100%" title="SGD optimization on saddle point"><br></figure><br>    </td><br>  </tr><br></table></p>
<h3 id="Which-Optimizer-to-use"><a href="#Which-Optimizer-to-use" class="headerlink" title="Which Optimizer to use"></a>Which Optimizer to use</h3><ul>
<li>对于稀疏数据，尽可能地选择能够自适应调整学习率(adaptive learning rate)的优化方法，不需手动调节，在设置好默认值的情况下可以实现较好的结果</li>
<li>RMSprop、Adadelta是Adagrad的扩展，解决了Adagrad学习率递减、过早结束训练的问题；RMSprop、Adadelta、Adam在相似的情况下表现差不多，相对而言Adam能够达到更好的效果</li>
<li>SGD通常训练时间更长，可能陷入鞍点，但是在好的参数初始化和学习率调度方案的情况下，结果依然十分可靠</li>
<li>在训练深层或复杂的网络时，如果考虑快速收敛，一般选择Adam这类自适应调整学习率的优化方法</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="external">http://ruder.io/optimizing-gradient-descent/</a></li>
<li><a href="https://arxiv.org/pdf/1609.04747.pdf" target="_blank" rel="external">Paper: An overview of gradient descent optimization algorithms</a></li>
<li><a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="external">http://cs231n.github.io/optimization-1/</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95</a></li>
<li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a></li>
<li><a href="http://blog.mrtanke.com/2016/10/24/An-overview-of-gradient-descent-optimization-algorithms/" target="_blank" rel="external">http://blog.mrtanke.com/2016/10/24/An-overview-of-gradient-descent-optimization-algorithms/</a></li>
<li><a href="http://shuokay.com/2016/06/11/optimization/" target="_blank" rel="external">http://shuokay.com/2016/06/11/optimization/</a></li>
<li><a href="https://blog.slinuxer.com/2016/09/sgd-comparison" target="_blank" rel="external">https://blog.slinuxer.com/2016/09/sgd-comparison</a></li>
<li><a href="https://json0071.gitbooks.io/svm/content/sui_ji_ti_du_xia_jiang.html" target="_blank" rel="external">https://json0071.gitbooks.io/svm/content/sui_ji_ti_du_xia_jiang.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/22252270</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;在机器学习中，模型优化通常会定义一个代价函数(Loss Function)，然后通过最小化代价函数，求得一组参数，例如Logistic Regression、SVM以及神经网络等都属于这类问题，而这类模型往往使用迭代法求解，其中梯度下降法(Gradient Descent)应用最为广泛。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="DL" scheme="https://senitco.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning -- Normalization</title>
    <link href="https://senitco.github.io/2017/09/12/deep-learning-normalization/"/>
    <id>https://senitco.github.io/2017/09/12/deep-learning-normalization/</id>
    <published>2017-09-11T16:00:00.000Z</published>
    <updated>2017-09-18T04:15:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;Normalization可理解为归一化、标准化或者规范化，广泛应用于诸多领域。整体来讲，Normalization扮演着对数据分布重新调整的角色。在图像处理领域，不同形式的归一化可以改变图像的灰度、对比度信息；在机器学习和神经网络中，Normalization可用于对数据去相关，加速模型训练，提高模型的泛化能力。<br><a id="more"></a></p>
<h3 id="Normalization-min-max"><a href="#Normalization-min-max" class="headerlink" title="Normalization(min-max)"></a>Normalization(min-max)</h3><p>&emsp;&emsp;通常意义上的归一化，也是使用较为频繁的一种是对数据按比例缩放，使之分布在一个特定的区间。例如将数据映射到[0, 1]区间，这在数据挖掘和机器学习中较为常见，在原始数据集中，不同维的数据（特征）往往具有不同的量纲或范围，会导致不同特征呈现不同的重要性，为了消除这种影响，将不同维的特征归一化到特定区间，使所有指标处于同一数量级，将归一化的数据应用于后续的算法模型，也会更加高效便捷。归一化的数学表达式如下：<br>$$x = \dfrac{x-x_{min}}{x_{max}-x_{min}}$$<br>如果是映射到特定区间[a,b]，公式如下：<br>$$x = \dfrac{x-x_{min}}{x_{max}-x_{min}} (b-a) + a$$<br>在图像处理中，经常会将像素灰度值映射到区间[0,255]。此外，还有直方图均衡化(Histogram Equalization)、按特定幂函数对图像进行对比度拉伸(Contrast Stretching)等图像预处理操作。</p>
<h3 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h3><p>&emsp;&emsp;z-score标准化方法使数据符合均值为0、标准差为1的分布，数学变换式如下：<br>$$\widehat x = \dfrac{x - \mu}{\sigma}$$<br>式中，$\mu、\sigma$分别为原始数据的均值和标准差。这种标准化方法在一定程度上可以降低数据之间的相关性，并加速训练过程。就图像和视觉领域而言，通常只需要减去均值即可。对于所有图像样本，对相同坐标的像素求取均值，这样便可得到一个均值模板图像，然后让所有图像减去这个均值模板，这是一种比较通用的处理方法；还有一种是对所有图像的所有像素在RGB三个通道求取均值，然后让所有像素的三通道值分别减去对应通道的均值，这种方法有在Faster R-CNN等系列目标检测领域中用到。</p>
<h3 id="ZCA-Whitening"><a href="#ZCA-Whitening" class="headerlink" title="ZCA Whitening"></a>ZCA Whitening</h3><p>&emsp;&emsp;白化(Whitening)的目的是降低数据之间的相关性，对于图像数据而言，相邻像素之间具有很强的相关性，也就是说数据是冗余的。通过白化过程，希望数据具备如下两点性质：</p>
<ul>
<li>特征之间的相关性降低</li>
<li>所有特征具有相同的方差</li>
</ul>
<p>&emsp;&emsp;主成分分析(PCA)一般用于数据降维，其思路是计算数据样本集$X$协方差矩阵的特征值和特征向量，通过选取前k个特征值对应的特征向量组成投影矩阵，将原始数据映射到新的特征空间，这样便去除了数据之间的相关性（具体内容可参考<a href="https://senitco.github.io/2017/05/10/data-dimensionality-reduction/">博文</a>）。白化的第一步操作就是PCA，如下图所示，假设由特征向量组成的变换矩阵为$U$（既可以保留所有特征向量，也可以只取前k个），那么变换后的数据为<br>$$X_{PCA}=U^T X$$</p>
<p><img src="https://i.loli.net/2017/09/14/59ba2ec2b1f8d.png" alt="pca.png"></p>
<p>第二步PCA白化对变换后数据的每一维特征做标准差归一化处理，使得不同特征的方差相同（单位方差），变换式如下：<br>$$X_{PCAwhite} = \dfrac{X_{PCA}}{std(X_{PCA})}$$<br>也可直接采用下式：<br>$$X_{PCAwhite} = \dfrac{X_{PCA}}{\sqrt{\lambda_i + \varepsilon}}$$<br>$\lambda_i$是PCA得到的特征值，$\varepsilon$是正则化项，避免除数为0，一般取值为$\varepsilon \approx 10^{-5}$。PCA白化后数据分布如下图所示：</p>
<p><img src="https://i.loli.net/2017/09/14/59ba31a97d72d.jpg" alt="pca whitening.jpg" title="PCA Whitening"></p>
<p>第三步ZCA白化是在PCA白化的基础上，将数据变换回原来的特征空间，使得白化后的数据尽可能地接近原始数据。<br>$$X_{ZCAwhite} = U X_{PCAwhite}$$</p>
<p><img src="https://i.loli.net/2017/09/14/59ba31ae6663c.jpg" alt="zca whitening.jpg" title="ZCA Whitening"></p>
<p>不同于 PCA白化，当使用ZCA白化时，通常保留数据的全部n个维度，不尝试去降低它的维数。关于图像的协方差矩阵和ZCA白化可参考<a href="http://218.199.87.242/cache/12/03/www.cs.toronto.edu/cb468cfcbdb22c46e3e49e2ba3c72198/learning-features-2009-TR.pdf" target="_blank" rel="external">Learning Multiple Layers of Features from Tiny Images</a></p>
<h3 id="Global-Contrast-Normalization-GCN"><a href="#Global-Contrast-Normalization-GCN" class="headerlink" title="Global Contrast Normalization(GCN)"></a>Global Contrast Normalization(GCN)</h3><p>&emsp;&emsp;全局对比度标准化(GCN)就是计算图像中所有像素的均值和标准差，然后每个像素分别减去权值并除以标准差，GCN和ZCA Whitening通常一起使用。</p>
<h3 id="Local-Contrast-Normalization-LCN"><a href="#Local-Contrast-Normalization-LCN" class="headerlink" title="Local Contrast Normalization(LCN)"></a>Local Contrast Normalization(LCN)</h3><p>&emsp;&emsp;局部对比度标准化(LCN)是通过计算图像中局部邻域的均值和标准差来进行标准化，可参考论文<a href="http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf" target="_blank" rel="external">What is the Best Multi-Stage Architecture for Object Recognition</a>，具体流程如下：</p>
<ul>
<li>对图像中的像素，考虑一个$3 \times 3$的邻域</li>
<li>计算邻域窗口内所有像素的均值(mean)和标准差(std)，并让中心像素减去均值</li>
<li>比较标准差与1的大小，如果大于1，则中心像素值（减去均值后）除以标准差作为标准化后的最终值；否则，直接将减去均值后的中心像素值作为最终值</li>
<li>遍历图像中的所有像素，按照上述流程分别进行局部标准化</li>
</ul>
<p>在实际处理过程中，可以考虑不同窗口大小的邻域，也可利用加权的方式求取邻域像素的均值。如果在卷积神经网络中，存在多个特征图(feature map)的情况，可以同时考虑空间邻域和特征邻域，例如相邻两个特征图的$3 \times 3$邻域，则每个像素有$3 \times 3 \times 3 -1 = 26$个邻域像素点。LCN可以增强某些特征图的特征表达能力，而限制另外一些特征图的特征表达(?)。</p>
<h3 id="Local-Response-Normalization-LRN"><a href="#Local-Response-Normalization-LRN" class="headerlink" title="Local Response Normalization(LRN)"></a>Local Response Normalization(LRN)</h3><p>&emsp;&emsp;局部响应标准化(LRN)这一概念是在<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">AlexNet</a>中提出的，受到LCN的启发，通过考虑相邻特征图进行标准化，数学表达式如下：<br>$$b_{x,y}^i = a_{x,y}^i / (k + \alpha \Sigma_{j=max(0,i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2)^\beta$$<br>$a_{x,y}^i$是第$i$个卷积核在特征图中坐标$(x,y)$处的激活输出，$b_{x,y}^i$为对应的LRN输出值，$N$是卷积核或者特征图的数量，$n$是考虑的邻域特征图个数，论文中取值为5，$k、\alpha、\beta$是超参数，分别取值为$k=2,\alpha=10^{-4},\beta=0,75$。LRN的流程图如下：</p>
<p><img src="https://i.loli.net/2017/09/14/59ba46a4b1b6c.jpg" alt="local_response_normalization_process.jpg" title="LRN流程图"></p>
<p>为了使LRN输入输出的特征图数量一致，需要对特征图进行扩充(padding)，两端分别增加$n/2$个特征图，可直接拷贝相邻特征图。后续有研究表明，LRN在实际模型中发挥的作用不大，因而使用逐渐变少。</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>&emsp;&emsp;在训练神经网络模型时，由于网络参数在不断更新，每一层的输入分布也在不断变化，这迫使网络设置更小的学习率，导致更慢的训练速度，而且对参数初始化的要求也较高，这种现象称为Internal Convariate Shift。而Batch Normalization通过对每层的输入进行归一化，可以有效地缓解这个问题，网络可以使用较大的学习率，参数初值也无需刻意设置，并且可以起到正则化(Regularization)的作用，减弱了对Dropout的需求，保证了更快的训练速度和更精确的网络模型。</p>
<h4 id="Why-need-BN"><a href="#Why-need-BN" class="headerlink" title="Why need BN"></a>Why need BN</h4><p>&emsp;&emsp;在神经网络中，最基本的参数优化方法就是随机梯度下降(SGD)，通过最小化代价函数来求得一组模型参数<br>$$\theta = \underset{\theta}{\mathrm{arg min}} \dfrac{1}{N} \sum_{i=1}^N{\mathcal L\left(x_i, \theta\right)}$$<br>在实际训练过程中，通常采用minibatch的方法，即每次迭代训练一批数据，梯度计算如下：<br>$$\dfrac{1}{m}\sum \dfrac{\partial{\mathcal L \left(x_i, \theta\right)}}{\partial \theta}$$<br>minibatch可以近似整个训练集的梯度，并且可以并行地计算m个样本，比单独计算每个样本要快。但SGD仍然存在一些固有缺陷：</p>
<ul>
<li>学习率比较难设置，为了尽量避免梯度弥散(Gradiant Vanish)，只能设置较小的学习率，导致训练缓慢</li>
<li>对权值参数的初始值要求较高，否则会造成收敛困难</li>
<li>每层的输入受到前面参数的影响，前面很小的变动，随着网络层数增加会不断被放大</li>
</ul>
<p>&emsp;&emsp;参数的更新会导致各层输入数据分布的变化，每次迭代，网络中的每一层需要学习拟合不同的数据分布，这样会显著降低网络的训练速度。当一个学习系统的输入分布是变化的，例如训练集和测试集的样本分布不一致，训练的模型就很难有较好的泛化能力。通常在将原始数据输入到网络中训练时，都会有一个归一化的预处理，也就是使得数据分布的均值为0、方差为1的近似白化过程。而Batch Normalization正是将这种归一化的方法应用到网络的每一层中，这样每一层的输入分布都不再受到前面参数变化的影响，该层网络也不需要适应输入分布的变化。</p>
<p>&emsp;&emsp;考虑到网络使用饱和非线性的激活函数例如Sigmoid，参数的变化可能会导致输入大量分布于饱和区域，局部梯度趋近于0，在反向传播的时候可能会出现梯度弥散的情况，进一步导致前面参数更新较慢或者停止更新，模型收敛困难。ReLU激活函数、合适的初值以及较小的学习率可以在一定程度上解决梯度消失的问题，而采用Batch Normalization的方法使每一层的输入分布更稳定，哪怕是使用Sigmoid激活函数，输入也不太可能分布于饱和区域，降低了模型对激活函数选择的依赖。</p>
<p>&emsp;&emsp;在传统的深度网络中，过高的学习率会导致梯度爆炸或消失(explode or vanish)，以及陷入局部极值，BN可以防止参数变化的影响逐层放大，避免陷入饱和区域。而且BN使得训练对参数的尺度(scale)更加鲁棒，一般高学习率会增加参数的scale，在反向传播中放大梯度，导致模型爆炸。在进行BN之后，<br>$$BN(wu) = BN((\alpha w)u)$$<br>$$\frac{\partial BN((\alpha w)u)}{\partial u} = \frac{\partial BN(wu)}{\partial u}$$<br>$$\frac{\partial BN((\alpha w)u)}{\partial (\alpha w)} = \frac{1}{\alpha} \frac{\partial BN(wu)}{\partial w}$$<br>可以看出较大的$w$将获得较小的梯度，也就是说较大的权值更新较小，较小的权值更新较大。因此，BN使得权重的更新更加稳健，反向传播的梯度不受参数scale的影响，解决了梯度爆炸或梯度消失的问题。</p>
<h4 id="What-is-BN"><a href="#What-is-BN" class="headerlink" title="What is BN"></a>What is BN</h4><p>&emsp;&emsp;Batch Normalization作用在每一层的输入，也就是激活函数（非线性变换）之前，而不是前一层的输出，BN的算法流程如下：</p>
<p><img src="https://i.loli.net/2017/09/16/59bce0018d695.jpg" alt="batch-norm.jpg" title="Batch Normalizing Transform"></p>
<p>BN Transform在对输入数据做0均值、单位方差的归一化处理后，又进行了一次线性变换，引入了一对可学习的参数$\gamma、\beta$，提升了模型的容纳能力(capacity)，如果$\gamma=\sqrt{\sigma_{\beta}^2}、\beta=\mu_{\beta}$，这样$y_i$就恢复成原来的输入分布$x_i$。 至于是否需要对$y_i$进行还原，则由网络模型自动从训练数据中学习决定。BN层的梯度反向传播公式如下：</p>
<p><img src="https://i.loli.net/2017/09/16/59bce41fc966a.jpg" alt="bn-bp.jpg" title="bn-bp.jpg"></p>
<p>训练结束将模型用于测试时，均值$\mu$和标准差$\sigma$是固定的，均值为训练阶段所有Batch均值的统计平均，方差为所有训练Batch方差的无偏估计，公式如下：<br>$$E[x]=E_{B}[\mu _B],&emsp;Var[x]=\dfrac{m}{m-1}E_{B}[\sigma_B^2]$$<br>在测试阶段，BN变换式为<br>$$y=\dfrac{\gamma}{\sqrt{Var[x]+\varepsilon}}x+(\beta-\dfrac{\gamma E[x]}{\sqrt{Var[x]+\varepsilon}})$$</p>
<p>整个训练和测试流程如下：</p>
<p><img src="https://i.loli.net/2017/09/16/59bce867e253a.jpg" alt="bn-inference.jpg" title="Training and Inference BN Network"></p>
<p>BN变换置于网络激活函数层的前面，对于普通前馈神经网络，BN是对一层的每个神经元进行归一化处理；而在卷积神经网络中，BN则是对一层的每个特征图进行处理，类似于权值共享，一个特征图只有一对可学习参数$\gamma、\beta$，如果某一层的输入张量为$[batch\_size, height, width, channels]$，则BN变换是按照channels进行的，minibatch的大小相当于$batch\_size \times height \times width$。</p>
<h4 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h4><p>总体来说，Batch Normalization主要有如下优点：</p>
<ul>
<li>保证网络各层输入的均值和方差稳定（数据分布不一定一致？），减弱Internal Convariate Shift的影响</li>
<li>减小参数scale和初值对梯度的影响，解决了梯度爆炸或梯度消失的问题</li>
<li>在某种程度上实现了正则化，减弱对L2-norm、Dropout的依赖</li>
<li>让输入分布于饱和区域(Sigmoid)的概率降低</li>
<li>降低超参数选择对模型的影响，可使用较大的学习率加速训练</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="external">Paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf" target="_blank" rel="external">Paper: Local Contrast Normalization: What is the Best Multi-Stage Architecture for Object Recognition?</a></li>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">Paper: ImageNet Classification with Deep Convolutional Neural Networks</a></li>
<li><a href="http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/" target="_blank" rel="external">http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/</a></li>
<li><a href="https://calculatedcontent.com/2017/06/16/normalization-in-deep-learning/" target="_blank" rel="external">https://calculatedcontent.com/2017/06/16/normalization-in-deep-learning/</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96</a></li>
<li><a href="http://www.csuldw.com/2015/11/15/2015-11-15%20normalization/" target="_blank" rel="external">http://www.csuldw.com/2015/11/15/2015-11-15%20normalization/</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="external">http://blog.csdn.net/hjimce/article/details/50866313</a></li>
<li><a href="https://stats.stackexchange.com/questions/117427/what-is-the-difference-between-zca-whitening-and-pca-whitening" target="_blank" rel="external">https://stats.stackexchange.com/questions/117427/what-is-the-difference-between-zca-whitening-and-pca-whitening</a></li>
<li><a href="http://shuokay.com/2016/05/28/batch-norm/" target="_blank" rel="external">http://shuokay.com/2016/05/28/batch-norm/</a></li>
<li><a href="http://jiangqh.info/Batch-Normalization%E8%AF%A6%E8%A7%A3/" target="_blank" rel="external">http://jiangqh.info/Batch-Normalization%E8%AF%A6%E8%A7%A3/</a></li>
<li><a href="http://blog.csdn.net/u012816943/article/details/51691868" target="_blank" rel="external">http://blog.csdn.net/u012816943/article/details/51691868</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="external">http://blog.csdn.net/hjimce/article/details/50866313</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;Normalization可理解为归一化、标准化或者规范化，广泛应用于诸多领域。整体来讲，Normalization扮演着对数据分布重新调整的角色。在图像处理领域，不同形式的归一化可以改变图像的灰度、对比度信息；在机器学习和神经网络中，Normalization可用于对数据去相关，加速模型训练，提高模型的泛化能力。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="DL" scheme="https://senitco.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning -- Regularization</title>
    <link href="https://senitco.github.io/2017/09/10/deep-learning-regularization/"/>
    <id>https://senitco.github.io/2017/09/10/deep-learning-regularization/</id>
    <published>2017-09-09T16:00:00.000Z</published>
    <updated>2017-09-18T04:15:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;在训练神经网络时，为了缓解网络规模较大、训练数据较少而可能导致的过拟合(Overfitting)问题，通常会采取正则化(Regularization)方法，以提高模型的泛化能力。<br><a id="more"></a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>&emsp;&emsp;过拟合具体表现如下图所示，在迭代训练的过程中，模型复杂度增加，在训练集上的错误率降低，而在验证集上反而上升，也就是说，模型过拟合了训练集，在除训练集以外的其他数据集上表现较差。有较多的方法可用来缓解过拟合问题，例如减小网络规模，扩增数据集(Date Argumentation)、正则化(Regulariztion)、提前终止(Early stopping)等。由于较大的网络规模通常具有较强的模型表达能力，所以减小网络规模以缓解过拟合并不十分可取。正则化则是一种使用较为广泛的方法，主要有L1-norm、L2-norm、Dropout。</p>
<p><img src="https://i.loli.net/2017/09/13/59b9296eb2674.png" alt="overfitting.png" title="Overfitting"></p>
<h3 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h3><p>&emsp;&emsp;L2正则也称为权重衰减(Weights Decay)，直接在代价函数(Loss Function)后面增加一个L2范数的正则化项，也就是对网络中所有权值求平方和然后开方。数学表达式如下：<br>$$C = C_0 + \dfrac{\lambda}{2n} \Sigma_w w^2$$<br>式中，$C_0$为原始代价函数，$n$为训练集大小，$\lambda$是两项的平衡系数。L2正则项通过惩罚(penalizes)权重的平方，倾向于让网络学习到更小的权值。添加正则项后的代价函数对网络参数的梯度为<br>$$\dfrac{\partial C}{\partial w} = \dfrac{\partial C_0}{\partial w} + \dfrac{\lambda}{n}w,&emsp;\dfrac{\partial C}{\partial b} = \dfrac{\partial C_0}{\partial b}$$<br>梯度反向传播用于更新网络权值<br>$$w = w - \eta (\dfrac{\partial C_0}{\partial w} + \dfrac{\lambda}{n}w) = (1-\dfrac{\eta \lambda}{n})w - \eta \dfrac{\partial C_0}{\partial w}$$<br>$$b = b - \eta \dfrac{\partial C_0}{\partial b}$$<br>式中，$\eta$为学习率(learning rate)，$w$的系数$(1-\dfrac{\eta \lambda}{n})$使得权重相比未正则化时减小，这也是$L2-norm$称作权重衰减的原因。<br>&emsp;&emsp;更小的权重，如何保证能够缓解过拟合？从某种意义上，权重越小表示模型复杂度更低，对数据的拟合刚好。在实际应用中，也验证了L2正则化的效果往往更好。过拟合的时候，拟合函数的系数通常较大，如下图所示，拟合函数需要顾及每个数据点，因此波动较大。在一些较小的区间内，函数值变化剧烈，意味着函数在这些小区间内的导数值（绝对值）非常大。由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。而L2正则化通过约束权值参数的范数使其不要太大，可以在一定程度上减少过拟合情况。</p>
<p><img src="https://i.loli.net/2017/09/13/59b93367e4f7f.png" alt="regular2.png"></p>
<h3 id="L1-Regularization"><a href="#L1-Regularization" class="headerlink" title="L1 Regularization"></a>L1 Regularization</h3><p>&emsp;&emsp;L1正则是在原始的代价函数后增加一个L1范数的正则化项，即所有权重绝对值的和。数学表达式如下：<br>$$C = C_0 + \dfrac{\lambda}{n} \Sigma_w w$$<br>梯度计算公式为<br>$$\dfrac{\partial C}{\partial w} = \dfrac{\partial C_0}{\partial w} + \dfrac{\lambda}{n}sgn(w)$$<br>式中$sgn(w)$表示权重的符号，权重更新规则为<br>$$w = w - \eta (\dfrac{\partial C_0}{\partial w} + \dfrac{\lambda}{n}sgn(w))$$<br>L1正则化对梯度的影响不再与权重$w$线性相关，而是一个常量因子。当$w$为正时，更新后的$w$变小；当$w$为负时，更新后的$w$变大。因此L1正则化的效果就是让$w$往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。与L2-norm相比，L1-norm会导致参数稀疏，因此可用于特征选取，参数被惩罚为0的特征可以被丢弃。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>&emsp;&emsp;L1、L2正则化是通过修改代价函数来实现的，而Dropout则是修改神经网络本身，使得网络中的部分神经元以一定概率停止工作，具体内容可参考博文<a href="https://senitco.github.io/2017/09/12/deep-learning-dropout/">Dropout</a></p>
<h3 id="Data-Argumentation"><a href="#Data-Argumentation" class="headerlink" title="Data Argumentation"></a>Data Argumentation</h3><p>&emsp;&emsp;引起过拟合的一个因素就是数据量较少，扩增数据集，可以适应更深的网络模型，训练表达能力更强的模型。数据扩增的方法主要有：</p>
<ul>
<li>沿水平方向将图像左右翻转</li>
<li>平移、缩放、旋转或者随机裁剪图像</li>
<li>添加随机噪声</li>
</ul>
<h3 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h3><p>&emsp;&emsp;过拟合在验证集上表现为模型的错误率降到最低后上升，因此提前终止就是在模型的错误率降到最低，性能变坏之前停止迭代训练，如下图所示。当然在实际训练过程中，并不需要如此操作，而且通过设置checkpoints，每迭代一定次数保存模型，最后选择最优的作为最终模型。</p>
<p><img src="https://i.loli.net/2017/09/13/59b93c6685cc0.png" alt="early stopping.png"></p>
<p>关于数据集的划分：<br>&emsp;&emsp;一般数据集分为三部分：训练集(training data)、验证集(validation data)、测试集(testing data)。训练集用于在迭代训练中计算梯度、更新网络权值。验证集则在训练中评估模型的性能，用于确定一组模型的超参数，如学习率等；也就是说通过验证集，需要确定一组最优的超参数，以及在该组超参数下训练出的网络模型。而测试集则用于评估最终的模型性能，既不参与训练，也不用于选取超参数。验证集和测试集容易搞混，如果直接将测试集作为验证集使用，随着训练的进行，网络实际上在拟合测试集中的数据，那么测试集对网络的评估没有任意意义，因为最终用于评估模型的数据集应当是一个未知数据集，不参与任何训练过程。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://chatbotslife.com/regularization-in-deep-learning-f649a45d6e0" target="_blank" rel="external">https://chatbotslife.com/regularization-in-deep-learning-f649a45d6e0</a></li>
<li><a href="http://www.deeplearningbook.org/contents/regularization.html" target="_blank" rel="external">http://www.deeplearningbook.org/contents/regularization.html</a></li>
<li><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s5ss1.html" target="_blank" rel="external">https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s5ss1.html</a></li>
<li><a href="http://shartoo.github.io/regularization-deeplearning/" target="_blank" rel="external">http://shartoo.github.io/regularization-deeplearning/</a></li>
<li><a href="http://blog.csdn.net/u012162613/article/details/44261657" target="_blank" rel="external">http://blog.csdn.net/u012162613/article/details/44261657</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;在训练神经网络时，为了缓解网络规模较大、训练数据较少而可能导致的过拟合(Overfitting)问题，通常会采取正则化(Regularization)方法，以提高模型的泛化能力。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="DL" scheme="https://senitco.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning -- Dropout</title>
    <link href="https://senitco.github.io/2017/09/08/deep-learning-dropout/"/>
    <id>https://senitco.github.io/2017/09/08/deep-learning-dropout/</id>
    <published>2017-09-07T16:00:00.000Z</published>
    <updated>2017-09-18T04:15:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;训练神经网络模型时(Nerual Network)，如果网络规模较大，训练样本较少，为了防止模型过拟合，通常会采用Regularization(正则化，e.g. L2-norm、Dropout)。Dropout的基本思想是在模型训练时，让某些神经元以一定的概率不工作。<br><a id="more"></a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>&emsp;&emsp;深度学习领域大神Hinton在2012的一篇文献<a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="external">《Improving neural networks by preventing co-adaptation of feature detectors》</a>中提出，在每次训练的时候，让一半的特征检测器停止工作，可以提高网络模型的泛化能力。Hinton认为过拟合可通过阻止某些特征的协同作用来缓解，并将这种方法称之为Dropout。<br>&emsp;&emsp;Dropout的工作原理是在模型的迭代训练过程中，让网络中的某些隐层节点(神经元)以一定概率不工作，相当于去除部分节点，但节点的权重会保留下来，只是暂时不更新。每次迭代去除的节点不一样，这样相当于每次训练不同的网络，而且实现了网络的权值共享。</p>
<h3 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h3><p>&emsp;&emsp;在训练一个普通的多层神经网络时，其流程是：将输入样本通过网络前向传播计算每次的激活值，并得到最后的Loss Function，然后反向传播误差并更新每层的权值参数。使用Dropout后，网络模型变化如下图所示：</p>
<p><img src="https://i.loli.net/2017/09/12/59b79168a4cf7.jpg" alt="dropout.jpg"></p>
<p>如何实现让神经元以一定概率停止工作，可参考以下公式，一般情况下，网络的前向计算公式如下：</p>
<p>$$z_i ^{(l+1)} = w_i ^{(l+1)} y^{(l)} + b_i ^{(l+1)}$$<br>$$y_i ^{(l+1)} = f(z_i ^{(l+1)})$$<br>采用Dropout后计算公式变成：<br>$$r_j ^{(l)} \sim Bernoulli(p),&emsp;\overline{y}^{(l)} = r^{(l)} \ast y^{(l)}$$<br>$$z_i ^{(l+1)} = w_i ^{(l+1)} \overline{y}^{(l)} + b_i ^{(l+1)},&emsp;y_i ^{(l+1)} = f(z_i ^{(l+1)})$$<br>上式中，$r_j ^{(l)}$服从概率为$p$的伯努利二项分布，$r^{(l)}$为生成的0、1向量。通过将激活值置0，实现了网络中$l$层部分节点停止工作，而$l+1$层的输入中，也只考虑了$l$层中非0的节点（其实考虑了所有节点，只是输出为0的结点对下一层网络不起作用，在反向传播时，也无法更新和其相关的网络权值）。关于Dropout还有几点需要注意：</p>
<ul>
<li>如果网络某一层的神经元个数为1000，dropout的比例为0.4(p=0.6)，那么经过Dropout操作后，大约会有400个神经元的激活值会置0</li>
<li>每次训练迭代，都会重新随机选取一定比例的节点激活值置0，因此每次删除的节点会有差异</li>
<li>反向传播时，非0节点的权值参数得到更新，而0节点的权值保持不变</li>
<li>删除某些节点使其激活值为0后，需要对其他节点进行rescale，也就是乘以$1/p$(p是保留节点的比例)；如果训练时没有rescale，测试时则需要对权重rescale，$W_{test}=pW$</li>
</ul>
<h3 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h3><p>&emsp;&emsp;关于Dropout能够缓解过拟合、提高模型泛化能力的几点直观解释：</p>
<ul>
<li>减少神经元之间的共适应关系：在迭代训练时，隐层节点以一定概率随机出现(或消失)，因此不能保证每两个节点同时出现，这样权值的更新不再依赖于有固定关系的隐层节点，阻止了某些特征在其他特征下才有效果的情况，迫使网络学习更鲁棒的特征</li>
<li>模型平均作用：Dropout删除不同的神经元，类似于每次训练不同的网络，整个训练过程相当于对不同的模型取平均，而且不同的网络结构共享权值参数</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>&emsp;&emsp;Dropout比较适用于大型网络缓解过拟合，能够取得明显的效果，但网络的训练速度也会明显变慢，不过对测试没有影响。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="external">Paper: Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">Paper: ImageNet Classification with Deep Convolutional Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="external">Paper: Improving neural networks by preventing co-adaptation of feature detectors</a></li>
<li><a href="http://www.cs.toronto.edu/~nitish/msc_thesis.pdf" target="_blank" rel="external">Paper: Improving Neural Networks with Dropout</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/50413257" target="_blank" rel="external">http://blog.csdn.net/hjimce/article/details/50413257</a></li>
<li><a href="http://www.cnblogs.com/tornadomeet/p/3258122.html" target="_blank" rel="external">http://www.cnblogs.com/tornadomeet/p/3258122.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/23178423" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/23178423</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;训练神经网络模型时(Nerual Network)，如果网络规模较大，训练样本较少，为了防止模型过拟合，通常会采用Regularization(正则化，e.g. L2-norm、Dropout)。Dropout的基本思想是在模型训练时，让某些神经元以一定的概率不工作。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="DL" scheme="https://senitco.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning -- Activation Function</title>
    <link href="https://senitco.github.io/2017/09/05/deep-learning-activation-function/"/>
    <id>https://senitco.github.io/2017/09/05/deep-learning-activation-function/</id>
    <published>2017-09-04T16:00:00.000Z</published>
    <updated>2017-09-18T04:15:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;神经网络的激活函数(activation function)通过引入非线性因素，使得网络可以逼近任何非线性函数，提高网络模型的表达能力，更好地解决复杂问题。<br><a id="more"></a></p>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>&emsp;&emsp;激活函数通常具有以下性质：</p>
<ul>
<li>非线性：使用非线性激活函数的多层神经网络可以逼近所有函数</li>
<li>可微性：对于常见的优化方法——梯度下降法，可微性是必要的</li>
<li>单调性：单调激活函数能够保证单层网络是凸函数</li>
<li>输出范围：激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当输出值的范围无界时，模型训练会更加高效，不过这种情况下一般需要更小的学习率(learning rate)，以保证收敛</li>
</ul>
<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>&emsp;&emsp;Sigmoid的数学公式为$f(x)=\dfrac{1}{1+e^{-x}}$，将输入映射到区间(0, 1)，函数曲线如下图所示:</p>
<p><img src="https://i.loli.net/2017/09/12/59b7cbe14ff94.jpeg" alt="sigmoid.jpeg" title="Sigmoid函数曲线"></p>
<p>Sigmoid函数曾被广泛使用，但现在使用较少，主要是存在以下缺点：</p>
<ul>
<li>函数饱和造成梯度消失(Sigmoids saturate and kill gradients)：神经元的激活值在趋近0或1时会饱和，在这些区域梯度值几乎为0，而且梯度值非0的输入范围非常有限。在反向传播时，此处局部梯度值将与损失函数关于该神经元输出的梯度相乘，如果局部梯度非常小，那么相乘的结果也会趋近于0，造成梯度消失，使得前面网络的权值参数无法更新。为了防止饱和，初始化权重不易过大，否则大多数神经元将会饱和，导致网络难以学习。</li>
<li>Sigmoid输出不是0均值(Sigmoid outputs are not zero-centered)：这一性质会导致后面网络层得到的输入数据不是零中心的，影响梯度下降的运作。因为如果输入神经元的数据总是正数（比如在$f=w^T x + b$中每个元素都$x&gt;0$），那么关于$w$的梯度在反向传播的过程中，要么全部是正数，要么全部是负数（具体依整个表达式$f$而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，如果是按batch去训练，那么每个batch可能得到不同的信号，整个批量的梯度加起来后，对于权重的最终更新将会有不同的正负，在一定程度上减轻了这个问题。</li>
</ul>
<p>此外，Sigmoid函数涉及到指数运算，增加了计算量。</p>
<h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><p>&emsp;&emsp;双曲正切函数的数学表达式为$f(x)=\dfrac{1-e^{-2x}}{1+e^{-2x}}=2Sigmoid(2x)-1$，函数曲线如下图所示，输出值的范围为(-1, 1)</p>
<p><img src="https://ooo.0o0.ooo/2017/09/12/59b7cbe14f982.jpeg" alt="tanh.jpeg" title="tanh函数曲线"></p>
<p>tanh函数同样存在饱和和梯度消失问题，但输出是0均值的，因此在一定程度上，性能略优于Sigmoid。</p>
<h3 id="Rectified-Linear-Units-ReLU"><a href="#Rectified-Linear-Units-ReLU" class="headerlink" title="Rectified Linear Units(ReLU)"></a>Rectified Linear Units(ReLU)</h3><p>&emsp;&emsp;ReLU应用较为广泛，其数学表达式为$f(x)=max(0,x)$，函数曲线如左下图所示</p>
<p><img src="https://i.loli.net/2017/09/12/59b7d5f56e5eb.jpg" alt="relu_alexplot.jpg" title="ReLU函数曲线(左)，迭代训练示意图(右)"></p>
<p>ReLU激活函数主要有如下优缺点：</p>
<ul>
<li>(+)相比于Sigmoid和tanh，ReLU对于随机梯度下降(SGD)的收敛有显著的加速作用（在AlexNet中，比tanh收敛快6倍）。据称这是由其(分段)线性、非饱和导致的</li>
<li>(+)Sigmoid、tanh包含指数运算，耗费计算资源，而ReLU通过和阈值比较即可得到激活值，不涉及复杂运算</li>
<li>(-)ReLU的缺点是在训练时神经元比较脆弱，可能会“死掉”。当一个很大的梯度反向传播经过ReLU神经元时，可能会导致权值更新过后，对任何数据都不再出现激活现象，所有流过该神经元的梯度都将变为0。也就是说，ReLU单元在训练中将不可逆转的死亡，导致数据多样性的丢失。实际上，如果学习率设置得过高，网络中约40%的神经元都会死掉，在整个训练集中都不会再激活。因此需要合理设置学习率。</li>
</ul>
<p>$w$是二维时，ReLU的效果如图：</p>
<p><img src="https://i.loli.net/2017/09/12/59b7dab7b55ed.png" alt="relu-perf.png" width="80%" height="80%" align="center"></p>
<h3 id="leaky-ReLU、P-ReLU、R-ReLU、ELU"><a href="#leaky-ReLU、P-ReLU、R-ReLU、ELU" class="headerlink" title="leaky-ReLU、P-ReLU、R-ReLU、ELU"></a>leaky-ReLU、P-ReLU、R-ReLU、ELU</h3><p>&emsp;&emsp;leaky-ReLU是用于解决ReLU中神经元死亡的尝试方案，其数学公式如下：<br>$$f(x)=\begin{cases} \alpha x,&emsp;x&lt;0 \\ x,&emsp;x \geq 0 \end{cases}$$<br>$\alpha$是一个很小的常数，可取值为0.01。有研究论文指出，leaky-ReLU激活函数的效果不错，但不是很稳定。Kaiming He等人在2015年发布的论文<a href="https://arxiv.org/pdf/1502.01852.pdf" target="_blank" rel="external">Delving Deep into Rectifiers</a>中介绍了一种新方法Parametric ReLU，把负区间上的斜率$\alpha$当做每个神经元中的一个参数来训练，然而该激活函数在在不同任务中表现的效果也没有特别清晰。在另外一个版本Randomized ReLU中</p>
<p>$$y_{ji}=\begin{cases} a_{ji}x_{ji},&emsp;x_{ji}&lt;0 \\ x_{ji},&emsp;&emsp;x_{ji} \geq 0 \end{cases}$$</p>
<p>在训练过程中，$a_{ji}$是从一个高斯分布$U(l,u)$中随机选取的；在测试阶段是固定的，将训练过程中的所有$a_{ji}$取平均值，测试阶段激活函数为$y_{ji}=\dfrac{x_{ji}}{(l+u)/2}$。此外，还有一个ELU版本，公式定义如下(式中$a&gt;0$)，相关内容可参考文献<a href="https://arxiv.org/pdf/1511.07289v5.pdf" target="_blank" rel="external">ELU</a><br>$$f(x)=\begin{cases} a(e^x-1),&emsp;x&lt;0 \\ x,&emsp;&emsp;&emsp;&emsp;x \geq 0 \end{cases}$$</p>
<p><img src="https://i.loli.net/2017/09/12/59b7e39e458d4.jpg" alt="elu.jpg" width="60%" height="60%" align="center"></p>
<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>&emsp;&emsp;Maxout源于大神Goodfellow在2013年发表的一篇论文<a href="https://arxiv.org/pdf/1302.4389.pdf" target="_blank" rel="external">Maxout Network</a>，可以将其看作网络中的激活函数层。假设网络某一层的输入特征向量为$x=(x_1,x_2,…,x_d) \in R^d$，Maxout隐层神经元的计算公式如下：<br>$$h_j(x)=max_{j \in [1,k]} z_{ij}$$<br>$$z_{ij}=x^TW_{…ij}+b_{ij}$$<br>式中，$W \in R^{d \times m \times k},b \in R^{m \times k}$，是需要学习的参数，$k$是Maxout层所需要的参数。对于传统的MLP算法，从第$l$层到第$l+1$层的某个神经元，其输入为$x_i ^{(l+1)} = w_i ^{(l+1)} y^{(l)}+b_i ^{l+1}$，对第$l$层每个神经元，原本只需要学习一组参数，引入了Maxout后，需要训练$k$组，并从中选取最大的输入值作为该神经元的激活值，相当于激活函数是一个分段线性函数。因此，Maxout可以说是ReLU和Leaky-ReLU的一般化归纳。Maxout具备ReLU的优点（线性、非饱和），而没有其缺点（神经元死亡）。Maxout在MLP网络和卷积网络中均可以使用，而且其参数是可学习的，激活函数并不固定。Maxout的本质就是一个线性分段函数，可以拟合任意的凸函数（“隐隐含层”节点数k足够大时），如下图所示。和其他激活函数相比，Maxout存在参数激增的现象(k倍)。</p>
<p><img src="https://i.loli.net/2017/09/12/59b7ed563f157.jpg" alt="maxout.jpg"></p>
<p>论文中给出了相关定理：对于任意的一个连续分段线性函数$g(v)$，可以找到两个凸的分段线性函数$h1(v)、h2(v)$，使得这两个凸函数的差值为$g(v)$。</p>
<p><img src="https://i.loli.net/2017/09/13/59b892f878f81.jpg" alt="piecewise linear approximation.jpg"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>&emsp;&emsp;通常来说，在一个网络中很少使用多种激活函数。如果使用ReLU，需要合理设置学习率，避免出现过多死亡神经元，也可以使用leaky-ReLU或者Maxout来解决该问题。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-1/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21462488" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21462488</a></li>
<li><a href="http://blog.csdn.net/cyh_24/article/details/50593400" target="_blank" rel="external">http://blog.csdn.net/cyh_24/article/details/50593400</a></li>
<li><a href="http://blog.csdn.net/hjimce/article/details/50414467" target="_blank" rel="external">http://blog.csdn.net/hjimce/article/details/50414467</a></li>
<li><a href="https://arxiv.org/pdf/1302.4389.pdf" target="_blank" rel="external">Paper: Maxout Networks</a></li>
<li><a href="https://arxiv.org/pdf/1502.01852.pdf" target="_blank" rel="external">Paper: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
<li><a href="https://github.com/philipperemy/tensorflow-maxout" target="_blank" rel="external">Code: Tensorflow implement of Maxout</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;神经网络的激活函数(activation function)通过引入非线性因素，使得网络可以逼近任何非线性函数，提高网络模型的表达能力，更好地解决复杂问题。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="DL" scheme="https://senitco.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Faster R-CNN论文及源码解读</title>
    <link href="https://senitco.github.io/2017/09/02/faster-rcnn/"/>
    <id>https://senitco.github.io/2017/09/02/faster-rcnn/</id>
    <published>2017-09-01T16:00:00.000Z</published>
    <updated>2017-09-18T04:15:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;R-CNN是目标检测领域中十分经典的方法，相比于传统的手工特征，R-CNN将卷积神经网络引入，用于提取深度特征，后接一个分类器判决搜索区域是否包含目标及其置信度，取得了较为准确的检测结果。Fast R-CNN和Faster R-CNN是R-CNN的升级版本，在准确率和实时性方面都得到了较大提升。在Fast R-CNN中，首先需要使用Selective Search的方法提取图像的候选目标区域(Proposal)。而新提出的Faster R-CNN模型则引入了RPN网络(Region Proposal Network)，将Proposal的提取部分嵌入到内部网络，实现了卷积层特征共享，Fast R-CNN则基于RPN提取的Proposal做进一步的分类判决和回归预测，因此，整个网络模型可以完成端到端的检测任务，而不需要先执行特定的候选框搜索算法，显著提升了算法模型的实时性。<br><a id="more"></a></p>
<h3 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h3><p>&emsp;&emsp;Faster R-CNN模型主要由两个模块组成：RPN候选框提取模块和Fast R-CNN检测模块，如下图所示，又可细分为4个部分；Conv Layer，Region Proposal Network(RPN)，RoI Pooling，Classification and Regression。</p>
<p><img src="https://i.loli.net/2017/09/03/59abba05e7eca.jpg" alt="Faster R-CNN.jpg" title="Faster R-CNN网络模型" width="40%" height="40%" align="center"></p>
<ul>
<li>Conv Layer: 卷积层包括一系列卷积(Conv + Relu)和池化(Pooling)操作，用于提取图像的特征(feature maps)，一般直接使用现有的经典网络模型ZF或者VGG16，而且卷积层的权值参数为RPN和Fast RCNN所共享，这也是能够加快训练过程、提升模型实时性的关键所在。</li>
<li>Region Proposal Network: RPN网络用于生成区域候选框Proposal，基于网络模型引入的多尺度Anchor，通过Softmax对anchors属于目标(foreground)还是背景(background)进行分类判决，并使用Bounding Box Regression对anchors进行回归预测，获取Proposal的精确位置，并用于后续的目标识别与检测。</li>
<li>RoI Pooling: 综合卷积层特征feature maps和候选框proposal的信息，将propopal在输入图像中的坐标映射到最后一层feature map(conv5-3)中，对feature map中的对应区域进行池化操作，得到固定大小($ 7 \times 7$)输出的池化结果，并与后面的全连接层相连。</li>
<li>Classification and Regression: 全连接层后接两个子连接层——分类层(cls)和回归层(reg)，分类层用于判断Proposal的类别，回归层则通过bounding box regression预测Proposal的准确位置。</li>
</ul>
<p>&emsp;&emsp;下图为Faster R-CNN测试网络结构(网络模型文件为<a href="https://github.com/rbgirshick/py-faster-rcnn/blob/master/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt" target="_blank" rel="external">faster_rcnn_test.pt</a>)，可以清楚地看到图像在网络中的前向计算过程。对于一幅任意大小$P \times Q$的图像，首先缩放至固定大小$M \times N$(源码中是要求长边不超过1000，短边不超过600)，然后将缩放后的图像输入至采用VGG16模型的Conv Layer中，最后一个feature map为conv5-3，特征数(channels)为512。RPN网络在特征图conv5-3上执行$3 \times 3$卷积操作，后接一个512维的全连接层，全连接层后接两个子连接层，分别用于anchors的分类和回归，再通过计算筛选得到proposals。RoIs Pooling层则利用Proposal从feature maps中提取Proposal feature进行池化操作，送入后续的Fast R-CNN网络做分类和回归。RPN网络和Fast R-CNN网络中均有分类和回归，但两者有所不同，RPN中分类是判断conv5-3中对应的anchors属于目标和背景的概率(score)，并通过回归获取anchors的偏移和缩放尺度，根据目标得分值筛选用于后续检测识别的Proposal；Fast R-CNN是对RPN网络提取的Proposal做分类识别，并通过回归参数调整得到目标(Object)的精确位置。具体的训练过程会在后面详述。接下来会重点介绍RPN网络和Fast R-CNN网络这两个模块，包括RPN网络中引入的Anchor机制、训练数据的生成、分类和回归的损失函数(Loss Function)计算以及RoI Pooling等。</p>
<p><img src="https://i.loli.net/2017/09/03/59abc3ac37ee6.jpg" alt="faster_rcnn_test_model.jpg" title="Fast R-CNN test网络结构" width="80%" height="80%" align="center"></p>
<h3 id="Region-Proposal-Network-RPN"><a href="#Region-Proposal-Network-RPN" class="headerlink" title="Region Proposal Network(RPN)"></a>Region Proposal Network(RPN)</h3><p>&emsp;&emsp;传统的目标检测方法中生成候选框都比较耗时，例如使用滑动窗口加图像金字塔的方式遍历图像，获取多尺度的候选区域；以及R-CNN、Fast R-CNN中均使用到的Selective Search的方法生成候选框。而Faster R-CNN则直接使用RPN网络，将检测框Proposal的提取嵌入到网络内部，通过共享卷积层参数的方式提升了Proposal的生成速度。</p>
<h4 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h4><p>&emsp;&emsp;Anchor是RPN网络中一个较为重要的概念，传统的检测方法中为了能够得到多尺度的检测框，需要通过建立图像金字塔的方式，对图像或者滤波器(滑动窗口)进行多尺度采样。RPN网络则是使用一个$3 \times 3$的卷积核，在最后一个特征图(conv5-3)上滑动，将卷积核中心对应位置映射回输入图像，生成3种尺度(scale)$ \lbrace 128^2,256^2,512^2 \rbrace$和3种长宽比(aspect ratio)$\lbrace 1:1, 1:2, 2:1 \rbrace$共9种Anchor，如下图所示。特征图conv5-3每个位置都对应9个anchors，如果feature map的大小为$W \times H$，则一共有$ W \times H \times 9$个anchors，滑动窗口的方式保证能够关联conv5-3的全部特征空间，最后在原图上得到多尺度多长宽比的anchors。</p>
<p><img src="https://i.loli.net/2017/09/04/59ad0eb24aad9.jpg" alt="anchors.jpg" title="Anchor示意图"></p>
<p>&emsp;&emsp;最后一个feature map后面会接一个全连接层，如下图所示，全连接的维数和feature map的特征数(channels)相同。对于原论文中采用的ZF模型，conv5的特征数为256，全连接层的维数也为256；对于VGG模型，conv5-3的特征数为512，全连接的的维数则为512，相当于feature map上的每一个点都输出一个512维的特征向量。</p>
<p><img src="https://i.loli.net/2017/09/03/59abba05e7273.jpg" alt="RPN.jpg" title="RPN网络结构" width="40%" height="40%" align="center"><br>关于anchors还有几点需要说明：</p>
<ul>
<li>conv5-3上使用了$3 \times 3$的卷积核，每个点都可以关联局部邻域的空间信息。</li>
<li>conv5-3上每个点前向映射得到k(k=9)个anchors，并且后向输出512维的特征向量，而anchors的作用是分类和回归得到Proposal，因此全连接层后须接两个子连接层————分类层(cls)和回归层(reg)，分类层用于判断anchors属于目标还是背景，向量维数为2k；回归层用于计算anchors的偏移量和缩放量，共4个参数$[dx,dy,dw,dh]$，向量维数为4k。</li>
</ul>
<h4 id="训练样本的生成"><a href="#训练样本的生成" class="headerlink" title="训练样本的生成"></a>训练样本的生成</h4><p>&emsp;&emsp;一般而言，特征图conv5-3的实际尺寸大致为$60 \times 40$，那么一共可以生成$60 \times 40 \times 9 \approx 20k$个anchors，显然不会将所有anchors用于训练，而是筛选一定数量的正负样本。对于数据集中包含有人工标定ground truth的图像，考虑一张图像上所有anchors:</p>
<ul>
<li>首先过滤掉超出图像边界的anchors</li>
<li>对每个标定的ground truth，与其重叠比例IoU最大的anchor记为正样本，这样可以保证每个ground truth至少对应一个正样本anchor</li>
<li>对每个anchors，如果其与某个ground truth的重叠比例IoU大于0.7，则记为正样本(目标)；如果小于0.3，则记为负样本(背景)</li>
<li>再从已经得到的正负样本中随机选取256个anchors组成一个minibatch用于训练，而且正负样本的比例为1:1,；如果正样本不够，则补充一些负样本以满足256个anchors用于训练，反之亦然。</li>
</ul>
<h4 id="Multi-task-Loss-Function"><a href="#Multi-task-Loss-Function" class="headerlink" title="Multi-task Loss Function"></a>Multi-task Loss Function</h4><p>&emsp;&emsp;由于涉及到分类和回归，所以需要定义一个多任务损失函数(Multi-task Loss Function)，包括Softmax Classification Loss和Bounding Box Regression Loss，公式定义如下：<br>$$L(\lbrace p_i \rbrace, \lbrace t_i \rbrace)=\dfrac{1}{N_{cls}}\Sigma_i L_{cls}(p_i,p_i^{\ast}) + \lambda \dfrac{1}{N_{reg}}\Sigma_i p_i^{\ast} L_{reg}(t_i, t_i^{\ast})$$<br><strong>Softmax Classification</strong>：对于RPN网络的分类层(cls)，其向量维数为2k = 18，考虑整个特征图conv5-3，则输出大小为$W \times H \times 18$，正好对应conv5-3上每个点有9个anchors，而每个anchor又有两个score(fg/bg)输出，对于单个anchor训练样本，其实是一个二分类问题。为了便于Softmax分类，需要对分类层执行reshape操作，这也是由底层数据结构决定的。在caffe中，Blob的数据存储形式为$Blob=[batch\_size,channel,height,width]$，而对于分类层(cls)，其在Blob中的实际存储形式为$[1,2k,H,W]$，而Softmax针对每个anchor进行二分类，所以需要在分类层后面增加一个reshape layer，将数据组织形式变换为$[1,2,k*H,W]$，之后再reshape回原来的结构，caffe中有对softmax_loss_layer.cpp的reshape函数做如下解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&quot;Number of labels must match number of predictions; &quot;  </div><div class="line">&quot;e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), &quot;  </div><div class="line">&quot;label count (number of labels) must be N*H*W, &quot;  </div><div class="line">&quot;with integer values in &#123;0, 1, ..., C-1&#125;.&quot;;</div></pre></td></tr></table></figure></p>
<p>在上式中，$p_i$为样本分类的概率值，$p_i^{\ast}$为样本的标定值(label)，anchor为正样本时$p_i^{\ast}$为1，为负样本时$p_i^{\ast}$为0，$L_{cls}$为两种类别的对数损失(log loss)。<br><strong>Bounding Box Regression</strong>：RPN网络的回归层输出向量的维数为4k = 36，回归参数为每个样本的坐标$[x,y,w,h]$，分别为box的中心位置和宽高，考虑三组参数预测框(predicted box)坐标$[x,y,w,h]$，anchor坐标$[x_a,y_a,w_a,h_a]$，ground truth坐标$[x^{\ast},y^{\ast},w^{\ast},h^{\ast}]$，分别计算预测框相对anchor中心位置的偏移量以及宽高的缩放量$\lbrace t \rbrace$，ground truth相对anchor的偏移量和缩放量$\lbrace t^{\ast} \rbrace$<br>$$t_x=(x-x_a)/w_a,&emsp;t_y=(y-y_a)/h_a,&emsp;t_w=log(w/w_a),&emsp;t_h=log(h/h_a)&emsp;(1)$$<br>$$t_x^{\ast}=(x^{\ast}-x_a)/w_a,&emsp;t_y^{\ast}=(y^{\ast}-y_a)/h_a,&emsp;t_w^{\ast}=log(w^{\ast}/w_a),&emsp;t_h^{\ast}=log(h^{\ast}/h_a)&emsp;(2)$$<br>回归目标就是让$\lbrace t \rbrace$尽可能地接近$ \lbrace t^{\ast} \rbrace$，所以回归真正预测输出的是$\lbrace t \rbrace$，而训练样本的标定真值为$\lbrace t^{\ast} \rbrace$。得到预测输出$\lbrace t \rbrace$后，通过上式(1)即可反推获取预测框的真实坐标。在损失函数中，回归损失采用Smooth L1函数<br>$$Smooth_{L1}(x)=\begin{cases} 0.5x^2&emsp;|x| \leq 1 \\ |x|-0.5&emsp;otherwise\end{cases}$$<br>$$L_{reg}=Smooth_{L1}(t-t^{\ast})$$<br>Smooth L1损失函数曲线如下图所示，相比于L2损失函数，L1对离群点或异常值不敏感，可控制梯度的量级使训练更易收敛。</p>
<p><img src="https://i.loli.net/2017/09/05/59ae4859d5b2b.jpg" alt="smooth-L1.jpg" title="Smooth L1损失函数"></p>
<p>在损失函数中，$p_i^{\ast}L_{reg}$这一项表示只有目标anchor($p_i^{\ast}=1$)才有回归损失，其他anchor不参与计算。这里需要注意的是，当样本bbox和ground truth比较接近时(IoU大于某一阈值)，可以认为上式的坐标变换是一种线性变换，因此可将样本用于训练线性回归模型，否则当bbox与ground truth离得较远时，就是非线性问题，用线性回归建模显然不合理，会导致模型不work。分类层(cls)和回归层(reg)的输出分别为$\lbrace p \rbrace$和$\lbrace t \rbrace$，两项损失函数分别由$N_{cls}$和$N_{reg}$以及一个平衡权重$\lambda$归一化。分类损失的归一化值为minibatch的大小，即$N_{cls}=256$；回归损失的归一化值为anchor位置的数量，即$N_{reg} \approx 2400$；$\lambda$一般取值为10，这样分类损失和回归损失差不多是等权重的。</p>
<h4 id="Proposal的生成"><a href="#Proposal的生成" class="headerlink" title="Proposal的生成"></a>Proposal的生成</h4><p>&emsp;&emsp;Proposal的生成就是将图像输入到RPN网络中进行一次前向(forward)计算，处理流程如下：</p>
<ul>
<li>计算特征图conv5-3映射到输入图像的所有anchors，并通过RPN网络前向计算得到anchors的score输出和bbox回归参数</li>
<li>由anchors坐标和bbox回归参数计算得到预测框proposal的坐标</li>
<li>处理proposal坐标超出图像边界的情况(使得坐标最小值为0，最大值为宽或高)</li>
<li>滤除掉尺寸(宽高)小于给定阈值的proposal</li>
<li>对剩下的proposal按照目标得分(fg score)从大到小排序，提取前pre_nms_topN(e.g. 6000)个proposal</li>
<li>对提取的proposal进行非极大值抑制(non-maximum suppression,nms)，再根据nms后的foreground score，筛选前post_nms_topN(e.g. 300)个proposal作为最后的输出</li>
</ul>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>&emsp;&emsp;对于RPN网络中生成的proposal，需要送入Fast R-CNN网络做进一步的精确分类和坐标回归，但proposal的尺寸可能大小不一，所以需要做RoI Pooling，输出统一尺寸的特征，再与后面的全连接层相连。</p>
<h4 id="RoI-Pooling"><a href="#RoI-Pooling" class="headerlink" title="RoI Pooling"></a>RoI Pooling</h4><p>&emsp;&emsp;对于传统的卷积神经网络，当网络训练好后输入图像的尺寸必须是固定值，同时网络输出的固定大小的向量或矩阵。如果输入图像大小不统一，则需要进行特殊处理，如下图所示：</p>
<ul>
<li>从图像中crop一部分传入网络</li>
<li>将图像warp成需要的大小后传入网络</li>
</ul>
<p><img src="https://i.loli.net/2017/09/05/59ae5e94e03ed.jpg" alt="crop-warp.jpg" title="crop与warp操作" width="70%" height="70%" align="center"></p>
<p>可以从图中看出，crop操作破坏了图像的完整结构，warp操作破坏了图像的原始形状信息，两种方法的效果都不太理想。RPN网络生成的proposal也存在尺寸不一的情况，但论文中提出了RoI Pooling的方法解决这个问题。</p>
<p>&emsp;&emsp;RoI Pooling结合特征图conv5-3和proposal的信息，proposal在输入图像中的坐标$[x1, y1, x2, y2]$对应$M \times N$尺度，将proposal的坐标映射到$\dfrac{M}{16} \times \dfrac{N}{16} $大小的conv5-3中，然后将Proposal在conv5-3的对应区域水平和竖直均分为7等份，并对每一份进行Max Pooling或Average Pooling处理，得到固定大小($7 \times 7$)输出的池化结果，实现固定长度输出(fixed-length output)，如下图所示。</p>
<p><img src="https://i.loli.net/2017/09/05/59ae6269923b9.jpg" alt="RoI Pooling.jpg" title="RoI Pooling示意图" width="60%" height="60%" align="center"></p>
<h4 id="Classification-and-Regression"><a href="#Classification-and-Regression" class="headerlink" title="Classification and Regression"></a>Classification and Regression</h4><p>&emsp;&emsp;RoI Pooling层后接多个全连接层，最后为两个子连接层——分类层(cls)和回归层(reg)，如下图所示，和RPN的输出类似，只不过输出向量的维数不一样。如果类别数为N+1(包括背景)，分类层的向量维数为N+1，回归层的向量维数则为4(N+1)。还有一个关键问题是RPN网络输出的proposal如何组织成Fast R-CNN的训练样本：</p>
<ul>
<li>对每个proposal，计算其与所有ground truth的重叠比例IoU</li>
<li>筛选出与每个proposal重叠比例最大的ground truth</li>
<li>如果proposal的最大IoU大于0.5则为目标(前景)，标签值(label)为对应ground truth的目标分类；如果IoU小于0.5且大于0.1则为背景，标签值为0</li>
<li>从2张图像中随机选取128个proposals组成一个minibatch，前景和背景的比例为1:3</li>
<li>计算样本proposal与对应ground truth的回归参数作为标定值，并且将回归参数从(4,)拓展为(4(N+1),)，只有对应类的标定值才为非0。</li>
<li>设定训练样本的回归权值，权值同样为4(N+1)维，且只有样本对应标签类的权值才为非0。</li>
</ul>
<p>在源码实现中，用于训练Fast R-CNN的Proposal除了RPN网络生成的，还有图像的ground truth，这两者归并到一起，然后通过筛选组成minibatch用于迭代训练。Fast R-CNN的损失函数也与RPN类似，二分类变成了多分类，背景同样不参与回归损失计算，且只考虑proposal预测为标签类的回归损失。</p>
<p><img src="https://i.loli.net/2017/09/05/59ae6440d7a34.jpg" alt="cls and reg.jpg" title="Classification and Regression"></p>
<h3 id="Faster-R-CNN的训练"><a href="#Faster-R-CNN的训练" class="headerlink" title="Faster R-CNN的训练"></a>Faster R-CNN的训练</h3><p>&emsp;&emsp;对于提取proposals的RPN，以及分类回归的Fast R-CNN，如何将这两个网络嵌入到同一个网络结构中，训练一个共享卷积层参数的多任务(Multi-task)网络模型。源码中有实现交替训练(Alternating training)和端到端训练(end-to-end)两种方式，这里介绍交替训练的方法。</p>
<ul>
<li>训练RPN网络，用ImageNet模型M0初始化，训练得到模型M1</li>
<li>利用第一步训练的RPN网络模型M1，生成Proposal P1</li>
<li>使用上一步生成的Proposal，训练Fast R-CNN网络，同样用ImageNet模型初始化，训练得到模型M2</li>
<li>训练RPN网络，用Fast R-CNN网络M2初始化，且固定卷积层参数，只微调RPN网络独有的层，训练得到模型M3</li>
<li>利用上一步训练的RPN网络模型M3，生成Proposal P2</li>
<li>训练Fast R-CNN网络，用RPN网络模型M3初始化，且卷积层参数和RPN参数不变，只微调Fast R-CNN独有的网络层，得到最终模型M4</li>
</ul>
<p>由训练流程可知，第4步训练RPN网络和第6步训练Fast R-CNN网络实现了卷积层参数共享。总体上看，训练过程只循环了2次，但每一步训练(M1，M2，M3，M4)都迭代了多次(e.g. 80k，60k)。对于固定卷积层参数，只需将学习率(learning rate)设置为0即可。</p>
<h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p>&emsp;&emsp;以上关于RPN的训练，Proposal的生成，以及Fast R-CNN的训练做了的详细讲解，接下来结合网络模型图和部分源码，对这些模块做进一步的分析。</p>
<h4 id="train-RPN"><a href="#train-RPN" class="headerlink" title="train RPN"></a>train RPN</h4><p>&emsp;&emsp;训练RPN的网络结构如下图所示，首先加载参数文件，并改动一些参数适应当前训练任务。在train_rpn函数中调用get_roidb、get_imdb、get_train_imdb_roidb等获取训练数据集，并通过调用gt_roidb和prepare_roidb方法对训练数据进行预处理，为样本增添一些属性，数据集roidb中的每个图像样本，主要有以下属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&apos;image&apos;:图像存储路径</div><div class="line">&apos;width&apos;:图像宽</div><div class="line">&apos;height&apos;:图像高</div><div class="line">&apos;boxes&apos;:图像中bbox(groundtruth or proposal)的坐标[x1,y1,x2,y2]</div><div class="line">&apos;gt_classes&apos;:每个bbox对应的类索引(1~20)</div><div class="line">&apos;gt_overlaps&apos;:二维数组，shape=[num_boxes * num_classes]，每个bbox(ground truth)对应的类索引处取值为1，其余为0</div><div class="line">&apos;flipped&apos;:取值为True/False，用于标记有无将图像水平翻转</div><div class="line">&apos;seg_area&apos;:bbox的面积</div><div class="line">&apos;max_classes&apos;:bbox与所有ground truth的重叠比例IoU最大的类索引(gt_overlaps.argmax(axis=1))</div><div class="line">&apos;max_overlaps&apos;:bbox与所有ground truth的IoU最大值(gt_overlaps.max(axis=1))</div></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2017/09/05/59aea51ce1310.jpg" alt="train_rpn_model.jpg" title="train_rpn_model"></p>
<p>获取数据集roidb中字典的属性后，设置输出路径output_dir，用来保存中间训练结果，然后调用train_net函数。在train_net函数中，首先调用filter_roidb，滤除掉既没有前景又没有背景的roidb。然后调用layer.py中的set_roidb方法，打乱训练样本roidb的顺序，将roidb中长宽比近似的图像放在一起。之后开始训练模型train_model，这里需要实例化每个层，对于第一层RoIDataLayer，通过setup方法进行实例化，并且在训练过程中通过forward方法，调用get_minibatch函数，获取每一次迭代训练的数据，在读取数据时，主要获取了3个属性组成Layer中的Blob<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&apos;data&apos;:单张图像数据im_blob=[1,3,H,W]</div><div class="line">&apos;gt_boxes&apos;:一幅图像中所有ground truth的坐标和类别[x1,y1,x2,y2,cls]</div><div class="line">&apos;im_info&apos;:图像的宽高和缩放比例 height,width,scale = [[im_blob.shape[2], im_blob.shape[2], im_scale[0]]]</div></pre></td></tr></table></figure></p>
<p>从网络结构图中可以看出，input-data(RoIDataLayer)的下一层是rpn-data(AnchorTargetLayer)，rpn-data计算所有anchors与ground truth的重叠比例IoU，从中筛选出一定数量(256)的正负样本组成一个minibatch，用于RPN网络的训练，这一层的输出有如下属性：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&apos;rpn_label&apos;:每个anchor对应的类别(1——fg，0——bg，-1——ignored)，shape=[1,1,A*height,width]</div><div class="line">&apos;rpn_bbox_targets&apos;:anchor与ground truth的回归参数[dx,dy,dw,dh]，shape=[1,A*4,height,width]</div><div class="line">&apos;rpn_box_inside_targets&apos;:回归损失函数中的样本权值，正样本为1，负样本为0，相当于损失函数中的p*，shape=[1,A*4,height,width]</div><div class="line">&apos;rpn_box_outside_targets&apos;:分类损失函数和回归损失函数的平衡权重，相当于λ，shape=[1,A*4,height,width]</div><div class="line">注：height、width为特征图conv5-3的高宽，A=9为Anchor种数</div></pre></td></tr></table></figure></p>
<p>对于分类损失rpn_loss_cls，输入的rpn_cls_scors_reshape和rpn_labels分别对应$p$与$p^{\ast}$；对于回归损失，输入的rpn_bbox_pred和rpn_bbox_targets分别对应$\lbrace t \rbrace$与$\lbrace t^{\ast} \rbrace$，pn_bbox_inside_weigths对应$p^{\ast}$，rpn_bbox_outside_weights对应$\lambda$。</p>
<h4 id="generate-proposals"><a href="#generate-proposals" class="headerlink" title="generate proposals"></a>generate proposals</h4><p>&emsp;&emsp;Proposal的生成只需将图像输入到RPN网络中，进行前向(forward)计算然后经过筛选即可得到，网络结构如下图所示</p>
<p><img src="https://i.loli.net/2017/09/05/59aeabc4e0a9d.jpg" alt="generate proposals.jpg" title="generate proposals"></p>
<p>从rpn_proposals = imdb_proposals(rpn_net, imdb)开始，使用im = cv2.imread(imdb.image_path_at(i))读入图片数据，调用 im_proposals生成单张图片的rpn proposals，以及得分。im_proposals函数会调用网络的forward方法，从而得到想要的boxes和scores，最后将获取的proposal保存在python pickle文件中。</p>
<h4 id="train-Fast-R-CNN"><a href="#train-Fast-R-CNN" class="headerlink" title="train Fast R-CNN"></a>train Fast R-CNN</h4><p>&emsp;&emsp;训练Fast R-CNN的网络结构如下图所示，首先设置参数适应训练任务，在预处理数据时，调用的不再是gt_roidb方法，而是rpn_roidb，通过使用类imdb的静态方法merge_roidb，将rpn_roidb和gt_roidb归并为一个roidb，因此数据集中的’boxes’属性除了包含ground truth，还有RPN网络生成的proposal，可通过上一步保存的文件直接读取。通过add_bbox_regression_targets方法给roidb的样本增添了额外的属性’bbox_targets’，用于表示回归参数的标定值。属性’gt_overlaps’是所有proposal与ground truth通过计算IoU得到的。最后就是调用get_minibatch方法从2张图像中选取128个proposal作为一次迭代的训练样本，读取数据时，获取如下属性组成Layer中的Blob<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&apos;data&apos;:图像数据</div><div class="line">&apos;rois&apos;:proposals的坐标[batch_inds,x1,y1,x2,y2]</div><div class="line">&apos;label&apos;:proposals对应的类别(0~20)</div><div class="line">&apos;bbox_targets&apos;:proposal回归参数的标定值，shape = [128, 4(N+1)]</div><div class="line">&apos;box_inside_targets&apos;:回归损失函数中的样本权值，正样本为1，负样本为0，相当于损失函数中的p*</div><div class="line">&apos;rpn_box_outside_targets&apos;:分类损失函数和回归损失函数的平衡权重，相当于λ</div></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2017/09/05/59aeabc5166ef.jpg" alt="train_fast_rcnn_model.jpg" title="train_fast_rcnn_model"></p>
<p>损失函数的计算与RPN网络类似。在Faster R-CNN中，自定义的Python Layer包括RoIDataLayer、AnchorTargetLay、ProposalLayer，都只实现了前向计算forward，因为这些Layer的作用是获取用于训练网络的数据，而对网络本身没有贡献任何权值参数，也不传播梯度值，因此不需要实现反向传播backward。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="external">Paper: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></li>
<li><a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="external">Paper: R-CNN：Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
<li><a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="external">Paper: SPP-Net: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></li>
<li><a href="https://arxiv.org/pdf/1504.08083.pdf" target="_blank" rel="external">Paper: Fast R-CNN</a></li>
<li><a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">Code: Caffe implement of Faster RCNN</a></li>
<li><a href="https://github.com/smallcorgi/Faster-RCNN_TF" target="_blank" rel="external">Code: Tensorflow implement of Faster RCNN</a></li>
<li><a href="http://blog.csdn.net/iamzhangzhuping/article/category/6230157" target="_blank" rel="external">http://blog.csdn.net/iamzhangzhuping/article/category/6230157</a></li>
<li><a href="http://www.infocool.net/kb/Python/201611/209696.html" target="_blank" rel="external">http://www.infocool.net/kb/Python/201611/209696.html</a></li>
<li><a href="http://www.cnblogs.com/venus024/p/5717766.html" target="_blank" rel="external">http://www.cnblogs.com/venus024/p/5717766.html</a></li>
<li><a href="http://blog.csdn.net/zy1034092330/article/details/62044941" target="_blank" rel="external">http://blog.csdn.net/zy1034092330/article/details/62044941</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;R-CNN是目标检测领域中十分经典的方法，相比于传统的手工特征，R-CNN将卷积神经网络引入，用于提取深度特征，后接一个分类器判决搜索区域是否包含目标及其置信度，取得了较为准确的检测结果。Fast R-CNN和Faster R-CNN是R-CNN的升级版本，在准确率和实时性方面都得到了较大提升。在Fast R-CNN中，首先需要使用Selective Search的方法提取图像的候选目标区域(Proposal)。而新提出的Faster R-CNN模型则引入了RPN网络(Region Proposal Network)，将Proposal的提取部分嵌入到内部网络，实现了卷积层特征共享，Fast R-CNN则基于RPN提取的Proposal做进一步的分类判决和回归预测，因此，整个网络模型可以完成端到端的检测任务，而不需要先执行特定的候选框搜索算法，显著提升了算法模型的实时性。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="DL" scheme="https://senitco.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Linux服务器下安装TensorFlow</title>
    <link href="https://senitco.github.io/2017/07/20/linux-install-tensorflow/"/>
    <id>https://senitco.github.io/2017/07/20/linux-install-tensorflow/</id>
    <published>2017-07-19T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;简单介绍在Linux服务器的个人目录下安装TensorFlow。TensorFlow的安装方式有多种，基于Pip的安装、基于Docker的安装、基于VirtualEnv的安装、基于Anaconda的安装，以及从源码编译安装，这些在<a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="external">官网</a>均有介绍，这里简单记录下基于Anaconda安装的方法。<br><a id="more"></a></p>
<h3 id="安装Anaconda"><a href="#安装Anaconda" class="headerlink" title="安装Anaconda"></a>安装Anaconda</h3><p>&emsp;&emsp;Anaconda是一个集成许多第三方科学计算库的Python科学计算环境，Anaconda使用conda 作为自己的包管理工具，同时具有自己的计算环境，类似Virtualenv。和Virtualenv一样,不同Python工程需要的依赖包，conda将其存储在不同的地方。TensorFlow上安装的Anaconda不会对之前安装的Python包进行覆盖。</p>
<ul>
<li><p>进入Anaconda官网<a href="https://www.continuum.io/downloads" target="_blank" rel="external">下载页面</a>，选择合适版本直接下载，或者在个人终端目录下，使用wget命令，示例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh</div></pre></td></tr></table></figure>
</li>
<li><p>下载到本地后运行安装脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bash Anaconda2-4.4.0-Linux-x86_64.sh</div></pre></td></tr></table></figure>
</li>
<li><p>安装完成后在~/.bashrc文件中添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export PATH=&quot;/$HOME/anaconda2/bin:$PATH&quot;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>此外，还需在~/.zshrc文件添加相关路径，否则在后面执行conda命令时，可能会出现错误信息：zsh: command not found: conda。在.zshrc文件的 #User configuration 处追加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export PATH = &quot;$PATH:$HOME/anaconda/bin&quot;</div></pre></td></tr></table></figure></p>
<p>添加完路径后分别执行以下命令使之生效<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">source ~/.bashrc</div><div class="line">source ~/.zshrc</div></pre></td></tr></table></figure></p>
<h3 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h3><ul>
<li><p>创建conda环境，命名为tensorflow</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">conda create -n tensorflow﻿​</div><div class="line">#也可指定Python版本</div><div class="line">conda create -n tensorflow python=2.7</div></pre></td></tr></table></figure>
</li>
<li><p>激活并进入创建的conda环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source activate tensorflow</div></pre></td></tr></table></figure>
</li>
<li><p>下载并安装TensorFlow</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install --ignore-installed --upgrade tfBinaryURL﻿​</div></pre></td></tr></table></figure>
</li>
<li><p><a href="https://www.tensorflow.org/install/install_linux#TF_PYTHON_URL" target="_blank" rel="external">tfBinaryURL</a>须根据平台环境进行选择。例如，对于Python2.7，GPU版本为CUDA8.0的平台，可安装如下版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install --ignore-installed --upgrade \</div><div class="line">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.1-cp27-cp27mu-manylinux1_x86_64.whl</div></pre></td></tr></table></figure>
</li>
<li><p>在不确定安装版本的情况下也可直接使用如下命令安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install tensorflow 		#CPU版本</div><div class="line">pip install tensorflow-gpu	#GPU版本</div></pre></td></tr></table></figure>
</li>
<li><p>退出虚拟环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source deactivate tensorflow</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ul>
<li><p>用source activate指令进入tensorflow环境，执行Python解释器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python</div></pre></td></tr></table></figure>
</li>
<li><p>在Python环境内，逐条输入以下语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import tensorflow as tf</div><div class="line">&gt;&gt;&gt; hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</div><div class="line">&gt;&gt;&gt; sess = tf.Session()</div><div class="line">&gt;&gt;&gt; print(sess.run(hello))</div></pre></td></tr></table></figure>
</li>
<li><p>如果成功打印下面语句，说明安装成功</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Hello, TensorFlow!</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>&emsp;&emsp;安装GPU版本的TensorFlow时，需要使用NVIDIA的显卡，并安装和配置CUDA和CUDNN环境。<br>&emsp;&emsp;一般对于多用户使用的服务器，系统主目录下都会安装有Python解释器(甚至是多个版本)。对于大多数用户而言都不具备管理员权限，在利用pip命令安装一些Python依赖库时会失败，因此在个人目录下安装集成的Anaconda环境，可以有效地与系统自带的Python解释器隔离，前提是在.bashrc文件中添加路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export PATH=&quot;$HOME/anaconda2/bin:$PATH&quot;</div></pre></td></tr></table></figure></p>
<p>并执行以下命令使之生效<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source ~/.bashrc</div></pre></td></tr></table></figure></p>
<p>这样每次执行python命令都是在个人目录下的Anaconda环境中。如果要与系统Python环境随时切换，可通过给命令起别名的方式，即别名声明alias。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">alias python27=&quot;/usr/bin/python2.7&quot;  	#系统Python环境</div><div class="line">alias python36=&quot;/usr/bin/python3.6&quot; </div><div class="line">alias pyana=&quot;/home/myname/anaconda2/bin/python2.7&quot;	#个人Python环境，精确到版本路径</div></pre></td></tr></table></figure></p>
<p>使用系统自带的Python时，执行Python27或者Python36命令即可；使用Anaconda时，执行pyana或者python命令。将上述命令添加到.bashrc文件中，这样每次开机都不需要重新输入。</p>
<p>conda的简单命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">conda create -n [name]      #创建名为name的conda环境，如tensorflow</div><div class="line"></div><div class="line">source activate [name]  	#激活并进入创建的环境</div><div class="line"></div><div class="line">source deactivate [name]    #退出名为name的环境，回到系统默认环境</div><div class="line"></div><div class="line">conda remove -n [name] --all   #删除创建的conda环境 </div><div class="line"></div><div class="line">conda info -envs    #查看所安装环境列表，创建的环境都在`~/anaconda2/envs/`目录下面</div><div class="line"></div><div class="line">conda list      #查看已经安装的包</div><div class="line"></div><div class="line">conda install [packagename]        #安装具体的包，加-n [name]可以安装到指定环境</div><div class="line"></div><div class="line">conda list -n [name]      #name环境下安装了哪些包</div><div class="line"></div><div class="line">conda update -n [name] [packagename]     #升级name环境的名为packagename的包</div><div class="line"></div><div class="line">conda remove -n [name] [packagename]     #删除name环境的名为packagename的包</div></pre></td></tr></table></figure></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="external">https://www.tensorflow.org/install/install_linux</a></li>
<li><a href="https://github.com/jikexueyuanwiki/tensorflow-zh/blob/master/SOURCE/get_started/os_setup.md" target="_blank" rel="external">https://github.com/jikexueyuanwiki/tensorflow-zh/blob/master/SOURCE/get_started/os_setup.md</a></li>
<li><a href="https://wxinlong.github.io/2017/02/23/InstallTensorflow/" target="_blank" rel="external">https://wxinlong.github.io/2017/02/23/InstallTensorflow/</a></li>
<li><a href="https://stackoverflow.com/questions/31615322/zsh-conda-pip-installs-command-not-found" target="_blank" rel="external">https://stackoverflow.com/questions/31615322/zsh-conda-pip-installs-command-not-found</a></li>
<li><a href="http://blog.csdn.net/zhangxinyu11021130/article/details/64125058" target="_blank" rel="external">http://blog.csdn.net/zhangxinyu11021130/article/details/64125058</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;简单介绍在Linux服务器的个人目录下安装TensorFlow。TensorFlow的安装方式有多种，基于Pip的安装、基于Docker的安装、基于VirtualEnv的安装、基于Anaconda的安装，以及从源码编译安装，这些在&lt;a href=&quot;https://www.tensorflow.org/install/install_linux&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官网&lt;/a&gt;均有介绍，这里简单记录下基于Anaconda安装的方法。&lt;br&gt;
    
    </summary>
    
      <category term="Compile &amp; Installation" scheme="https://senitco.github.io/categories/Compile-Installation/"/>
    
    
      <category term="Sofeware" scheme="https://senitco.github.io/tags/Sofeware/"/>
    
  </entry>
  
  <entry>
    <title>图像局部特征描述总结</title>
    <link href="https://senitco.github.io/2017/07/18/image-local-feature-summary/"/>
    <id>https://senitco.github.io/2017/07/18/image-local-feature-summary/</id>
    <published>2017-07-17T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;局部图像特征描述是计算机视觉的一个基本研究问题，在寻找图像中的对应点以及物体特征描述中有着重要的作用。它是许多方法的基础，因此也是目前视觉研究中的一个热点，每年在视觉领域的顶级会议ICCV/CVPR/ECCV上都有高质量的特征描述论文发表。同时它也有着广泛的应用，举例来说，在利用多幅二维图像进行三维重建、恢复场景三维结构的应用中，其基本出发点是要有一个可靠的图像对应点集合，而自动地建立图像之间点与点之间的可靠对应关系通常都依赖于一个优秀的局部图像特征描述子。又比如，在物体识别中，目前非常流行以及切实可行的方法之一是基于局部特征的，由于特征的局部性，使得物体识别可以处理遮挡、复杂背景等比较复杂的情况。<br><a id="more"></a></p>
<h3 id="图像特征综述"><a href="#图像特征综述" class="headerlink" title="图像特征综述"></a>图像特征综述</h3><p>&emsp;&emsp;局部图像特征描述的核心问题是不变性（鲁棒性）和可区分性。由于使用局部图像特征描述子的时候，通常是为了鲁棒地处理各种图像变换的情况。因此，在构建/设计特征描述子的时候，不变性问题就是首先需要考虑的问题。在宽基线匹配中，需要考虑特征描述子对于视角变化的不变性、对尺度变化的不变性、对旋转变化的不变性等；在形状识别和物体检索中，需要考虑特征描述子对形状的不变性。</p>
<p>&emsp;&emsp;然而，特征描述子的可区分性的强弱往往和其不变性是矛盾的，也就是说，一个具有众多不变性的特征描述子，它区分局部图像内容的能力就稍弱；而如果一个非常容易区分不同局部图像内容的特征描述子，它的鲁棒性往往比较低。举个例子，假定我们需要对一个点周围固定大小的局部图像内容进行描述。如果我们直接将图像内容展开成一个列向量对其进行描述，那么只要局部图像内容发生了一点变化，就会使得它的特征描述子发生较大的变化，因此这样的特征描述方式很容易区分不同的局部图像内容，但是对于相同的局部图像内容发生旋转变化等情况，它同样会产生很大的差异，即不变性弱。</p>
<p>&emsp;&emsp;而另一方面，如果我们通过统计局部图像灰度直方图来进行特征描述，这种描述方式具有较强的不变性，对于局部图像内容发生旋转变化等情况比较鲁棒，但是区分能力较弱，例如无法区分两个灰度直方图相同但内容不同的局部图像块。</p>
<p>&emsp;&emsp;综上所述，一个优秀的特征描述子不仅应该具有很强不变性，还应该具有很强的可区分性。</p>
<h3 id="典型图像特征分析"><a href="#典型图像特征分析" class="headerlink" title="典型图像特征分析"></a>典型图像特征分析</h3><p>&emsp;&emsp;在诸多的局部图像特征描述子中，SIFT（Scale Invariant Feature Transform）是其中应用最广的，它在1999年首次提出，至2004年得到完善。SIFT的提出也是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，自它提出以来，很快在物体识别、宽基线图像匹配、三维重建、图像检索中得到了应用，局部图像特征描述子在计算机视觉领域内也得到了更加广泛的关注，涌现了一大批各具特色的局部图像特征描述子。</p>
<p>&emsp;&emsp;SURF（Speeded Up Robust Features）是对SIFT的改进版本，它利用Haar小波来近似SIFT方法中的梯度操作，同时利用积分图技术进行快速计算，SURF的速度是SIFT的3-7倍，大部分情况下它和SIFT的性能相当，因此它在很多应用中得到了应用，尤其是对运行时间要求高的场合。</p>
<p>&emsp;&emsp;DAISY是面向稠密特征提取的可快速计算的局部图像特征描述子，它本质思想和SIFT是一样的：分块统计梯度方向直方图，不同的是，DAISY在分块策略上进行了改进，利用高斯卷积来进行梯度方向直方图的分块汇聚，这样利用高斯卷积的可快速计算性就可以快速稠密地进行特征描述子的提取。比较巧合的是，DAISY这种特征汇聚策略被一些研究者（Matthen Brown，Gang Hua，Simon Winder）通过机器学习的方法证明相对于其他几种特征汇聚策略（卡迪尔坐标下分块、极坐标下分块）是最优的。</p>
<p>&emsp;&emsp;ASIFT（Affine SIFT）通过模拟所有成像视角下得到的图像进行特征匹配，可以很好地处理视角变化的情况，尤其是大视角变化下的图像匹配。</p>
<p>&emsp;&emsp;MROGH（Multi-support Region Order-based Gradient Histogram）则是特征汇聚策略上寻求创新，之前的局部图像特征描述子，其特征汇聚策略都是基于邻域内点的几何位置的，而MROGH基于点的灰度序进行特征汇聚。</p>
<p>&emsp;&emsp;BRIEF（Binary Robust Independent Element Feature）利用局部图像邻域内随机点对的灰度大小关系来建立局部图像特征描述子，得到的二值特征描述子不仅匹配速度快，而且存储要求内存低，因此手机应用中具有很好的应用前景。其实，利用邻域内点对的灰度大小关系进行特征描述这一思想在SMD（ECCV’08）中就已经有了。</p>
<p>&emsp;&emsp;除了BRIEF，近两年还提出了许多二值特征描述子，例如ORB、BRISK、FREAK。上述这些特征描述子都是基于手动设计得到的，也有一些研究试图利用机器学习的方法，通过数据驱动得到想要的特征描述子。这类特征描述子包括PCA-SIFT，Linear Discriminative Embedding，LDA-Hash等。当然，除了提到的这些特征描述子之外，还有许多其他的特征描述子，在这就不再一一叙述了。</p>
<p>&emsp;&emsp;最近几年局部图像特征描述子的发展趋势是：快速、低存储。这两个趋势使得局部图像特征描述子可以在快速实时、大规模应用中发挥作用，而且有利于将许多应用做到手机上去进行开发，实实在在的将计算机视觉技术应用于我们周围的世界中。为了满足快速和低存储这两个需求，二值特征描述子得到了研究者的广泛关注，这两年CVPR和ICCV中关于局部图像特征描述子的文章，大部分都是这类的。相信它们在未来几年还会继续受到关注，期待出现一些深入大众生活中的成功应用。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://www.eng.auburn.edu/~troppel/courses/7970%202015A%20AdvMobRob%20sp15/literature/%5B2008%5D%20Local%20Invariant%20Feature%20Detectors-%20A%20Survey.pdf" target="_blank" rel="external">Paper: Local Invariant Feature Detectors: A Survey</a></li>
<li><a href="https://web.eecs.umich.edu/~jjcorso/t/598F14/files/lecture_0929_features.pdf" target="_blank" rel="external">PPT: Local Image Features</a></li>
<li><a href="http://mamicode.com/info-detail-296752.html" target="_blank" rel="external">http://mamicode.com/info-detail-296752.html</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4260167.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4260167.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;局部图像特征描述是计算机视觉的一个基本研究问题，在寻找图像中的对应点以及物体特征描述中有着重要的作用。它是许多方法的基础，因此也是目前视觉研究中的一个热点，每年在视觉领域的顶级会议ICCV/CVPR/ECCV上都有高质量的特征描述论文发表。同时它也有着广泛的应用，举例来说，在利用多幅二维图像进行三维重建、恢复场景三维结构的应用中，其基本出发点是要有一个可靠的图像对应点集合，而自动地建立图像之间点与点之间的可靠对应关系通常都依赖于一个优秀的局部图像特征描述子。又比如，在物体识别中，目前非常流行以及切实可行的方法之一是基于局部特征的，由于特征的局部性，使得物体识别可以处理遮挡、复杂背景等比较复杂的情况。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征描述子之FREAK</title>
    <link href="https://senitco.github.io/2017/07/15/image-feature-freak/"/>
    <id>https://senitco.github.io/2017/07/15/image-feature-freak/</id>
    <published>2017-07-14T16:00:00.000Z</published>
    <updated>2017-09-18T04:15:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;FREAK算法来源2012年CVPR上的一篇文章<a href="https://infoscience.epfl.ch/record/175537/files/2069.pdf" target="_blank" rel="external">FREAK: Fast Retina Keypoint</a>，与ORB、BRISK算法类似，FREAK也是一种基于二进制编码的图像特征描述子，计算较快，对噪声鲁棒，具有尺度不变性和旋转不变性。此外，该算法还有一个突出特点就是受到人眼视网膜视觉机理的启发而提出。<br><a id="more"></a></p>
<p>&emsp;&emsp;在前面的博文中，介绍的BRIEF、ORB、BRISK算法都是基于特征点周围邻域像素点对之间的比较，形成二进制编码串作为特征描述子，这种描述方法计算速度快，且占用内存小，满足一些实时应用场景的需求。对于这类特征描述子，关键是确定邻域哪些像素点对进行比较，以及如何匹配。BRIEF算法中特征点邻域的像素点对是随机采样生成的，ORB算法是通过贪婪穷举的方法，在所有可能的像素点对中选取相关性较小的若干点对，BRISK则是采用平均采样的方法生成若干采样点。特征匹配方法通常都是采样Hamming距离来进行度量，由于是二进制编码方式，可通过异或操作快速计算。</p>
<h3 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h3><p>&emsp;&emsp;FAST算法可实现快速检测图像特征点，而且对应有一个加速版本AGAST，因此在诸多特征描述子中，都是首先通过FAST算法搜索定位特征点，再加以描述。FREAK同BRISK算法类似，也是建立多尺度空间，在不同尺度的图像上使用FAST算法检测特征点。</p>
<h3 id="采样模式"><a href="#采样模式" class="headerlink" title="采样模式"></a>采样模式</h3><p>&emsp;&emsp;FREAK算法中采样模式接近于人眼视网膜接收图像信息的采样模型，如下图所示，人眼视网膜中，Fovea区域主要对高精度的图像信息进行处理，而Para区域则主要对低精度的图像信息进行处理。</p>
<p><img src="https://i.loli.net/2017/07/15/596a168be43b5.jpg" alt="retina.jpg"></p>
<p>在FREAK的采样模式中，图中每一个黑点代表一个采样点，每个圆圈代表一个感受野，每个采样点需进行高斯模糊处理，以降低噪声影响，感受野的半径表示高斯模糊的标准差。这种采样模式与BRISK的不同之处在于，感受野之间存在重叠的区域；与BRIEF和ORB算法的不同之处在于，FREAK的采样点根据与特征点的距离远近，采用了不同大小的高斯核函数进行平滑处理。不同大小的感受野在人眼视网膜中也存在类似的结构，通过重叠的感受野，可以获得更多的信息，使最终的描述符更具独特性和可区分性。最终FREAK算法的采样结构为6、6、6、6、6、6、6、1，6代表每层中有6个采样点并且这6个采样点在一个同心圆上，一共有7个同心圆，最后的1表示特征点。</p>
<h3 id="特征描述"><a href="#特征描述" class="headerlink" title="特征描述"></a>特征描述</h3><p>&emsp;&emsp;FREAK算法同样采用二进制编码描述特征点，用$F$表示编码特征<br>$$F=\Sigma_{0 \leq a &lt; N} 2^a T(P_a)$$<br>$$T(P_a) = \begin{cases} 1,&emsp;I(P_a^{r_1}) &gt; I(P_a^{r_2}) \\ 0,&emsp;otherwise \end{cases}$$<br>式中，$I(P_a^{r_1})$表示采样点经过高斯模糊后的灰度值。<br>&emsp;&emsp;FREAK的采样模式中一共有43个采样点，可以产生$N = 43(43 - 1)/2 = 903$个采样点对，有些采样点对的编码值对特征描述并没有实际作用，反而会造成特征冗余，因此需要对特征的描述向量进行筛选，也就是降维。原论文中采用与ORB中类似的贪婪穷举的方法筛选采样点对。</p>
<ul>
<li>对$M(M=50000)$个特征点建立一个$M \times N$的矩阵$D$，矩阵的每一行表示每个特征点的二进制描述符</li>
<li>对矩阵的每一列，计算其均值。由于$D$中元素是0/1分布的，均值越接近0.5说明方差越大</li>
<li>根据均值与0.5的距离从小到大，对矩阵的所有列重新排序(即按方差从大到小)</li>
<li>选取前$k(k=512)$列作为最终的二进制描述符</li>
</ul>
<p>原论文中作者将得到的512个采样点对分成4组，每组128个，把这些采样点对进行连线，得到下图结果。作者发现，这四组连线中的第一组主要在外围，之后每一组连线逐渐内缩，最后一组的连线主要在中央部分，这与人眼视觉系统很相似，人眼视网膜首先也是通过perifoveal区域对感兴趣物体的位置进行估计，然后通过感光细胞更加密集的fovea区域进行验证，最终确定物体的信息。</p>
<p><img src="https://i.loli.net/2017/07/15/596a1f0191ac7.jpg" alt="sample points.jpg"></p>
<h3 id="特征方向"><a href="#特征方向" class="headerlink" title="特征方向"></a>特征方向</h3><p>&emsp;&emsp;FREAK描述子自身的圆形对称采样结构，在某种程度上使其具有旋转不变性；采样的位置和半径随着尺度的变化使其具有尺度不变性；对每个采样点进行高斯模糊，也具有一定的抗噪性能；像素点的强度对比生成二进制描述子使其具有光照不变性。由此产生的二进制描述子可以用来进行特征匹配。但是在匹配之前，可以进一步描述特征点的方向信息。由于BRISK算法与FREAK算法对特征点邻域的采样模式相近，因此FREAK算法特征点方向的计算也与之类似。BRISK算法是通过计算具有长距离的采样点对的梯度来表示特征点的方向，FREAK算法则采用其中45个距离长的、对称的采样点计算其梯度，如下图所示：</p>
<p><img src="https://i.loli.net/2017/07/15/596a1f01a4d82.jpg" alt="main direction.jpg"></p>
<p>梯度计算公式为<br>$$O = \dfrac{1}{M} \Sigma_{P_o \in G} (I(P_o^{r_1}) - I(P_o^{r_2})) \dfrac{P_o^{r_1} - P_o^{r_2}}{||P_o^{r_1} - P_o^{r_2}||}$$<br>式中，$O$表示特征点局部邻域梯度，$M$表示采样点对的个数，$G$表示采样点对集合，$P_o$表示采样点对的位置。可根据梯度进一步求得特征点的主方向。</p>
<h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>&emsp;&emsp;在特征描述中，得到了512bit的二进制描述符，该描述符的列是方差由高到低的排列，而高方差表征了模糊信息，低方差表征了细节信息，与人眼视网膜相似，人眼先处理的是模糊信息，再处理细节信息。因此，选取前128bit即16bytes进行匹配，若两个待匹配的特征点前16bytes距离小于设定的阈值，则再用剩余的比特位进行匹配。第一步匹配可以剔除掉90%的不相关匹配点，这种级联的操作在很大程度上提高了匹配的速度。</p>
<h3 id="Experiment-amp-Result"><a href="#Experiment-amp-Result" class="headerlink" title="Experiment &amp; Result"></a>Experiment &amp; Result</h3><p>OpenCV实现BRISK检测与匹配参考代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">#include &lt;opencv2/core/core.hpp&gt;  </div><div class="line">#include &lt;opencv2/features2d/features2d.hpp&gt;  </div><div class="line">#include &lt;opencv2/highgui/highgui.hpp&gt;  </div><div class="line">#include &lt;opencv2/nonfree/features2d.hpp&gt;  </div><div class="line">#include &lt;opencv2/legacy/legacy.hpp&gt;  </div><div class="line">#include &lt;iostream&gt;  </div><div class="line">#include &lt;vector&gt;  </div><div class="line">  </div><div class="line">using namespace cv;  </div><div class="line">using namespace std;  </div><div class="line">  </div><div class="line">int main(void)  </div><div class="line">&#123;  </div><div class="line">    string filename1 = &quot;beaver1.png&quot;;  </div><div class="line">    string filename2 = &quot;beaver2.png&quot;;  </div><div class="line">    // FREAK  </div><div class="line">    Mat imgA_Freak = imread(filename1);  </div><div class="line">    Mat imgB_Freak = imread(filename2);  </div><div class="line">    vector&lt;KeyPoint&gt; keypointsA_Freak, keypointsB_Freak;  </div><div class="line">    Mat descriptorsA_Freak, descriptorsB_Freak;  </div><div class="line">    vector&lt;DMatch&gt; matches_Freak;  </div><div class="line">  </div><div class="line">    // Detector </div><div class="line">    SurfFeatureDetector detector_Freak(200, 4);  </div><div class="line">  </div><div class="line">    // Descriptor  </div><div class="line">    FREAK freak;  </div><div class="line">  </div><div class="line">    // MAatcher</div><div class="line">    BruteForceMatcher&lt;HammingLUT&gt; matcher_Freak;  </div><div class="line">  </div><div class="line">     </div><div class="line">    detector_Freak.detect(imgA_Freak, keypointsA_Freak);  </div><div class="line">    detector_Freak.detect(imgB_Freak, keypointsB_Freak);  </div><div class="line">   </div><div class="line">    freak.compute(imgA_Freak, keypointsA_Freak, descriptorsA_Freak);  </div><div class="line">    freak.compute(imgB_Freak, keypointsB_Freak, descriptorsB_Freak);  </div><div class="line">  </div><div class="line">    matcher_Freak.match(descriptorsA_Freak, descriptorsB_Freak, matches_Freak);   </div><div class="line">  </div><div class="line">    double max_dist = 0;  </div><div class="line">    double min_dist = 100;  </div><div class="line">    </div><div class="line">    for (int i=0; i&lt;descriptorsA_Freak.rows; i++)  </div><div class="line">    &#123;   </div><div class="line">        double dist = matches_Freak[i].distance;  </div><div class="line">        if (dist &lt; min_dist) min_dist = dist;  </div><div class="line">        if(dist &gt; max_dist) max_dist = dist;  </div><div class="line">    &#125;  </div><div class="line">    </div><div class="line">    vector&lt;DMatch&gt; good_matches_Freak;  </div><div class="line">    for (int i=0; i&lt;descriptorsA_Freak.rows; i++)  </div><div class="line">    &#123;   </div><div class="line">        if(matches_Freak[i].distance &lt; 0.7*max_dist)  </div><div class="line">        &#123;   </div><div class="line">            good_matches_Freak.push_back(matches_Freak[i]);   </div><div class="line">        &#125;  </div><div class="line">    &#125;  </div><div class="line">  </div><div class="line">    </div><div class="line">    Mat imgMatch_Freak;  </div><div class="line">    drawMatches(imgA_Freak, keypointsA_Freak, imgB_Freak, keypointsB_Freak, good_matches_Freak, imgMatch_Freak,  </div><div class="line">        Scalar::all(-1), Scalar::all(-1),  </div><div class="line">        vector&lt;char&gt;(), DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS);  </div><div class="line">    </div><div class="line">    imshow(&quot;matchFREAK&quot;, imgMatch_Freak);  </div><div class="line">      </div><div class="line">    waitKey(0);  </div><div class="line">    return 0;  </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2017/07/15/596a253ce5bb7.jpg" alt="result.jpg" title="ORB(左)和FREAK(右)的对比实验结果"></p>
<p>从图示对比实验结果可以看出，ORB的特征点匹配效果要好。FREAK的突出特点在于将人眼视网膜的视觉机理引入了随机点对的采样模式，以及在特征匹配时采用Saccadic Search由粗到精的级联匹配方式，提高了特征匹配速度。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://infoscience.epfl.ch/record/175537/files/2069.pdf" target="_blank" rel="external">Paper: FREAK: Fast Retina Keypoint</a></li>
<li><a href="https://gilscvblog.com/2013/12/09/a-tutorial-on-binary-descriptors-part-5-the-freak-descriptor/" target="_blank" rel="external">https://gilscvblog.com/2013/12/09/a-tutorial-on-binary-descriptors-part-5-the-freak-descriptor/</a></li>
<li><a href="http://blog.csdn.net/hujingshuang/article/details/47060677" target="_blank" rel="external">http://blog.csdn.net/hujingshuang/article/details/47060677</a></li>
<li><a href="http://blog.csdn.net/lhanchao/article/details/52744514" target="_blank" rel="external">http://blog.csdn.net/lhanchao/article/details/52744514</a></li>
<li><a href="http://blog.csdn.net/yang_xian521/article/details/7732835" target="_blank" rel="external">http://blog.csdn.net/yang_xian521/article/details/7732835</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;FREAK算法来源2012年CVPR上的一篇文章&lt;a href=&quot;https://infoscience.epfl.ch/record/175537/files/2069.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;FREAK: Fast Retina Keypoint&lt;/a&gt;，与ORB、BRISK算法类似，FREAK也是一种基于二进制编码的图像特征描述子，计算较快，对噪声鲁棒，具有尺度不变性和旋转不变性。此外，该算法还有一个突出特点就是受到人眼视网膜视觉机理的启发而提出。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征描述子之BRISK</title>
    <link href="https://senitco.github.io/2017/07/12/image-feature-brisk/"/>
    <id>https://senitco.github.io/2017/07/12/image-feature-brisk/</id>
    <published>2017-07-11T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;BRISK(Binary Robust Invariant Scalable Keypoints)是BRIEF算法的一种改进，也是一种基于二进制编码的特征描述子，而且对噪声鲁棒，具有尺度不变性和旋转不变性。<br><a id="more"></a></p>
<h3 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h3><p>&emsp;&emsp;BRISK主要利用FAST算法进行特征点检测，为了满足尺度不变性，BRISK构造图像金字塔在多尺度空间检测特征点。</p>
<h4 id="构建尺度空间"><a href="#构建尺度空间" class="headerlink" title="构建尺度空间"></a>构建尺度空间</h4><p>&emsp;&emsp;尺度空间包含n个octave($c_i$表示)和n个intro-octave($d_i$表示)，原论文中n=4。$c_0$是原始图像，$c_{i+1}$是$c_i$的降采样图像，缩放因子为2，即$c_{i+1}$的宽高分别为$c_i$的1/2；$d_0$是相对于原图缩放因子为1.5的降采样图像，同样，$d_{i+1}$是$d_i$的2倍降采样。$c_i$、$d_i$与原图像的大小关系如下表所示。</p>
<table>
<thead>
<tr>
<th style="text-align:center">image</th>
<th style="text-align:center">$c_0$</th>
<th style="text-align:center">$d_0$</th>
<th style="text-align:center">$c_1$</th>
<th style="text-align:center">$d_1$</th>
<th style="text-align:center">$c_2$</th>
<th style="text-align:center">$d_2$</th>
<th style="text-align:center">$c_3$</th>
<th style="text-align:center">$d_3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">width</td>
<td style="text-align:center">w</td>
<td style="text-align:center">2w/3</td>
<td style="text-align:center">w/2</td>
<td style="text-align:center">w/3</td>
<td style="text-align:center">w/4</td>
<td style="text-align:center">w/6</td>
<td style="text-align:center">w/8</td>
<td style="text-align:center">w/12   </td>
</tr>
<tr>
<td style="text-align:center">height</td>
<td style="text-align:center">h</td>
<td style="text-align:center">2h/2</td>
<td style="text-align:center">h/2</td>
<td style="text-align:center">h/3</td>
<td style="text-align:center">h/4</td>
<td style="text-align:center">h/6</td>
<td style="text-align:center">h/8</td>
<td style="text-align:center">h/12 </td>
</tr>
</tbody>
</table>
<p>由于n = 4，一共可以得到8张不同尺度的图像。在多尺度空间中，利用FAST9-16检测算子定位特征点，即在特征点邻域边界圆上的16个像素，至少有9个连续像素与特征点的灰度差值大于给定阈值T。此外，对原图像进行一次FAST5-8角点检测，作为$d_{-1}$层，方便后续在做非极大值抑制处理时，可以对相邻尺度空间的图像特征点进行比对。在前面博文中已详细介绍<a href="https://senitco.github.io/2017/07/10/image-feature-fast/">FAST角点检测</a>。</p>
<h4 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h4><p>&emsp;&emsp;对多尺度空间的9幅图像进行非极大值抑制，与SIFT算法类似，在特征点的图像空间(8邻域)和尺度空间(上下两层18邻域)共26个邻域点做比较，FAST响应值须取得极大值，否则不能作为特征点。由于是在离散坐标空间中，特征点的位置比较粗糙，还需进一步精确定位。</p>
<h4 id="亚像素精确定位"><a href="#亚像素精确定位" class="headerlink" title="亚像素精确定位"></a>亚像素精确定位</h4><p>&emsp;&emsp;得到图像特征点的坐标和尺度信息后，在极值点所在层及其上下层所对应的位置，对3个相应关键点的FAST响应进行二维二次函数插值(x,y方向)，得到二维平面精确的极值点位置和响应值后，再对尺度方向进行一维插值，得到特征点所对应的精确尺度。</p>
<p><img src="https://i.loli.net/2017/07/15/59698a0fcb428.jpg" alt="multi-scale.jpg"></p>
<h3 id="特征点描述"><a href="#特征点描述" class="headerlink" title="特征点描述"></a>特征点描述</h3><p>&emsp;&emsp;给定一组特征点(包含亚像素图像位置和浮点型尺度值)，BRISK通过比较邻域Patch内像素点对的灰度值，并进行二进制编码得到特征描述子。为了满足旋转不变性，需要选取合适的特征点主方向。</p>
<h4 id="采样模式和旋转估计"><a href="#采样模式和旋转估计" class="headerlink" title="采样模式和旋转估计"></a>采样模式和旋转估计</h4><p>&emsp;&emsp;特征点邻域的采样模式如下图所示，以特征点为中心，构建不同半径的同心圆，在每个圆上获取一定数目的等间隔采样点，所有采样点包括特征点一共有$N$个。由于这种采样模式会引起混叠效应，需要对所有采样点进行高斯滤波，滤波半径$r$和高斯方差$\sigma$成正比，同心圆半径越大，采样点滤波半径也越大。</p>
<p><img src="https://i.loli.net/2017/07/15/596996704acaf.jpg" alt="sample pattern.jpg"></p>
<p>$N$个采样点两两组合共有$\dfrac{N(N-1)}{2}$个点对，用集合$\cal A$表示，$I(p_i, \sigma_i)$为像素灰度值，$\sigma$表示尺度，用$g(p_i, p_j)$表示特征点局部梯度值，其计算公式为<br>$$g(p_i, p_j)=(p_j - p_i) \dfrac{I(p_j, \sigma_j) - I(p_i, \sigma_i)}{||p_j - p_i||^2}$$<br>采样点对的集合表示为<br>$$\cal A = \lbrace (p_i, p_j) \in R^2 \times R^2 | i &lt; N \wedge j &lt; i \rbrace $$<br>定义短距离点对子集$\cal S$和长距离点对子集$\cal L$<br>$$\cal S = \lbrace (p_i, p_j) \in \cal A | \left| p_j - p_i \right| &lt; \sigma_{max} \rbrace \subseteq \cal A$$<br>$$\cal L = \lbrace (p_i, p_j) \in \cal A | \left| p_j - p_i \right| &gt; \sigma_{min} \rbrace \subseteq \cal A$$<br>式中，阈值分别设置为$\sigma_{max} = 9.57t, \sigma_{min} = 13.67t$，$t$是特征点所在的尺度。特征点的主方向计算如下(此处仅用到了长距离点对子集)：<br>$$g = \left( \begin{matrix} g_x \\ g_y\end{matrix} \right) = \dfrac{1}{L} \Sigma_{p_i,p_j \in \cal L} g(p_i, p_j)$$<br>$$\alpha = arctan2(g_y, g_x)$$<br>长距离的点对均参与了运算，基于本地梯度互相抵消的假设，全局梯度的计算是不必要的。</p>
<h4 id="生成描述子"><a href="#生成描述子" class="headerlink" title="生成描述子"></a>生成描述子</h4><p>&emsp;&emsp;要解决旋转不变性，需要对特征点周围的采样区域旋转至主方向，得到新的采样区域，采样模式同上。采样点集合中包含$\dfrac{N(N-1)}{2}$个采样点对，考虑其中短距离点对子集中的512个点对，进行二进制编码，编码方式如下：<br>$$b = \begin{cases} 1, &emsp;I(p_j^\alpha, \sigma_j) &gt; I(p_i^\alpha, \sigma_i) \\ 0, &emsp;otherwise \end{cases}, &emsp; \forall (p_i^\alpha, p_j^\alpha) \in \cal S$$<br>$p_i^\alpha$表示旋转角度$\alpha$后的采样点。由此得到512bit也就是64Byte的二进制编码(BRISK64)。</p>
<h3 id="特征点匹配"><a href="#特征点匹配" class="headerlink" title="特征点匹配"></a>特征点匹配</h3><p>&emsp;&emsp;BRISK特征匹配和BRIEF一样，都是通过计算特征描述子的Hamming距离来实现。</p>
<h3 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h3><p>&emsp;&emsp;简单总结，BRISK算法具有较好的尺度不变性、旋转不变性，以及抗噪性能。在图像特征点的检测与匹配中，计算速度优于SIFT、SURF，而次于FREAK、ORB。对于较模糊的图像，能够取得较为出色的匹配结果。</p>
<h3 id="Experiment-amp-Result"><a href="#Experiment-amp-Result" class="headerlink" title="Experiment &amp; Result"></a>Experiment &amp; Result</h3><p>OpenCV实现BRISK特征检测与描述<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">#include &lt;opencv2/highgui/highgui.hpp&gt;  </div><div class="line">#include &lt;opencv2/core/core.hpp&gt;  </div><div class="line">#include &lt;opencv2/nonfree/features2d.hpp&gt;  </div><div class="line">#include &lt;opencv2/nonfree/nonfree.hpp&gt;  </div><div class="line">  </div><div class="line">using namespace cv;  </div><div class="line">using namespace std;  </div><div class="line">  </div><div class="line">int main()  </div><div class="line">&#123;  </div><div class="line">    //Load Image  </div><div class="line">    Mat c_src1 =  imread( &quot;1.png&quot;);  </div><div class="line">    Mat c_src2 = imread(&quot;2.png&quot;);  </div><div class="line">    Mat src1 = imread( &quot;1.png&quot;, CV_LOAD_IMAGE_GRAYSCALE);  </div><div class="line">    Mat src2 = imread( &quot;2.png&quot;, CV_LOAD_IMAGE_GRAYSCALE);  </div><div class="line">    if( !src1.data || !src2.data )  </div><div class="line">    &#123;  </div><div class="line">        cout&lt;&lt; &quot;Error reading images &quot; &lt;&lt; std::endl;  </div><div class="line">        return -1;  </div><div class="line">    &#125;  </div><div class="line"></div><div class="line">    //feature detect  </div><div class="line">    BRISK detector;  </div><div class="line">    vector&lt;KeyPoint&gt; kp1, kp2;     </div><div class="line">    detector.detect( src1, kp1 );  </div><div class="line">    detector.detect( src2, kp2 ); </div><div class="line"></div><div class="line">    //cv::BRISK extractor;  </div><div class="line">    Mat des1,des2;//descriptor  </div><div class="line">    detector.compute(src1, kp1, des1);  </div><div class="line">    detector.compute(src2, kp2, des2);  </div><div class="line">    Mat res1,res2;  </div><div class="line">    int drawmode = DrawMatchesFlags::DRAW_RICH_KEYPOINTS;  </div><div class="line">    drawKeypoints(c_src1, kp1, res1, Scalar::all(-1), drawmode);</div><div class="line">    drawKeypoints(c_src2, kp2, res2, Scalar::all(-1), drawmode);  </div><div class="line">     </div><div class="line">    BFMatcher matcher(NORM_HAMMING);  </div><div class="line">    vector&lt;DMatch&gt; matches;  </div><div class="line">    matcher.match(des1, des2, matches);  </div><div class="line">   </div><div class="line">    Mat img_match;  </div><div class="line">    drawMatches(src1, kp1, src2, kp2, matches, img_match);   </div><div class="line">    imshow(&quot;matches&quot;,img_match);  </div><div class="line">    cvWaitKey(0);  </div><div class="line">    cvDestroyAllWindows();  </div><div class="line">    return 0;  </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><img src="https://i.loli.net/2017/07/15/5969c49105899.jpg" alt="result.jpg"></p>
<p>OpenCV中BRISK算法的部分源码实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div></pre></td><td class="code"><pre><div class="line">// construct the image pyramids</div><div class="line">void BriskScaleSpace::constructPyramid(const cv::Mat&amp; image)  </div><div class="line">&#123;  </div><div class="line">  </div><div class="line">  // set correct size:  </div><div class="line">  pyramid_.clear();  </div><div class="line">  </div><div class="line">  // fill the pyramid:  </div><div class="line">  pyramid_.push_back(BriskLayer(image.clone()));  </div><div class="line">  if (layers_ &gt; 1)  </div><div class="line">  &#123;  </div><div class="line">    pyramid_.push_back(BriskLayer(pyramid_.back(), BriskLayer::CommonParams::TWOTHIRDSAMPLE)); </div><div class="line">  &#125;  </div><div class="line">  const int octaves2 = layers_;  </div><div class="line">  </div><div class="line">  for (uchar i = 2; i &lt; octaves2; i += 2)  </div><div class="line">  &#123;  </div><div class="line">    pyramid_.push_back(BriskLayer(pyramid_[i - 2], BriskLayer::CommonParams::HALFSAMPLE));//  </div><div class="line">    pyramid_.push_back(BriskLayer(pyramid_[i - 1], BriskLayer::CommonParams::HALFSAMPLE));//  </div><div class="line">  &#125;  </div><div class="line">&#125;  </div><div class="line"></div><div class="line"></div><div class="line">//extract the feature points </div><div class="line">void BriskScaleSpace::getKeypoints(const int threshold_, std::vector&lt;cv::KeyPoint&gt;&amp; keypoints)  </div><div class="line">&#123;  </div><div class="line">  // make sure keypoints is empty  </div><div class="line">  keypoints.resize(0);  </div><div class="line">  keypoints.reserve(2000);  </div><div class="line">  </div><div class="line">  // assign thresholds  </div><div class="line">  int safeThreshold_ = (int)(threshold_ * safetyFactor_);  </div><div class="line">  std::vector&lt;std::vector&lt;cv::KeyPoint&gt; &gt; agastPoints;  </div><div class="line">  agastPoints.resize(layers_);  </div><div class="line">  </div><div class="line">  // go through the octaves and intra layers and calculate fast corner scores:  </div><div class="line">  for (int i = 0; i &lt; layers_; i++)  </div><div class="line">  &#123;  </div><div class="line">    // call OAST16_9 without nms  </div><div class="line">    BriskLayer&amp; l = pyramid_[i];  </div><div class="line">    l.getAgastPoints(safeThreshold_, agastPoints[i]);  </div><div class="line">  &#125;  </div><div class="line">  </div><div class="line">  if (layers_ == 1)  </div><div class="line">  &#123;  </div><div class="line">    // just do a simple 2d subpixel refinement...  </div><div class="line">    const size_t num = agastPoints[0].size();  </div><div class="line">    for (size_t n = 0; n &lt; num; n++)  </div><div class="line">    &#123;  </div><div class="line">      const cv::Point2f&amp; point = agastPoints.at(0)[n].pt;  </div><div class="line">      // first check if it is a maximum:  </div><div class="line">      if (!isMax2D(0, (int)point.x, (int)point.y))  </div><div class="line">        continue;  </div><div class="line">  </div><div class="line">      // let&apos;s do the subpixel and float scale refinement:  </div><div class="line">      BriskLayer&amp; l = pyramid_[0];  </div><div class="line">      int s_0_0 = l.getAgastScore(point.x - 1, point.y - 1, 1);  </div><div class="line">      int s_1_0 = l.getAgastScore(point.x, point.y - 1, 1);  </div><div class="line">      int s_2_0 = l.getAgastScore(point.x + 1, point.y - 1, 1);  </div><div class="line">      int s_2_1 = l.getAgastScore(point.x + 1, point.y, 1);  </div><div class="line">      int s_1_1 = l.getAgastScore(point.x, point.y, 1);  </div><div class="line">      int s_0_1 = l.getAgastScore(point.x - 1, point.y, 1);  </div><div class="line">      int s_0_2 = l.getAgastScore(point.x - 1, point.y + 1, 1);  </div><div class="line">      int s_1_2 = l.getAgastScore(point.x, point.y + 1, 1);  </div><div class="line">      int s_2_2 = l.getAgastScore(point.x + 1, point.y + 1, 1);  </div><div class="line">      float delta_x, delta_y;  </div><div class="line">      float max = subpixel2D(s_0_0, s_0_1, s_0_2, s_1_0, s_1_1, s_1_2, s_2_0, s_2_1, s_2_2, delta_x, delta_y);  </div><div class="line">  </div><div class="line">      // store:  </div><div class="line">      keypoints.push_back(cv::KeyPoint(float(point.x) + delta_x, float(point.y) + delta_y, basicSize_, -1, max, 0));  </div><div class="line">  </div><div class="line">    &#125;  </div><div class="line">  </div><div class="line">    return;  </div><div class="line">  &#125;  </div><div class="line">  </div><div class="line">  float x, y, scale, score;  </div><div class="line">  for (int i = 0; i &lt; layers_; i++)  </div><div class="line">  &#123;  </div><div class="line">    BriskLayer&amp; l = pyramid_[i];  </div><div class="line">    const size_t num = agastPoints[i].size();  </div><div class="line">    if (i == layers_ - 1)  </div><div class="line">    &#123;  </div><div class="line">      for (size_t n = 0; n &lt; num; n++)  </div><div class="line">      &#123;  </div><div class="line">        const cv::Point2f&amp; point = agastPoints.at(i)[n].pt;  </div><div class="line">        // consider only 2D maxima...  </div><div class="line">        if (!isMax2D(i, (int)point.x, (int)point.y))  </div><div class="line">          continue;  </div><div class="line">  </div><div class="line">        bool ismax;  </div><div class="line">        float dx, dy;  </div><div class="line">        getScoreMaxBelow(i, (int)point.x, (int)point.y, l.getAgastScore(point.x, point.y, safeThreshold_), ismax, dx, dy);  </div><div class="line">        if (!ismax)  </div><div class="line">          continue;  </div><div class="line">  </div><div class="line">        // get the patch on this layer:  </div><div class="line">        int s_0_0 = l.getAgastScore(point.x - 1, point.y - 1, 1);  </div><div class="line">        int s_1_0 = l.getAgastScore(point.x, point.y - 1, 1);  </div><div class="line">        int s_2_0 = l.getAgastScore(point.x + 1, point.y - 1, 1);  </div><div class="line">        int s_2_1 = l.getAgastScore(point.x + 1, point.y, 1);  </div><div class="line">        int s_1_1 = l.getAgastScore(point.x, point.y, 1);  </div><div class="line">        int s_0_1 = l.getAgastScore(point.x - 1, point.y, 1);  </div><div class="line">        int s_0_2 = l.getAgastScore(point.x - 1, point.y + 1, 1);  </div><div class="line">        int s_1_2 = l.getAgastScore(point.x, point.y + 1, 1);  </div><div class="line">        int s_2_2 = l.getAgastScore(point.x + 1, point.y + 1, 1);  </div><div class="line">        float delta_x, delta_y;  </div><div class="line">        float max = subpixel2D(s_0_0, s_0_1, s_0_2, s_1_0, s_1_1, s_1_2, s_2_0, s_2_1, s_2_2, delta_x, delta_y);  </div><div class="line">  </div><div class="line">        // store:  </div><div class="line">        keypoints.push_back(cv::KeyPoint((float(point.x) + delta_x) * l.scale() + l.offset(), (float(point.y) + delta_y) * l.scale() + l.offset(), basicSize_ * l.scale(), -1, max, i));  </div><div class="line">      &#125;  </div><div class="line">    &#125;  </div><div class="line">    else  </div><div class="line">    &#123;  </div><div class="line">      // not the last layer:  </div><div class="line">      for (size_t n = 0; n &lt; num; n++)  </div><div class="line">      &#123;  </div><div class="line">        const cv::Point2f&amp; point = agastPoints.at(i)[n].pt;  </div><div class="line">  </div><div class="line">        // first check if it is a maximum:  </div><div class="line">        if (!isMax2D(i, (int)point.x, (int)point.y))  </div><div class="line">          continue;  </div><div class="line">  </div><div class="line">        // let&apos;s do the subpixel and float scale refinement:  </div><div class="line">        bool ismax=false;  </div><div class="line">        score = refine3D(i, (int)point.x, (int)point.y, x, y, scale, ismax);  </div><div class="line">        if (!ismax)  </div><div class="line">        &#123;  </div><div class="line">          continue;  </div><div class="line">        &#125;  </div><div class="line">  </div><div class="line">        // finally store the detected keypoint:  </div><div class="line">        if (score &gt; float(threshold_))  </div><div class="line">        &#123;  </div><div class="line">          keypoints.push_back(cv::KeyPoint(x, y, basicSize_ * scale, -1, score, i));  </div><div class="line">        &#125;  </div><div class="line">      &#125;  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/brisk.pdf" target="_blank" rel="external">Paper: BRISK: Binary Robust Invariant Scalable Keypoints</a></li>
<li><a href="http://blog.csdn.net/jinxueliu31/article/details/18556855" target="_blank" rel="external">http://blog.csdn.net/jinxueliu31/article/details/18556855</a></li>
<li><a href="http://blog.csdn.net/hujingshuang/article/details/47045497" target="_blank" rel="external">http://blog.csdn.net/hujingshuang/article/details/47045497</a></li>
<li><a href="http://blog.csdn.net/luoshixian099/article/details/50731801" target="_blank" rel="external">http://blog.csdn.net/luoshixian099/article/details/50731801</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4260167.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4260167.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;BRISK(Binary Robust Invariant Scalable Keypoints)是BRIEF算法的一种改进，也是一种基于二进制编码的特征描述子，而且对噪声鲁棒，具有尺度不变性和旋转不变性。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征描述子之ORB</title>
    <link href="https://senitco.github.io/2017/07/09/image-feature-orb/"/>
    <id>https://senitco.github.io/2017/07/09/image-feature-orb/</id>
    <published>2017-07-08T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;ORB(Oriented FAST and Rotated BRIEF)算法是对FAST特征点检测和BRIEF特征描述子的一种结合，在原有的基础上做了改进与优化，使得ORB特征具备多种局部不变性，并为实时计算提供了可能。<br><a id="more"></a></p>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h3 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h3><p>&emsp;&emsp;ORB首先利用FAST算法检测特征点，然后计算每个特征点的Harris角点响应值，从中筛选出$N$个最大的特征点，Harris角点的响应函数如下：<br>$$R = detM-\alpha (traceM)^2$$<br>相关内容已在前面的博文<a href="https://senitco.github.io/2017/07/10/image-feature-fast/">FAST角点检测</a>和<a href="https://senitco.github.io/2017/06/28/image-feature-harris/">Harris角点检测</a>分别做了详细的介绍。<br>FAST检测特征点不具备尺度不变性，可以像SIFT特征一样，借助尺度空间理论构建图像高斯金字塔，然后在每一层金字塔图像上检测角点，以实现尺度不变性。对于旋转不变性，原论文中提出了一种利用图像矩(几何矩)，在半径为r的邻域内求取灰度质心的方法，从特征点到灰度质心的向量，定义为该特征点的主方向。图像矩定义如下：<br>$$m_{pq}=\Sigma_{x,y} x^p y^q I(x,y),&emsp;x,y \in [-r,r]$$<br>$I(x,y)$表示像素灰度值，0阶矩$m_{00}$即图像邻域窗口内所有像素的灰度和，$m_{10}$和$m_{01}$分别相对$x$和相对$y$的一阶矩，因此图像局部邻域的中心矩或者质心可定义为<br>$$C = ( \dfrac{m_{10}}{m_{00}}, \dfrac{m_{01}}{m_{00}}) $$<br>特征点与质心形成的向量与$X$轴的夹角定义为特征点的主方向<br>$$\theta = arctan(m_{01}, m_{10})$$</p>
<h3 id="特征点描述"><a href="#特征点描述" class="headerlink" title="特征点描述"></a>特征点描述</h3><p>&emsp;&emsp;ORB采用BRIEF作为特征描述方法，BRIEF虽然速度优势明显，但也存在一些缺陷，例如不具备尺度不变性和旋转不变性，对噪声敏感。尺度不变性的问题在利用FAST检测特征点时，通过构建高斯金字塔得以解决。BRIEF中采用$9 \times 9$的高斯卷积核进行滤波降噪，可以在一定程度上缓解噪声敏感问题；ORB中利用积分图像，在$31 \times 31$的Patch中选取随机点对，并以选取的随机点为中心，在$5 \times 5$的窗口内计算灰度平均值(灰度和)，比较随机点对的邻域灰度均值，进行二进制编码，而不是仅仅由两个随机点对的像素值决定编码结果，可以有效地解决噪声问题。<br>&emsp;&emsp;至于旋转不变性问题，可利用FAST特征点检测时求取的主方向，旋转特征点邻域，但旋转整个Patch再提取BRIEF特征描述子的计算代价较大，因此，ORB采用了一种更高效的方式，在每个特征点邻域Patch内，先选取256对随机点，将其进行旋转，然后做判决编码为二进制串。n个点对构成矩阵$S$<br>$$S=\left[ \begin{matrix} x_1 &amp; x_2 &amp; \ldots  &amp; x_{2n}\\ y_1 &amp; y_2 &amp; \ldots &amp; y_{2n}\end{matrix} \right]$$<br>旋转矩阵$R_{\theta}为$<br>$$R_{\theta} = \left[ \begin{matrix} cos \theta &amp; -sin \theta \\ sin \theta &amp; cos \theta \end{matrix} \right]$$<br>旋转后的坐标矩阵为<br>$$S_{\theta} = R_{\theta}S$$</p>
<h3 id="描述子的区分性"><a href="#描述子的区分性" class="headerlink" title="描述子的区分性"></a>描述子的区分性</h3><p>&emsp;&emsp;通过上述方法得到的特征描述子具有旋转不变性，称为steered BRIEF(sBRIEF)，但匹配效果却不如原始BRIEF算法，因为可区分性减弱了。特征描述子的一个要求就是要尽可能地表达特征点的独特性，便于区分不同的特征点。如下图所示，为几种特征描述子的均值分布，横轴为均值与0.5之间的距离，纵轴为相应均值下特征点的统计数量。可以看出，BRIEF描述子所有比特位的均值接近于0.5，且方差很大；方差越大表明可区分性越好。不同特征点的描述子表现出较大的差异性，不易造成无匹配。但steered BRIEF进行了坐标旋转，损失了这个特性，导致可区分性减弱，相关性变强，不利于匹配。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/14/5968d134c4b7e.jpg" alt="sBRIEF-rBRIEF.jpg"></p>
<p>&emsp;&emsp;为了解决steered BRIEF可区分性降低的问题，ORB使用了一种基于学习的方法来选择一定数量的随机点对。首先建立一个大约300k特征点的数据集(特征点来源于PASCAL2006中的图像)，对每个特征点，考虑其$31 \times 31$的邻域Patch，为了去除噪声的干扰，选择$5 \times 5$的子窗口的灰度均值代替单个像素的灰度，这样每个Patch内就有$N = (31-5+1) \times (31-5+1) = 27 \times 27 = 729$个子窗口，从中随机选取2个非重复的子窗口，一共有$M = C_N ^ 2$中方法。这样，每个特征点便可提取出一个长度为$M$的二进制串，所有特征点可构成一个$300k \times M$的二进制矩阵$Q$，矩阵中每个元素取值为0或1。现在需要从$M$个点对中选取256个相关性最小、可区分性最大的点对，作为最终的二进制编码。筛选方法如下：</p>
<ul>
<li>对矩阵$Q$的每一列求取均值，并根据均值与0.5之间的距离从小到大的顺序，依次对所有列向量进行重新排序，得到矩阵$T$</li>
<li>将$T$中的第一列向量放到结果矩阵$R$中</li>
<li>取出$T$中的下一列向量，计算其与矩阵$R$中所有列向量的相关性，如果相关系数小于给定阈值，则将$T$中的该列向量移至矩阵$R$中，否则丢弃</li>
<li>循环执行上一步，直到$R$中有256个列向量；如果遍历$T$中所有列，$R$中向量列数还不满256，则增大阈值，重复以上步骤。</li>
</ul>
<p>这样，最后得到的就是相关性最小的256对随机点，该方法称为rBRIEF。</p>
<h3 id="Experiment-amp-Result"><a href="#Experiment-amp-Result" class="headerlink" title="Experiment &amp; Result"></a>Experiment &amp; Result</h3><p>OpenCV实现ORB特征检测与描述<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">#include &lt;opencv2/core/core.hpp&gt; </div><div class="line">#include &lt;opencv2/highgui/highgui.hpp&gt; </div><div class="line">#include &lt;opencv2/imgproc/imgproc.hpp&gt; </div><div class="line">#include &lt;opencv2/features2d/features2d.hpp&gt;</div><div class="line"></div><div class="line">using namespace cv;</div><div class="line"></div><div class="line">int main(int argc, char** argv) </div><div class="line">&#123; </div><div class="line">    Mat img_1 = imread(&quot;box.png&quot;); </div><div class="line">    Mat img_2 = imread(&quot;box_in_scene.png&quot;);</div><div class="line"></div><div class="line">    // -- Step 1: Detect the keypoints using STAR Detector </div><div class="line">    std::vector&lt;KeyPoint&gt; keypoints_1,keypoints_2; </div><div class="line">    ORB orb; </div><div class="line">    orb.detect(img_1, keypoints_1); </div><div class="line">    orb.detect(img_2, keypoints_2);</div><div class="line"></div><div class="line">    // -- Stpe 2: Calculate descriptors (feature vectors) </div><div class="line">    Mat descriptors_1, descriptors_2; </div><div class="line">    orb.compute(img_1, keypoints_1, descriptors_1); </div><div class="line">    orb.compute(img_2, keypoints_2, descriptors_2);</div><div class="line"></div><div class="line">    //-- Step 3: Matching descriptor vectors with a brute force matcher </div><div class="line">    BFMatcher matcher(NORM_HAMMING); </div><div class="line">    std::vector&lt;DMatch&gt; mathces; </div><div class="line">    matcher.match(descriptors_1, descriptors_2, mathces); </div><div class="line">    // -- dwaw matches </div><div class="line">    Mat img_mathes; </div><div class="line">    drawMatches(img_1, keypoints_1, img_2, keypoints_2, mathces, img_mathes); </div><div class="line">    // -- show </div><div class="line">    imshow(&quot;Mathces&quot;, img_mathes);</div><div class="line"></div><div class="line">    waitKey(0); </div><div class="line">    return 0; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><img src="https://ooo.0o0.ooo/2017/07/14/5968d13635388.png" alt="result.png"></p>
<p>OpenCV中ORB算法的部分源码实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div></pre></td><td class="code"><pre><div class="line">//计算Harris角点响应  </div><div class="line">static void HarrisResponses(const Mat&amp; img, vector&lt;KeyPoint&gt;&amp; pts, int blockSize, float harris_k)  </div><div class="line">&#123;  </div><div class="line">    CV_Assert( img.type() == CV_8UC1 &amp;&amp; blockSize*blockSize &lt;= 2048 );  </div><div class="line">  </div><div class="line">    size_t ptidx, ptsize = pts.size();  </div><div class="line">  </div><div class="line">    const uchar* ptr00 = img.ptr&lt;uchar&gt;();  </div><div class="line">    int step = (int)(img.step/img.elemSize1());  </div><div class="line">    int r = blockSize/2;  </div><div class="line">  </div><div class="line">    float scale = (1 &lt;&lt; 2) * blockSize * 255.0f;  </div><div class="line">    scale = 1.0f / scale;  </div><div class="line">    float scale_sq_sq = scale * scale * scale * scale;  </div><div class="line">  </div><div class="line">    AutoBuffer&lt;int&gt; ofsbuf(blockSize*blockSize);  </div><div class="line">    int* ofs = ofsbuf;  </div><div class="line">    for( int i = 0; i &lt; blockSize; i++ )  </div><div class="line">        for( int j = 0; j &lt; blockSize; j++ )  </div><div class="line">            ofs[i*blockSize + j] = (int)(i*step + j);  </div><div class="line">  </div><div class="line">    for( ptidx = 0; ptidx &lt; ptsize; ptidx++ )  </div><div class="line">    &#123;  </div><div class="line">        int x0 = cvRound(pts[ptidx].pt.x - r);  </div><div class="line">        int y0 = cvRound(pts[ptidx].pt.y - r);  </div><div class="line">  </div><div class="line">        const uchar* ptr0 = ptr00 + y0*step + x0;  </div><div class="line">        int a = 0, b = 0, c = 0;  </div><div class="line">  </div><div class="line">        for( int k = 0; k &lt; blockSize*blockSize; k++ )  </div><div class="line">        &#123;  </div><div class="line">            const uchar* ptr = ptr0 + ofs[k];  </div><div class="line">            int Ix = (ptr[1] - ptr[-1])*2 + (ptr[-step+1] - ptr[-step-1]) + (ptr[step+1] - ptr[step-1]);  </div><div class="line">            int Iy = (ptr[step] - ptr[-step])*2 + (ptr[step-1] - ptr[-step-1]) + (ptr[step+1] - ptr[-step+1]);  </div><div class="line">            a += Ix*Ix;  </div><div class="line">            b += Iy*Iy;  </div><div class="line">            c += Ix*Iy;  </div><div class="line">        &#125;  </div><div class="line">        pts[ptidx].response = ((float)a * b - (float)c * c -  harris_k * ((float)a + b) * ((float)a + b))*scale_sq_sq;  </div><div class="line">    &#125;  </div><div class="line">&#125;  </div><div class="line"></div><div class="line">//计算FAST角点的主方向  </div><div class="line">static float IC_Angle(const Mat&amp; image, const int half_k, Point2f pt, const vector&lt;int&gt; &amp; u_max)  </div><div class="line">&#123;  </div><div class="line">    int m_01 = 0, m_10 = 0;  </div><div class="line">  </div><div class="line">    const uchar* center = &amp;image.at&lt;uchar&gt; (cvRound(pt.y), cvRound(pt.x));  </div><div class="line">  </div><div class="line">    // Treat the center line differently, v=0  </div><div class="line">    for (int u = -half_k; u &lt;= half_k; ++u)  </div><div class="line">        m_10 += u * center[u];  </div><div class="line">  </div><div class="line">    // Go line by line in the circular patch  </div><div class="line">    int step = (int)image.step1();  </div><div class="line">    for (int v = 1; v &lt;= half_k; ++v)  </div><div class="line">    &#123;  </div><div class="line">        // Proceed over the two lines  </div><div class="line">        int v_sum = 0;  </div><div class="line">        int d = u_max[v];  </div><div class="line">        for (int u = -d; u &lt;= d; ++u)  </div><div class="line">        &#123;  </div><div class="line">            int val_plus = center[u + v*step], val_minus = center[u - v*step];  </div><div class="line">            v_sum += (val_plus - val_minus);  </div><div class="line">            m_10 += u * (val_plus + val_minus);  </div><div class="line">        &#125;  </div><div class="line">        m_01 += v * v_sum;  </div><div class="line">    &#125;  </div><div class="line">  </div><div class="line">    return fastAtan2((float)m_01, (float)m_10);  </div><div class="line">&#125;  </div><div class="line"></div><div class="line"></div><div class="line">//计算ORB特征描述子</div><div class="line">static void computeOrbDescriptor(const KeyPoint&amp; kpt, const Mat&amp; img, const Point* pattern, uchar* desc, int dsize, int WTA_K)  </div><div class="line">&#123;  </div><div class="line">    float angle = kpt.angle;   </div><div class="line">    //angle = cvFloor(angle/12)*12.f;  </div><div class="line">    angle *= (float)(CV_PI/180.f);  </div><div class="line">    float a = (float)cos(angle), b = (float)sin(angle);  </div><div class="line">  </div><div class="line">    const uchar* center = &amp;img.at&lt;uchar&gt;(cvRound(kpt.pt.y), cvRound(kpt.pt.x));  </div><div class="line">    int step = (int)img.step;  </div><div class="line">  </div><div class="line">#if 1  </div><div class="line">    #define GET_VALUE(idx) \       //取旋转后一个像素点的值  </div><div class="line">        center[cvRound(pattern[idx].x*b + pattern[idx].y*a)*step + \  </div><div class="line">               cvRound(pattern[idx].x*a - pattern[idx].y*b)]  </div><div class="line">#else  </div><div class="line">    float x, y;  </div><div class="line">    int ix, iy;  </div><div class="line">    #define GET_VALUE(idx) \ //取旋转后一个像素点，插值法  </div><div class="line">        (x = pattern[idx].x*a - pattern[idx].y*b, \  </div><div class="line">        y = pattern[idx].x*b + pattern[idx].y*a, \  </div><div class="line">        ix = cvFloor(x), iy = cvFloor(y), \  </div><div class="line">        x -= ix, y -= iy, \  </div><div class="line">        cvRound(center[iy*step + ix]*(1-x)*(1-y) + center[(iy+1)*step + ix]*(1-x)*y + \  </div><div class="line">                center[iy*step + ix+1]*x*(1-y) + center[(iy+1)*step + ix+1]*x*y))  </div><div class="line">#endif  </div><div class="line">  </div><div class="line">    if( WTA_K == 2 )  </div><div class="line">    &#123;  </div><div class="line">        for (int i = 0; i &lt; dsize; ++i, pattern += 16)//每个特征描述子长度为32个字节  </div><div class="line">        &#123;  </div><div class="line">            int t0, t1, val;  </div><div class="line">            t0 = GET_VALUE(0); t1 = GET_VALUE(1);  </div><div class="line">            val = t0 &lt; t1;  </div><div class="line">            t0 = GET_VALUE(2); t1 = GET_VALUE(3);  </div><div class="line">            val |= (t0 &lt; t1) &lt;&lt; 1;  </div><div class="line">            t0 = GET_VALUE(4); t1 = GET_VALUE(5);  </div><div class="line">            val |= (t0 &lt; t1) &lt;&lt; 2;  </div><div class="line">            t0 = GET_VALUE(6); t1 = GET_VALUE(7);  </div><div class="line">            val |= (t0 &lt; t1) &lt;&lt; 3;  </div><div class="line">            t0 = GET_VALUE(8); t1 = GET_VALUE(9);  </div><div class="line">            val |= (t0 &lt; t1) &lt;&lt; 4;  </div><div class="line">            t0 = GET_VALUE(10); t1 = GET_VALUE(11);  </div><div class="line">            val |= (t0 &lt; t1) &lt;&lt; 5;  </div><div class="line">            t0 = GET_VALUE(12); t1 = GET_VALUE(13);  </div><div class="line">            val |= (t0 &lt; t1) &lt;&lt; 6;  </div><div class="line">            t0 = GET_VALUE(14); t1 = GET_VALUE(15);  </div><div class="line">            val |= (t0 &lt; t1) &lt;&lt; 7;  </div><div class="line">  </div><div class="line">            desc[i] = (uchar)val;  </div><div class="line">        &#125;  </div><div class="line">    &#125;  </div><div class="line">    else if( WTA_K == 3 )  </div><div class="line">    &#123;  </div><div class="line">        for (int i = 0; i &lt; dsize; ++i, pattern += 12)  </div><div class="line">        &#123;  </div><div class="line">            int t0, t1, t2, val;  </div><div class="line">            t0 = GET_VALUE(0); t1 = GET_VALUE(1); t2 = GET_VALUE(2);  </div><div class="line">            val = t2 &gt; t1 ? (t2 &gt; t0 ? 2 : 0) : (t1 &gt; t0);  </div><div class="line">  </div><div class="line">            t0 = GET_VALUE(3); t1 = GET_VALUE(4); t2 = GET_VALUE(5);  </div><div class="line">            val |= (t2 &gt; t1 ? (t2 &gt; t0 ? 2 : 0) : (t1 &gt; t0)) &lt;&lt; 2;  </div><div class="line">  </div><div class="line">            t0 = GET_VALUE(6); t1 = GET_VALUE(7); t2 = GET_VALUE(8);  </div><div class="line">            val |= (t2 &gt; t1 ? (t2 &gt; t0 ? 2 : 0) : (t1 &gt; t0)) &lt;&lt; 4;  </div><div class="line">  </div><div class="line">            t0 = GET_VALUE(9); t1 = GET_VALUE(10); t2 = GET_VALUE(11);  </div><div class="line">            val |= (t2 &gt; t1 ? (t2 &gt; t0 ? 2 : 0) : (t1 &gt; t0)) &lt;&lt; 6;  </div><div class="line">  </div><div class="line">            desc[i] = (uchar)val;  </div><div class="line">        &#125;  </div><div class="line">    &#125;  </div><div class="line">    else if( WTA_K == 4 )  </div><div class="line">    &#123;  </div><div class="line">        for (int i = 0; i &lt; dsize; ++i, pattern += 16)  </div><div class="line">        &#123;  </div><div class="line">            int t0, t1, t2, t3, u, v, k, val;  </div><div class="line">            t0 = GET_VALUE(0); t1 = GET_VALUE(1);  </div><div class="line">            t2 = GET_VALUE(2); t3 = GET_VALUE(3);  </div><div class="line">            u = 0, v = 2;  </div><div class="line">            if( t1 &gt; t0 ) t0 = t1, u = 1;  </div><div class="line">            if( t3 &gt; t2 ) t2 = t3, v = 3;  </div><div class="line">            k = t0 &gt; t2 ? u : v;  </div><div class="line">            val = k;  </div><div class="line">  </div><div class="line">            t0 = GET_VALUE(4); t1 = GET_VALUE(5);  </div><div class="line">            t2 = GET_VALUE(6); t3 = GET_VALUE(7);  </div><div class="line">            u = 0, v = 2;  </div><div class="line">            if( t1 &gt; t0 ) t0 = t1, u = 1;  </div><div class="line">            if( t3 &gt; t2 ) t2 = t3, v = 3;  </div><div class="line">            k = t0 &gt; t2 ? u : v;  </div><div class="line">            val |= k &lt;&lt; 2;  </div><div class="line">  </div><div class="line">            t0 = GET_VALUE(8); t1 = GET_VALUE(9);  </div><div class="line">            t2 = GET_VALUE(10); t3 = GET_VALUE(11);  </div><div class="line">            u = 0, v = 2;  </div><div class="line">            if( t1 &gt; t0 ) t0 = t1, u = 1;  </div><div class="line">            if( t3 &gt; t2 ) t2 = t3, v = 3;  </div><div class="line">            k = t0 &gt; t2 ? u : v;  </div><div class="line">            val |= k &lt;&lt; 4;  </div><div class="line">  </div><div class="line">            t0 = GET_VALUE(12); t1 = GET_VALUE(13);  </div><div class="line">            t2 = GET_VALUE(14); t3 = GET_VALUE(15);  </div><div class="line">            u = 0, v = 2;  </div><div class="line">            if( t1 &gt; t0 ) t0 = t1, u = 1;  </div><div class="line">            if( t3 &gt; t2 ) t2 = t3, v = 3;  </div><div class="line">            k = t0 &gt; t2 ? u : v;  </div><div class="line">            val |= k &lt;&lt; 6;  </div><div class="line">  </div><div class="line">            desc[i] = (uchar)val;  </div><div class="line">        &#125;  </div><div class="line">    &#125;  </div><div class="line">    else  </div><div class="line">        CV_Error( CV_StsBadSize, &quot;Wrong WTA_K. It can be only 2, 3 or 4.&quot; );  </div><div class="line">  </div><div class="line">    #undef GET_VALUE  </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://218.199.87.242/cache/2/03/willowgarage.com/fccf2f159ae03231bd5f5e4335e0c429/orb_final.pdf" target="_blank" rel="external">Paper: ORB: an efficient alternative to SIFT or SURF</a></li>
<li><a href="http://blog.csdn.net/hujingshuang/article/details/46984411" target="_blank" rel="external">http://blog.csdn.net/hujingshuang/article/details/46984411</a></li>
<li><a href="http://blog.csdn.net/zouzoupaopao229/article/details/52625678" target="_blank" rel="external">http://blog.csdn.net/zouzoupaopao229/article/details/52625678</a></li>
<li><a href="https://segmentfault.com/a/1190000004200111" target="_blank" rel="external">https://segmentfault.com/a/1190000004200111</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4083537.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4083537.html</a></li>
<li><a href="http://blog.csdn.net/luoshixian099/article/details/48523267" target="_blank" rel="external">http://blog.csdn.net/luoshixian099/article/details/48523267</a></li>
<li><a href="http://blog.csdn.net/tiandijun/article/details/40679581" target="_blank" rel="external">http://blog.csdn.net/tiandijun/article/details/40679581</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;ORB(Oriented FAST and Rotated BRIEF)算法是对FAST特征点检测和BRIEF特征描述子的一种结合，在原有的基础上做了改进与优化，使得ORB特征具备多种局部不变性，并为实时计算提供了可能。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征描述子之BRIEF</title>
    <link href="https://senitco.github.io/2017/07/05/image-feature-brief/"/>
    <id>https://senitco.github.io/2017/07/05/image-feature-brief/</id>
    <published>2017-07-04T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;BRIEF(Binary Robust Independent Elementary Features)是一种对已检测到的特征点进行表示和描述的特征描述方法，和传统的利用图像局部邻域的灰度直方图或梯度直方图提取特征的方式不同，BRIEF是一种二进制编码的特征描述子，既降低了存储空间的需求，提升了特征描述子生成的速度，也减少了特征匹配时所需的时间。<br><a id="more"></a></p>
<h3 id="原理概述"><a href="#原理概述" class="headerlink" title="原理概述"></a>原理概述</h3><p>&emsp;&emsp;经典的图像特征描述子SIFT和SURF采用128维(SIFT)或者64维(SURF)特征向量，每维数据一般占用4个字节(Byte)，一个特征点的特征描述向量需要占用512或者256个字节。如果一幅图像中包含有大量的特征点，那么特征描述子将占用大量的存储，而且生成描述子的过程也会相当耗时。在SIFT特征的实际应用中，可以采用PCA、LDA等特征降维的方法来减少特征描述子的维度，例如PCA-SIFT；此外还可以采用一些局部敏感哈希(Locality-Sensitive Hashing, LSH)的方法将特征描述子编码为二进制串，然后使用汉明距离(Hamming Distance)进行特征点的匹配，汉明距离计算的是两个二进制比特串中同一位置不同值的个数，可通过异或操作快速实现，大大提升了特征匹配的效率。</p>
<p>&emsp;&emsp;BRIEF正是这样一种基于二进制编码生成特征描述子，以及利用汉明距离进行特征匹配的算法。由于BRIEF只是一种特征描述子，因此事先得检测并定位特征点，可采用Harris、FAST或者是SIFT算法检测特征点，在此基础上利用BRIEF算法建立特征描述符，在特征点邻域Patch内随机选取若干点对$(p,q)$，并比较这些点对的灰度值，若$I(p)&gt;I(q)$，则编码为1，否则编码为0。这样便可得到一个特定长度的二进制编码串，即BRIEF特征描述子。</p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ul>
<li>利用Harris或者FAST等方法检测特征点</li>
<li>确定特征点的邻域窗口Patch，并对该邻域内像素点进行$\sigma=2$、窗口尺寸为9的高斯平滑，以滤除噪声(也可直接对整幅图像做高斯平滑)</li>
<li>在邻域窗口内随机选取n对(n可取128、256等)像素点，并根据灰度值大小编码成二进制串，生成n位(bit)的特征描述子</li>
</ul>
<h3 id="采样方式"><a href="#采样方式" class="headerlink" title="采样方式"></a>采样方式</h3><p>&emsp;&emsp;论文原作者Calonder提供了5种在$S \times S$的邻域Patch内随机选取点对$(X,Y)$的方法，如下图所示，一条线段的两个端点表示一个随机点对$(x_i,y_i)$。</p>
<ul>
<li>$X、Y$为均匀分布$[-S/2,S/2]$</li>
<li>$X、Y$均为高斯分布$[0, S^2 / 25]$，采样准则服从各向同性的同一高斯分布</li>
<li>$X$服从高斯分布$[0, S^2 / 25]$，$Y$服从高斯分布$(x_i, S^2 / 100)$，即采样分为两步，首先在原点处为$x_i$进行高斯采样，然后在中心为$x_i$处为$y_i$进行高斯采样</li>
<li>$X、Y$在空间量化极坐标下的离散位置处进行随机采样</li>
<li>$X$固定为$(0,0)$，$Y$在空间量化极坐标下的离散位置处进行随机从采样。</li>
</ul>
<p><img src="https://ooo.0o0.ooo/2017/07/14/596875e06003f.jpg" alt="sample.jpg" title="随机点对选取方式"></p>
<h3 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h3><p>&emsp;&emsp;BRIEF算法通过检测随机响应，并采用二进制编码方式建立特征描述子，减少了特征的存储空间需求，并提升了特征生成的速度；Hamming距离的度量方式便于进行特征点的快速匹配，而且大量实验数据表明，不匹配特征点的Hamming距离为128左右(特征维数为256)，而匹配点的Hamming距离则远小于128。<br>&emsp;&emsp;BRIEF算法的缺点是不具备尺度不变性和旋转不变性，在图像的旋转角度超过$30^{\circ}$时，特征点匹配的准确率快速下降。</p>
<h3 id="Experiment-amp-Result"><a href="#Experiment-amp-Result" class="headerlink" title="Experiment &amp; Result"></a>Experiment &amp; Result</h3><p>OpenCV实现BRIEF特征检测与匹配<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">#include &lt;opencv2/core/core.hpp&gt; </div><div class="line">#include &lt;opencv2/highgui/highgui.hpp&gt; </div><div class="line">#include &lt;opencv2/imgproc/imgproc.hpp&gt; </div><div class="line">#include &lt;opencv2/features2d/features2d.hpp&gt;</div><div class="line"></div><div class="line">using namespace cv;</div><div class="line"></div><div class="line">int main(int argc, char** argv) </div><div class="line">&#123; </div><div class="line">    Mat img_1 = imread(&quot;box.png&quot;); </div><div class="line">    Mat img_2 = imread(&quot;box_in_scene.png&quot;);</div><div class="line"></div><div class="line">    // -- Step 1: Detect the keypoints using STAR Detector </div><div class="line">    std::vector&lt;KeyPoint&gt; keypoints_1,keypoints_2; </div><div class="line">    StarDetector detector; </div><div class="line">    detector.detect(img_1, keypoints_1); </div><div class="line">    detector.detect(img_2, keypoints_2);</div><div class="line"></div><div class="line">    // -- Stpe 2: Calculate descriptors (feature vectors) </div><div class="line">    BriefDescriptorExtractor brief; </div><div class="line">    Mat descriptors_1, descriptors_2; </div><div class="line">    brief.compute(img_1, keypoints_1, descriptors_1); </div><div class="line">    brief.compute(img_2, keypoints_2, descriptors_2);</div><div class="line"></div><div class="line">    //-- Step 3: Matching descriptor vectors with a brute force matcher </div><div class="line">    BFMatcher matcher(NORM_HAMMING); </div><div class="line">    std::vector&lt;DMatch&gt; mathces; </div><div class="line">    matcher.match(descriptors_1, descriptors_2, mathces); </div><div class="line">    // -- dwaw matches </div><div class="line">    Mat img_mathes; </div><div class="line">    drawMatches(img_1, keypoints_1, img_2, keypoints_2, mathces, img_mathes); </div><div class="line">    // -- show </div><div class="line">    imshow(&quot;Mathces&quot;, img_mathes);</div><div class="line"></div><div class="line">    waitKey(0); </div><div class="line">    return 0; </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><img src="https://ooo.0o0.ooo/2017/07/14/596879b800c06.jpg" alt="match result.jpg" title="BRIEF特征描述与匹配"></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://cvlabwww.epfl.ch/~lepetit/papers/calonder_eccv10.pdf" target="_blank" rel="external">Paper: BRIEF: Binary Robust Independent Elementary Features</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4081362.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4081362.html</a></li>
<li><a href="http://blog.csdn.net/songzitea/article/details/18272559" target="_blank" rel="external">http://blog.csdn.net/songzitea/article/details/18272559</a></li>
<li><a href="http://blog.csdn.net/luoshixian099/article/details/48338273" target="_blank" rel="external">http://blog.csdn.net/luoshixian099/article/details/48338273</a></li>
<li><a href="http://blog.csdn.net/hujingshuang/article/details/46910259" target="_blank" rel="external">http://blog.csdn.net/hujingshuang/article/details/46910259</a></li>
<li><a href="http://blog.csdn.net/icvpr/article/details/12342159" target="_blank" rel="external">http://blog.csdn.net/icvpr/article/details/12342159</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;BRIEF(Binary Robust Independent Elementary Features)是一种对已检测到的特征点进行表示和描述的特征描述方法，和传统的利用图像局部邻域的灰度直方图或梯度直方图提取特征的方式不同，BRIEF是一种二进制编码的特征描述子，既降低了存储空间的需求，提升了特征描述子生成的速度，也减少了特征匹配时所需的时间。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征之SUSAN角点检测</title>
    <link href="https://senitco.github.io/2017/07/01/image-feature-susan/"/>
    <id>https://senitco.github.io/2017/07/01/image-feature-susan/</id>
    <published>2017-06-30T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;SUSAN(Small univalue segment assimilating nucleus)是一种基于灰度图像以及窗口模板的特征点获取方法，适用于图像中边缘和角点的检测，对噪声鲁棒，而且具有简单、有效、计算速度快等特点。<br><a id="more"></a></p>
<h3 id="原理概述"><a href="#原理概述" class="headerlink" title="原理概述"></a>原理概述</h3><p>&emsp;&emsp;SUSAN算子采用一种近似圆形的滑动窗口模板，邻域窗口内的每个像素点的灰度值和中心像素作比较，若两者的灰度差值小于一定阈值，则认为该像素点与中心像素(核)具有相似的灰度值，满足这一条件的像素组成的区域称为吸收核同值区(Univalue Segment Assimilating Nucleus, USAN)。</p>
<p><img src="https://i.loli.net/2017/07/12/5965d644ac5c4.jpg" alt="different usan.jpg" title="模板在不同位置的USAN变化"></p>
<p>如上图所示，当圆形模板处于灰度均匀区域(背景或目标内)，USAN区域面积最大；当模板移向图像边缘时，USAN面积逐渐变小，模板中心处于边缘位置时，USAN面积为最大值的1/2；当模板中心位于角点处时，USAN面积最小，约为最大值的1/4。因此，USAN面积越小，其中心像素为角点的概率就越大。通过计算每个像素的USAN值，并与给定阈值作比较，如果该像素的USAN值小于给定阈值，则认为是一个角点。USAN的三维显示如下图所示：</p>
<p><img src="https://ooo.0o0.ooo/2017/07/12/5965d644d4320.jpg" alt="usan.jpg" title="USAN三维显示"></p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ul>
<li>定义一个半径为r(r=3)的圆形滑动模板，比较模板内像素与中心像素的灰度值差异，构成USAN区域。公式定义如下：<br>$$c(\overrightarrow{r}, \overrightarrow{r_0}) = \begin{cases} 1,&emsp;|I(\overrightarrow{r}) - I(\overrightarrow{r_0})| \leq t \\ 0,&emsp;|I(\overrightarrow{r}) - I(\overrightarrow{r_0})| &gt; t\end{cases}$$<br>式中，$r_0$表示模板核(中心像素)在图像中位置，$r$则是模板内其他像素的位置，$I(r)$表示图像灰度值。为了得到更稳定的结果，避免相似度函数在阈值边界处发生突变，亦可采用下式计算：<br>$$c(r,r_0)=e^{-(\dfrac{I(r)-I(r_0)}{t})^6}$$</li>
<li>计算USAN区域的面积<br>$$n(r_0) = \Sigma_r c(r, r_0)$$</li>
<li>计算角点响应值<br>$$R(r_0) = \begin{cases} g - n(r_0),&emsp;n(r_0) &lt; g \\ 0,&emsp;&emsp;&emsp;&emsp; n(r_0) \geq g \end{cases}$$<br>阈值可取$g=n_{max}/2$，USAN面积达到最小时，角点响应值达到最大。</li>
<li>在邻域内对角点响应值做非极大值抑制</li>
</ul>
<h3 id="参数分析"><a href="#参数分析" class="headerlink" title="参数分析"></a>参数分析</h3><p>&emsp;&emsp;SUSAN算子采用的是圆形模板，窗口半径为r=3，窗口内包含37个像素，如下图所示：</p>
<p><img src="https://i.loli.net/2017/07/12/5965d644c2903.png" alt="template.png" title="圆形模板"></p>
<p>在进行角点检测时，需要确定两个重要的参数——g值和t值。阈值g决定了USAN区域面积的最大值，以及所检测角点的尖锐程度，g值越小，检测到的角点越尖锐。阈值t表示所能检测角点的最小对比度，决定了角点提取的数量，t值越小，可提取的角点数量越多。对于不同对比度和噪声的图像，应取不同的阈值。</p>
<h3 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h3><ul>
<li>在对边缘和角点进行检测时，不涉及微分操作，因此对噪声的鲁棒性较好</li>
<li>SUSAN算子比较的是邻域像素的灰度相似性，具有光强不变性、旋转不变性；而且检测算子不依赖模板尺寸，在一定程度具备尺度不变性</li>
<li>参数较少，计算较快，抗干扰能力强</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://www-2.dc.uba.ar/materias/ipdi/smith95susan.pdf" target="_blank" rel="external">Paper: SUSAN — A New Approach to Low Level Image Processing</a></li>
<li><a href="http://www.cnblogs.com/luo-peng/p/5615359.html" target="_blank" rel="external">http://www.cnblogs.com/luo-peng/p/5615359.html</a></li>
<li><a href="http://blog.csdn.net/tostq/article/details/49305615" target="_blank" rel="external">http://blog.csdn.net/tostq/article/details/49305615</a></li>
<li><a href="http://xandl.cn/2017/03/23/extraction/" target="_blank" rel="external">http://xandl.cn/2017/03/23/extraction/</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;SUSAN(Small univalue segment assimilating nucleus)是一种基于灰度图像以及窗口模板的特征点获取方法，适用于图像中边缘和角点的检测，对噪声鲁棒，而且具有简单、有效、计算速度快等特点。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征之FAST角点检测</title>
    <link href="https://senitco.github.io/2017/06/30/image-feature-fast/"/>
    <id>https://senitco.github.io/2017/06/30/image-feature-fast/</id>
    <published>2017-06-29T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;前面已经介绍多种图像特征点(角点、斑点、极值点)的检测算法，包括Harris、LoG、HoG以及SIFT、SURF等，这些方法大多涉及图像局部邻域的梯度计算和统计，相比较而言，FAST(Features From Accelerated Segment Test)在进行角点检测时，计算速度更快，实时性更好。<br><a id="more"></a></p>
<h3 id="FAST角点定义"><a href="#FAST角点定义" class="headerlink" title="FAST角点定义"></a>FAST角点定义</h3><p>&emsp;&emsp;FAST角点定义为：若某像素点与周围邻域足够多的像素点处于不同区域，则该像素可能为角点。考虑灰度图像，即若某像素点的灰度值比周围邻域足够多的像素点的灰度值大或小，则该点可能为角点。</p>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ul>
<li>对于图像中一个像素点$p$，其灰度值为$I_p$</li>
<li>以该像素点为中心考虑一个半径为3的离散化的Bresenham圆，圆边界上有16个像素(如下图所示)</li>
<li>设定一个合适的阈值$t$，如果圆上有n个连续像素点的灰度值小于$I_p-t$或者大于$I_p+t$，那么这个点即可判断为角点(n的值可取12或9)</li>
</ul>
<p><img src="https://ooo.0o0.ooo/2017/07/10/596386112222c.jpg" alt="circle.jpg"></p>
<p>一种快速排除大部分非角点像素的方法就是检查周围1、5、9、13四个位置的像素，如果位置1和9与中心像素P点的灰度差小于给定阈值，则P点不可能是角点，直接排除；否则进一步判断位置5和13与中心像素的灰度差，如果四个像素中至少有3个像素与P点的灰度差超过阈值，则考察邻域圆上16个像素点与中心点的灰度差，如果有至少9个超过给定阈值则认为是角点。</p>
<h3 id="角点分类器"><a href="#角点分类器" class="headerlink" title="角点分类器"></a>角点分类器</h3><ul>
<li>选取需要检测的场景的多张图像进行FAST角点检测，选取合适的阈值n(n&lt;12)，提取多个特征点作为训练数据</li>
<li>对于特征点邻域圆上的16个像素$x \in {1,2,…,16 }$，按下式将其划分为3类<br>$$S_{p\rightarrow x} = \begin{cases} d, &emsp;I_{p\rightarrow x} \leq I_p-t \\ s, &emsp;I_p-t \leq I_{p\rightarrow x} \leq I_p+t \\ b, &emsp;I_p+t \leq  I_{p\rightarrow x} \end{cases}$$</li>
<li>对每个特征点定义一个bool变量$K_p$，如果$p$是一个角点，则$K_p$为真，否则为假</li>
<li>对提取的特征点集进行训练，使用ID3算法建立一颗决策树，通过第$x$个像素点进行决策树的划分，对集合$P$，得到熵值为<br>$$H(P)=(c+\hat{c})log_2 (c+\hat{c})-clog_2 c - \hat{c}log_2 \hat{c} $$<br>其中$c$为角点的数目，$\hat{c}$为非角点的数目。由此得到的信息增益为<br>$$\Delta H = H(P) - H(P_d) - H(P_s) - H(P_b)$$<br>选择信息增益最大位置进行划分，得到决策树</li>
<li>使用决策树对类似场景进行特征点的检测与分类</li>
</ul>
<h3 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h3><p>&emsp;&emsp;对于邻近位置存在多个特征点的情况，需要进一步做非极大值抑制(Non-Maximal Suppression)。给每个已经检测到的角点一个量化的值$V$，然后比较相邻角点的$V$值，保留局部邻域内$V$值最大的点。$V$值可定义为</p>
<ul>
<li>特征点与邻域16个像素点灰度绝对差值的和</li>
<li>$V = max(\Sigma_{x \in S_{bright}} |I_{p\rightarrow x} - I_p| - t, \Sigma_{x \in S_{dark}} |I_{p\rightarrow x} - I_p| - t)$<br>式中，$S_{bright}$是16个邻域像素点中灰度值大于$I_p+t$的像素点的集合，而$S_{dark}$表示的是那些灰度值小于$I_p−t$的像素点。</li>
</ul>
<h3 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h3><ul>
<li>FAST算法比其他角点检测算法要快</li>
<li>受图像噪声以及设定阈值影响较大</li>
<li>当设置$n&lt;12$时，不能用快速方法过滤非角点</li>
<li>FAST不产生多尺度特征，不具备旋转不变性，而且检测到的角点不是最优</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="https://www.edwardrosten.com/work/rosten_2006_machine.pdf" target="_blank" rel="external">Paper: Machine learning for high-speed corner detection</a></li>
<li><a href="https://pdfs.semanticscholar.org/a963/288ffecda4fd2bc475efe7cfb59ab094e7c1.pdf" target="_blank" rel="external">Paper: Faster and better: a machine learning approach to corner detection</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4078710.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4078710.html</a></li>
<li><a href="http://blog.csdn.net/hujingshuang/article/details/46898007" target="_blank" rel="external">http://blog.csdn.net/hujingshuang/article/details/46898007</a></li>
<li><a href="http://blog.csdn.net/lql0716/article/details/65662648" target="_blank" rel="external">http://blog.csdn.net/lql0716/article/details/65662648</a></li>
<li><a href="https://liu-wenwu.github.io/2016/10/08/fast-corners/" target="_blank" rel="external">https://liu-wenwu.github.io/2016/10/08/fast-corners/</a></li>
<li><a href="http://blog.csdn.net/skeeee/article/details/9405531" target="_blank" rel="external">http://blog.csdn.net/skeeee/article/details/9405531</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;前面已经介绍多种图像特征点(角点、斑点、极值点)的检测算法，包括Harris、LoG、HoG以及SIFT、SURF等，这些方法大多涉及图像局部邻域的梯度计算和统计，相比较而言，FAST(Features From Accelerated Segment Test)在进行角点检测时，计算速度更快，实时性更好。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征描述子之PCA-SIFT与GLOH</title>
    <link href="https://senitco.github.io/2017/06/28/image-feature-PCA_SIFT-GLOH/"/>
    <id>https://senitco.github.io/2017/06/28/image-feature-PCA_SIFT-GLOH/</id>
    <published>2017-06-27T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;SIFT和SURF是两种应用较为广泛的图像特征描述子，SURF可以看做是SIFT特征的加速版本。在SIFT的基础上，又陆续诞生了其他的变体：PCA-SIFT和GLOH(Gradient Location-Orientation Histogram)。<br><a id="more"></a></p>
<h3 id="PCA-SIFT"><a href="#PCA-SIFT" class="headerlink" title="PCA-SIFT"></a>PCA-SIFT</h3><p>&emsp;&emsp;SIFT特征提取主要分为4步：尺度空间构建，关键点定位，主方向分配，生成特征描述子。PCA-SIFT的前3步和标准SIFT相同，也就说PCA-SIFT和标准SIFT具有相同的尺度空间、亚像素定位以及主方向。但在生成特征描述子时，使用特征点周围$41 \times 41$的邻域计算特征向量，并通过主成分分析(PCA)，对特征向量进行降维，以滤除噪声，保留有效信息，并提高匹配效率。PCA-SIFT生成特征描述子的算法流程如下： </p>
<ol>
<li>以特征点为中心，选定$41 \times 41$的矩形邻域  </li>
<li>计算邻域内所有像素水平和垂直方向的梯度(偏导数)，得到一个$39 \times 39 \times 2 = 3042$维的特征向量(不计最外层像素)</li>
<li>假设有$N$个特征点，所有特征点描述子向量构成一个$N \times 3042$的矩阵，计算所有向量的协方差矩阵$C$</li>
<li>计算协方差矩阵$C$前$k$个最大特征值对应的特征向量，组成一个$3042 \times k$的投影矩阵$T$</li>
<li>对于新的特征描述子向量，乘以投影矩阵$T$，可以得到降维后的特征向量</li>
</ol>
<p>&emsp;&emsp;实际上，第3步和第4步一般提前计算好，也就是投影矩阵$T$是事先通过大量典型样本的训练得到。关于维数$k$的选择，可以是一个经验设定的固定值，也可以是基于特征值能量百分比动态选择。一般取20可得到较佳的效果。<br>&emsp;&emsp;PCA-SIFT描述子和标准SIFT相比，在保持各种不变性的同时，降低了特征向量的维数，使得特征点匹配速度大大提升。但其缺点是事先需要有一组典型图像的学习，而且，训练得到的投影矩阵只适用于同类的输入图像。</p>
<h3 id="GLOH"><a href="#GLOH" class="headerlink" title="GLOH"></a>GLOH</h3><p>&emsp;&emsp;梯度位置方向直方图(Gradient Location-Orientation Histogram, GLOH)也是SIFT特征描述子的一种扩展，其目的是为了增加特征描述子的鲁棒性和独特性。GLOH把标准SIFT中$4 \times 4$的邻域子块改成仿射状的对数-极坐标同心圆，同心圆半径分别设为6、11、15。在角度方向分成8等分，每等分为$\pi / 4$，这样一共产生了17个图像子块。如下图所示，在每个子块中，计算梯度方向直方图，梯度方向分为16个方向区间，因此可生成一个$17 \times 16 = 272$的特征向量。借助PCA-SIFT的思想，通过事先建立典型图像的协方差矩阵，并得到投影矩阵。然后，对每个特征点进行PCA降维处理，最终得到一个128维的特征向量，与标准SIFT保持一致。此外，也可以对GLOH进行简化，在生成梯度直方图时，只分8个方向，这样特征向量的维数为$17 \times 8 = 136$，就不需要进行降维处理，减少对样本图像的依赖性。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/04/595b01624b9bf.jpg" alt="keypoint descriptor.jpg" title="GLOH特征点描述子"></p>
<h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p>&emsp;&emsp;SIFT、PCA-SIFT、GLOH的实验结果如下所示</p>
<p><img src="https://ooo.0o0.ooo/2017/07/04/595b027a6da58.jpg" alt="keypoint match.jpg" title="SIFT、PCA-SIFT特征点匹配"></p>
<p><img src="https://ooo.0o0.ooo/2017/07/04/595b027a6c35e.jpg" alt="keypoint detection.jpg" title="GLOH特征点检测与匹配"></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://www.cs.cmu.edu/~rahuls/pub/cvpr2004-keypoint-rahuls.pdf" target="_blank" rel="external">Paper: PCA-SIFT: A More Distinctive Representation for Local Image Descriptors</a></li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/mikolajczyk_pami2004.pdf" target="_blank" rel="external">Paper: A Performance Evaluation of Local Descriptors</a></li>
<li><a href="http://blog.csdn.net/JIEJINQUANIL/article/details/50419119" target="_blank" rel="external">http://blog.csdn.net/JIEJINQUANIL/article/details/50419119</a></li>
<li><a href="http://blog.csdn.net/luoshixian099/article/details/49174869" target="_blank" rel="external">http://blog.csdn.net/luoshixian099/article/details/49174869</a></li>
<li><a href="http://blog.csdn.net/songzitea/article/details/18270457" target="_blank" rel="external">http://blog.csdn.net/songzitea/article/details/18270457</a></li>
<li><a href="http://blog.csdn.net/abcjennifer/article/details/7681718" target="_blank" rel="external">http://blog.csdn.net/abcjennifer/article/details/7681718</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;SIFT和SURF是两种应用较为广泛的图像特征描述子，SURF可以看做是SIFT特征的加速版本。在SIFT的基础上，又陆续诞生了其他的变体：PCA-SIFT和GLOH(Gradient Location-Orientation Histogram)。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征之SURF特征匹配</title>
    <link href="https://senitco.github.io/2017/06/27/image-feature-surf/"/>
    <id>https://senitco.github.io/2017/06/27/image-feature-surf/</id>
    <published>2017-06-26T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;加速鲁棒特征(Speed Up Robust Feature, SURF)和SIFT特征类似，同样是一个用于检测、描述、匹配图像局部特征点的特征描述子。SIFT是被广泛应用的特征点提取算法，但其实时性较差，如果不借助于硬件的加速和专用图形处理器(GPUs)的配合，很难达到实时的要求。对于一些实时应用场景，如基于特征点匹配的实时目标跟踪系统，每秒要处理数十帧的图像，需要在毫秒级完成特征点的搜索定位、特征向量的生成、特征向量的匹配以及目标锁定等工作，SIFT特征很难满足这种需求。SURF借鉴了SIFT中近似简化(DoG近似替代LoG)的思想，将Hessian矩阵的高斯二阶微分模板进行了简化，借助于积分图，使得模板对图像的滤波只需要进行几次简单的加减法运算，并且这种运算与滤波模板的尺寸无关。SURF相当于SIFT的加速改进版本，在特征点检测取得相似性能的条件下，提高了运算速度。整体来说，SUFR比SIFT在运算速度上要快数倍，综合性能更优。<br><a id="more"></a></p>
<h3 id="积分图像"><a href="#积分图像" class="headerlink" title="积分图像"></a>积分图像</h3><p>&emsp;&emsp;SURF算法中用到了积分图的概念，积分图(Integral Image)由Viola和Jones提出，在前面的博文<a href="https://senitco.github.io/2017/06/25/image-feature-haar/">Haar特征提取</a>中做了详细的介绍。借助积分图，图像与高斯二阶微分模板的滤波转化为对积分图像的加减运算，从而在特征点的检测时大大缩短了搜索时间。<br>&mesp;&emsp;积分图中任意一点$(i,j)$的值$ii(i,j)$，为原图像左上角到任意点$(i,j)$相应对角线区域灰度值的总和，即<br>$$ii(x,y) = \Sigma_{x’\leq x,y’\leq y} i(x’,y’)$$<br>式中，$i(x’,y’)$表示原图像中的灰度值，具体实现时$ii(x,y)$可由下式迭代计算得到<br>$$s(x,y)=s(x,y-1)+i(x,y)$$<br>$$ii(x,y)=ii(x-1,y)+s(x,y)$$<br>求取积分图时，对图像所有像素遍历一遍，得到积分图后，计算任何矩形区域内的像素灰度和只需进行三次加减运算，如下图所示。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/5959f26a69af1.png" alt="integral image.png"></p>
<h3 id="Hessian矩阵近似"><a href="#Hessian矩阵近似" class="headerlink" title="Hessian矩阵近似"></a>Hessian矩阵近似</h3><p>&emsp;&emsp;图像点的二阶微分Hessian矩阵的行列式(Determinant of Hessian, DoH)极大值，可用于图像的斑点检测(Blob Detection)。Hessian矩阵定义如下：<br>$$H(x,y,\sigma)=\left( \begin{matrix} L_{xx}&amp; L_{xy} \\ L_{xy} &amp; L_{yy}\end{matrix} \right)$$<br>式中，$L_{xx}、L_{yy}、L_{xy}$分别是高斯二阶微分算子$\dfrac{\partial ^2 g}{\partial x^2}、\dfrac{\partial ^2 g}{\partial y^2}、\dfrac{\partial ^2 g}{\partial x \partial y}$与原图像的卷积，Hessian矩阵的行列式值DoH为<br>$$detH = L_{xx} L_{yy} - L_{xy}^2$$<br>与LoG算子一样，DoH同样反映了图像局部的纹理或结构信息，与LoG相比，DoH对图像中细长结构的斑点有较好的抑制作用。LoG和DoH在利用二阶微分算子对图像进行斑点检测时，都需要利用高斯滤波平滑图像、抑制噪声，检测过程主要分为以下两步：</p>
<ul>
<li>使用不同的$\sigma$生成$(\dfrac{\partial ^2 g}{\partial x^2} + \dfrac{\partial ^2 g}{\partial y^2})$或$\dfrac{\partial ^2 g}{\partial x^2}、\dfrac{\partial ^2 g}{\partial y^2}、\dfrac{\partial ^2 g}{\partial x \partial y}$高斯卷积模板，并对图像进行卷积运算。</li>
<li>在图像的位置空间和尺度空间搜索LoG或DoH的峰值，并进行非极大值抑制，精确定位到图像极值点。</li>
</ul>
<p>三个高斯微分算子的响应图像如下图所示：</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/5959f8e4d9702.png" alt="gaussian.png"></p>
<p>由于二阶高斯微分模板被离散化和裁剪的原因，导致了图像在旋转奇数倍的$\pi/4$即模板对角线方向时，特征点检测的重复性(Repeatability)降低，即原来是特征点的地方在旋转后可能检测不到了；而旋转$\pi/2$时，特征点检测的重复性最高。不过这一不足并不影响Hessian矩阵检测特征点。</p>
<p>&emsp;&emsp;为了将模板与图像的卷积转化为盒子滤波器(Box Filter)运算，并能够使用积分图，需要对高斯二阶微分模板进行简化，使得简化后的模板只是由几个矩形区域组成，矩形区域内填充同一值，如下图所示，在简化模板中白色区域的值为1，黑色区域的值为-1或-2(由相对面积决定)，灰色区域的值为0。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/5959fd140cfe1.png" alt="simplify mask.png" title="高斯二阶微分模板及相应简化模板"></p>
<p>对于$\sigma=1.2$的高斯二阶微分滤波，设定模板的尺寸为$9 \times 9$的大小，并用它作为最小尺度空间值对图像进行滤波和斑点检测。使用$D_{xx}、D_{yy}、D_{xy}$表示简化模板与图像进行卷积的结果，Hessian矩阵的行列式可进一步简化为：<br>$$Det(H)=L_{xx}L_{yy}-L_{xy}^2=D_{xx} \dfrac{L_{xx}}{D_{xx}} D_{yy} \dfrac{L_{yy}}{D_{yy}} - D_{xy} \dfrac{L_{xy}}{D_{xy}} D_{xy} \dfrac{L_{xy}}{D_{xy}}$$<br>$$= D_{xx}D_{yy}(\dfrac{L_{xx}}{D_{xx}} \dfrac{L_{yy}}{D_{yy}}) - D_{xy} D_{xy} \dfrac{L_{xy}}{D_{xy}} \dfrac{L_{xy}}{D_{xy}} = (D_{xx}D_{yy} - D_{xy} D_{xy} Y)C$$<br>$$Y = (\dfrac{L_{xy}}{D_{xy}} \dfrac{L_{xy}}{D_{xy}}) (\dfrac{D_{xx}}{L_{xx}} \dfrac{D_{yy}}{L_{yy}})$$<br>$$C = \dfrac{L_{xx}}{D_{xx}} \dfrac{L_{yy}}{D_{yy}}$$<br>式中，$Y=(||L_{xy}(1.2)||_F ||D_{xx}(9)||_F)/(||L_{xx}(1.2)||_F ||D_{xy}(9))||_F)=0.912 \approx 0.9$，$||X||_F$为Frobenius范数，$1.2$是LoG的尺度$\sigma$，$9$是box filter的尺寸。理论上说，对于不同的$\sigma$值和对应的模板尺寸，$Y$值应该是不同的，但为了简化起见，可将其视为一个常数，同样$C$也为一常数，且不影响极值求取，因此，DoH可近似如下：<br>$$Det(H_{approx}) = D_{xx} D_{yy} - (0.9D_{xy})^2$$<br>在实际计算滤波响应值时，需要使用模板中盒子(矩形)区域的面积进行归一化处理，以保证一个统一的Frobenius范数能适应所有的滤波尺寸。</p>
<p>&emsp;&emsp;使用近似的Hessian矩阵行列式来表示一个图像中某一点处的斑点响应值，遍历图像中的所有像素，便形成了在某一尺度下斑点检测的响应图像。使用不同的模糊尺度和模板尺寸，便形成了多尺度斑点响应的金字塔图像，利用这一金字塔图像，可以进行斑点响应极值点的搜索定位，其过程与SIFT算法类似。</p>
<h3 id="尺度空间表示"><a href="#尺度空间表示" class="headerlink" title="尺度空间表示"></a>尺度空间表示</h3><p>&emsp;&emsp;要想检测不同尺度的极值点，必须建立图像的尺度空间金字塔。一般的方法是通过采用不同$\sigma$的高斯函数，对图像进行平滑滤波，然后重采样获得更高一层(Octave)的金字塔图像。Lowe在SIFT算法中就是通过相邻两层(Interval)高斯金字塔图像相减得到DoG图像，然后在DoG金字塔图像上进行特征点检测。与SIFT特征不同的是，SURF算法不需要通过降采样的方式得到不同尺寸大小的图像建立金字塔，而是借助于盒子滤波和积分图像，不断增大盒子滤波模板，通过积分图快速计算盒子滤波的响应图像。然后在响应图像上采用非极大值抑制，检测不同尺度的特征点。SIFT算法的LoG金字塔和SURF算法的近似DoH金字塔如下图所示：</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a2141bea36.jpg" alt="pyramid.jpg" title="LoG金字塔与盒状滤波金字塔"></p>
<p>&emsp;&emsp;如前所述，使用$9 \times 9$的模板对图像滤波，其结果作为最初始的尺度空间层，后续层将通过逐步增大滤波模板尺寸，以及放大后的模板与图像卷积得到。由于采用了box filter和积分图，滤波过程并不随着滤波模板尺寸的增大而增加运算量。<br>&emsp;&emsp;在建立盒状滤波金字塔时，与SIFT算法类似，需要将尺度空间划分为若干组(Octaves)。每组又由若干固定层组成，包括不同尺寸的滤波模板对同一输入图像进行滤波得到的一系列响应图。由于积分图像的离散特性，两个相邻层之间的最小尺度变化量，是由高斯二阶微分滤波模板在微分方向上对正负斑点响应长度(波瓣长度)$l_0$决定的，它是盒子滤波模板尺寸的1/3。对于$9 \times 9$的滤波模板，$l_0$为3。下一层的响应长度至少应该在$l_0$的基础上增加2个像素，以保证一边一个像素，即$l_0=5$，这样模板的尺寸为$15 \times 15$，如下图所示。依次类推，可以得到一个尺寸逐渐增大的模板序列，尺寸分别为$9 \times 9$、$15 \times 15$、$21 \times 21$、$27 \times 27$。显然，第一个模板和最后一份模板产生的Hessian响应图像只作为比较用，而不会产生最后的响应极值。这样，通过插值计算，可能的最小尺度值为$\sigma=1.2 \times \dfrac{(15+9)/2}{9}=1.6$，对应的模板尺寸为$12 \times 12$；可能的最大尺度值为$\sigma=1.2 \times \dfrac{(27+21)/2}{9}=3.2$，对应的模板尺寸为$24 \times 24$。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a295515d31.jpg" alt="filter template.jpg" title="滤波模板尺寸变化"></p>
<p>&emsp;&emsp;采用类似的方法处理其他组的模板序列，其方法是将滤波器尺寸增加量按Octave的组数$m$翻倍，即$6 \times 2^{m-1} $，序列依次为$(6, 12, 24, 48, …)$，这样，在盒状滤波金字塔中，每组滤波器的尺寸如下图所示，滤波器的组数可由原始图像的尺寸决定。对数水平轴代表尺度，组之间有相互重叠，其目的是为了覆盖所有可能的尺度。在通常尺度分析情况下，随着尺度的增大，被检测的特征点数迅速衰减。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a2b5a57f41.png" alt="scale-octaves.png" title="不同组滤波器尺寸"><br>滤波器的尺寸$L$、滤波响应长度$l$、组索引$o$、层索引$s$、尺度$\sigma$之间的相互关系如下：<br>$$L=3 \times (2^{o+1}(s+1)+1)$$<br>$$l=\dfrac{L}{3}=2^{o+1}(s+1)+1$$<br>$$\sigma=1.2 \times \dfrac{L}{9} = 1.2 \times \dfrac{l}{3}$$</p>
<h3 id="关键点定位"><a href="#关键点定位" class="headerlink" title="关键点定位"></a>关键点定位</h3><p>&emsp;&emsp;和LoG、DoG类似，建立尺度空间后，需要搜索定位关键点。将经过box filter处理过的响应图像中每个像素点<br>与其3维邻域中的26个像素点进行比较，若是最极大值点，则认为是该区域的局部特征点。然后，采用3维线性插值法得到亚像素级的特征点，同时去掉一些小于给定阈值的点，使得极值检测出来的特征点更稳健。和DoG不同的是，不需要剔除边缘导致的极值点，因为Hessian矩阵的行列式已经考虑了边缘的问题。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a2ee656e20.png" alt="maximum detectoin.png" title="极值检测"></p>
<h3 id="特征点方向分配"><a href="#特征点方向分配" class="headerlink" title="特征点方向分配"></a>特征点方向分配</h3><p>&emsp;&emsp;为了保证特征描述子具有旋转不变性，与SIFT一样，需要对每个特征点分配一个主方向。为此，在以特征点为中心，以$6s$(s为特征点的尺度)为半径的区域内，计算图像的Haar小波响应，实际上就是对图像进行梯度运算，只不过需要利用积分图，提高梯度计算效率。求Haar小波响应的图像区域和Haar小波模板如下图所示，用于计算梯度的Haar小波的尺度是$4s$，扫描步长为$s$。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a34455c031.jpg" alt="haar response.jpg" title="Haar小波响应计算"></p>
<p>使用$\sigma=2s$的高斯函数对Haar小波的响应值进行加权。为了求取主方向，设计一个以特征点为中心，张角为$\pi/3$的扇形窗口，如下图所示，以一定旋转角度$\theta$旋转窗口，并对窗口内的Haar小波响应值$dx、dy$进行累加，得到一个矢量$(m_w, \theta_w)$<br>$$m_w=\Sigma_w dx + \Sigma_w dy$$<br>$$\theta_w=arctan(\Sigma_w dy / \Sigma_w dx) $$<br>主方向为最大Haar响应累加值所对应的方向，即$\theta=\theta_w|max{m_w}$</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a3745a0dc7.png" alt="main direction.png" title="统计旋转窗口内的Haar小波响应和值"></p>
<p>仿照SIFT求主方向时策略，当存在大于主峰值80%以上的峰值时，则将对应方向认为是该特征点的辅方向。一个特征点可能会被指定多个方向，可以增强匹配的鲁棒性。</p>
<h3 id="特征描述子生成"><a href="#特征描述子生成" class="headerlink" title="特征描述子生成"></a>特征描述子生成</h3><p>&emsp;&emsp;生成特征点描述子时，同样需要计算图像的Haar小波响应。与确定主方向不同的是，这里不再使用圆形区域，而是在一个矩形区域计算Haar小波响应。以特征点为中心，沿主方向将$20s \times 20s$的邻域划分为$4 \times 4$个子块，每个子块利用尺寸为$2s$的Haar模板计算响应值，然后对响应值统计$\Sigma dx、\Sigma |dx|、\Sigma dy、\Sigma |dy|$形成特征向量，如下图所示：</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a3c15098a0.png" alt="feature descriptor.png" title="特征描述子生成"></p>
<p>将$20s$的窗口划分为$4 \times 4$个子窗口，每个子窗口大小为$5s \times 5s$，使用尺寸为$2s$的Haar小波计算子窗口的响应值；然后，以特征点为中心，用$\sigma=10s/3=3.3s$的高斯核函数对$dx、dy$进行加权计算；最后，分别对每个子块的加权响应值进行统计，得到每个子块的向量：<br>$$V_i=[\Sigma dx,\Sigma |dx|,\Sigma dy,\Sigma |dy|]$$<br>由于共有$4 \times 4$个子块，特征描述子的特征维数为$4 \times 4 \times 4 = 64$。SURF描述子不仅具有尺度和旋转不变性，还具有光照不变性，这由小波响应本身决定，而对比度不变性则是通过将特征向量归一化来实现。下图为3种简单模式图像及其对应的特征描述子，可以看出，引入Haar小波响应绝对值的统计和是必要的，否则只计算$\Sigma dx、\Sigma dy$的话，第一幅图和第二幅图的特征表现形式是一样的，因此，采用4个统计量描述子区域使特征更具有区分度。</p>
<p><img src="https://ooo.0o0.ooo/2017/07/03/595a3f7e23db4.png" alt="pattern feature.png" title="不同模式图像的描述子"></p>
<p>&emsp;&emsp;为了充分利用积分图像计算Haar小波的响应值，在具体实现中，并不是直接通过旋转Haar小波模板求其响应值，而是在积分图像上先使用水平和垂直的Haar模板求得响应值$dx、dy$，对$dx、dy$进行高斯加权处理，并根据主方向的角度，对$dx、dy$进行旋转变换，从而得到旋转后的$dx’、dy’$。</p>
<p>&emsp;&emsp;SURF在求取描述子特征向量时，是对一个子块的梯度信息进行求和，而SIFT是依靠单个像素计算梯度的方向。在有噪声的干扰下，SURF描述子具有更好的鲁棒性。一般而言，特征向量的长度越长，所承载的信息量就越大，特征描述子的独特性就越好，但匹配时所付出的时间代价也越大。对于SURF描述子，可以将其扩展到128维。具体方法就是在求Haar小波响应值的统计和时，区分$dx \geq 0$和$dx &lt; 0$的情况，以及$dy \geq 0$和$dy &lt; 0$的情况。为了实现快速匹配，SURF在特征向量中增加了一个新的元素，即特征点的拉普拉斯响应正负号。在特征点检测时，将Hessian矩阵的迹(Trace)的正负号记录下来，作为特征向量中的一个变量。在特征匹配时可以节省运算时间，因为只用具有相同正负号的特征点才可能匹配，对于不同正负号的特征点不再进行相似性计算。<br>下图是用SURF进行特征点匹配的实验结果</p>
<p><img src="https://ooo.0o0.ooo/2017/07/04/595b05c548ec2.png" alt="result.png" title="SURF特征点匹配"></p>
<h3 id="SURF与SIFT的对比"><a href="#SURF与SIFT的对比" class="headerlink" title="SURF与SIFT的对比"></a>SURF与SIFT的对比</h3><ul>
<li>尺度空间：SIFT使用DoG金字塔与图像进行卷积操作，而且对图像有做降采样处理；SURF是用近似DoH金字塔(即不同尺度的box filters)与图像做卷积，借助积分图，实际操作只涉及到数次简单的加减运算，而且不改变图像大小。</li>
<li>特征点检测：SIFT是先进行非极大值抑制，去除对比度低的点，再通过Hessian矩阵剔除边缘点。而SURF是计算Hessian矩阵的行列式值(DoH)，再进行非极大值抑制。</li>
<li>特征点主方向：SIFT在方形邻域窗口内统计梯度方向直方图，并对梯度幅值加权，取最大峰对应的方向；SURF是在圆形区域内，计算各个扇形范围内$x、y$方向的Haar小波响应值，确定响应累加和值最大的扇形方向。</li>
<li>特征描述子：SIFT将关键点附近的邻域划分为$4 \times 4$的区域，统计每个子区域的梯度方向直方图，连接成一个$4 \times 4 \times 8 = 128$维的特征向量；SURF将$20s \times 20s$的邻域划分为$4 \times 4$个子块，计算每个子块的Haar小波响应，并统计4个特征量，得到$4 \times 4 \times 4 = 64$维的特征向量。   </li>
</ul>
<p>&emsp;&emsp;总体来说，SURF和SIFT算法在特征点的检测取得了相似的性能，SURF借助积分图，将模板卷积操作近似转换为加减运算，在计算速度方面要优于SIFT特征。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://www.vision.ee.ethz.ch/~surf/eccv06.pdf" target="_blank" rel="external">Paper: SURF: Speeded Up Robust Features</a></li>
<li><a href="http://blog.csdn.net/songzitea/article/details/16986423" target="_blank" rel="external">http://blog.csdn.net/songzitea/article/details/16986423</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4045979.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4045979.html</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4048213.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4048213.html</a></li>
<li><a href="http://www.cnblogs.com/YiXiaoZhou/p/5903690.html" target="_blank" rel="external">http://www.cnblogs.com/YiXiaoZhou/p/5903690.html</a></li>
<li><a href="http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html" target="_blank" rel="external">http://www.cnblogs.com/tornadomeet/archive/2012/08/17/2644903.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;加速鲁棒特征(Speed Up Robust Feature, SURF)和SIFT特征类似，同样是一个用于检测、描述、匹配图像局部特征点的特征描述子。SIFT是被广泛应用的特征点提取算法，但其实时性较差，如果不借助于硬件的加速和专用图形处理器(GPUs)的配合，很难达到实时的要求。对于一些实时应用场景，如基于特征点匹配的实时目标跟踪系统，每秒要处理数十帧的图像，需要在毫秒级完成特征点的搜索定位、特征向量的生成、特征向量的匹配以及目标锁定等工作，SIFT特征很难满足这种需求。SURF借鉴了SIFT中近似简化(DoG近似替代LoG)的思想，将Hessian矩阵的高斯二阶微分模板进行了简化，借助于积分图，使得模板对图像的滤波只需要进行几次简单的加减法运算，并且这种运算与滤波模板的尺寸无关。SURF相当于SIFT的加速改进版本，在特征点检测取得相似性能的条件下，提高了运算速度。整体来说，SUFR比SIFT在运算速度上要快数倍，综合性能更优。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征之SIFT特征匹配</title>
    <link href="https://senitco.github.io/2017/06/24/image-feature-sift/"/>
    <id>https://senitco.github.io/2017/06/24/image-feature-sift/</id>
    <published>2017-06-23T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;尺度不变特征变换(Scale-invariant feature transform, SIFT)是计算机视觉中一种检测、描述和匹配图像局部特征点的方法，通过在不同的尺度空间中检测极值点或特征点(Conrner Point, Interest Point)，提取出其位置、尺度和旋转不变量，并生成特征描述子，最后用于图像的特征点匹配。SIFT特征凭借其良好的性能广泛应用于运动跟踪(Motion tracking)、图像拼接(Automatic mosaicing)、3D重建(3D reconstruction)、移动机器人导航(Mobile robot navigation)以及目标识别(Object Recognition)等领域。<br><a id="more"></a></p>
<h3 id="尺度空间极值检测"><a href="#尺度空间极值检测" class="headerlink" title="尺度空间极值检测"></a>尺度空间极值检测</h3><p>&emsp;&emsp;Harris特征进行角点检测时，不具备尺度不变性。为了能够在不同的尺度检测到尽可能完整的特征点或关键点，需要借助尺度空间理论来描述图像的多尺度特征。相关研究证明高斯卷积核是实现尺度变换的唯一线性核。因此可用图像的高斯金字塔表示尺度空间，而且尺度规范化的LoG算子具有尺度不变性，在具体实现中，可用高斯差分(DoG)算子近似LoG算子，在构建的尺度空间中检测稳定的特征点。</p>
<h4 id="构建尺度空间"><a href="#构建尺度空间" class="headerlink" title="构建尺度空间"></a>构建尺度空间</h4><p>&emsp;&emsp;尺度空间理论的基本思想是：在图像处理模型中引入一个被视为尺度的参数，通过连续变化尺度参数获取多尺度下的空间表示序列，对这些空间序列提取某些特征描述子，抽象成特征向量，实现图像在不同尺度或不同分辨率的特征提取。尺度空间中各尺度图像的模糊程度逐渐变大，模拟人在由近到远时目标在人眼视网膜上的成像过程。而且尺度空间需满足一定的不变性，包括图像灰度不变性、对比度不变性、平移不变性、尺度不变性以及旋转不变性等。在某些情况下甚至要求尺度空间算子具备仿射不变性。<br>&emsp;&emsp;一幅图像的尺度空间可定义为对原图像进行可变尺度的高斯卷积：<br>$$L(x,y,\sigma)=G(x,y,\sigma) \ast I(x,y)$$<br>$$G(x,y,\sigma)=\dfrac{1}{2 \pi \sigma^2} e^{-\dfrac{x^2+y^2}{2\sigma^2}}$$<br>式中，$G(x,y,\sigma)$是尺度可变的高斯函数，$(x,y)$是图像的空间坐标，$\sigma$是尺度坐标(尺度变化因子)，$\sigma$大小决定图像的平滑程度，值越大图像模糊得越严重。大尺度对应图像的概貌特征，小尺度对应图像的细节特征。一般根据$3\sigma$原则，高斯核矩阵的大小设为$(6\sigma+1) \times (6\sigma+1)$。<br>&emsp;&emsp;在使用高斯金字塔构建尺度空间时，主要分成两部分，对图像做降采样，以及对图像做不同尺度的高斯模糊。对图像做降采样得到不同尺度的图像，也就是不同的组(Octave)，后面的Octave(高一层的金字塔)为上一个Octave(低一层的金字塔)降采样得到，图像宽高分别为上一个Octave的1/2。每组(Octave)又分为若干层(Interval)，通过对图像做不同尺度的高斯模糊得到。为了有效地在尺度空间检测稳定的关键点，提出了高斯差分尺度空间(DoG Scale-Space)，利用不同尺度的高斯差分核与图像卷积生成。<br>$$D(x,y,\sigma)=(G(x,y,k\sigma)-G(x,y,\sigma)) \ast I(x,y) = L(x,y,k\sigma)-L(x,y,\sigma)$$<br>图像的高斯金字塔和高斯差分金字塔如下图所示，高斯差分图像由高斯金字塔中同一组(Octave)内相邻层(Interval)的图像作差得到。</p>
<p><img src="https://ooo.0o0.ooo/2017/06/30/5956098d36da8.jpg" alt="Gaussian Pyramid.jpg" title="图像高斯金字塔"></p>
<h4 id="尺度空间的参数确定"><a href="#尺度空间的参数确定" class="headerlink" title="尺度空间的参数确定"></a>尺度空间的参数确定</h4><p>&emsp;&emsp;在由图像金字塔表示的尺度空间中，图像的组数(Octave)由原始图像的大小和塔顶图像的大小决定。<br>$$Octave = log_2(min(width_0, height_0)) - log_2(min(width, height)) $$<br>式中，$width_0、height_0$分别为原始图像的宽高，$width、height$为塔顶图像的宽高。对于一幅大小为$512 \times 512$的图像，当塔顶图像大小为$4 \times 4$时，图像的组数为$Octave = 7$。<br>尺度参数$\sigma$的取值与金字塔的组数和层数相关，设第一组第一层的尺度参数取值为$\sigma(1,1)=\sigma_0$，则第$m$组第$n$层的$\sigma$取值为<br>$$\sigma(m,n)=\sigma_0 \cdot 2^{m-1} \cdot k^{n-1}, &emsp;k=2^{\dfrac{1}{S}}$$<br>式中，$S$为金字塔中每组的有效层数。为了得到更多的特征点，将图像扩大为原来的两倍，原始图像的尺度参数为$\sigma=0.5$，扩大两倍后的尺度为$\sigma=1.6$。在检测极值点前对原始图像的高斯平滑会导致图像高频信息的丢失，所以在建立尺度空间前先将图像扩大为原来的两倍，以保留原始图像信息，增加特征点数量。</p>
<h4 id="DoG算子检测极值点"><a href="#DoG算子检测极值点" class="headerlink" title="DoG算子检测极值点"></a>DoG算子检测极值点</h4><p>&emsp;&emsp;为了寻找DoG尺度空间的极值点，每一个采样点要和其所有邻域像素相比较，如下图所示，中间检测点与其同尺度的$8$个邻域像素点以及上下相邻两层对应的$9 \times 2$个像素点一共$26$个点作比较，以确保在图像空间和尺度空间都能检测到极值点。一个像素点如果在DoG尺度空间本层及上下两层的26邻域中取得最大或最小值时，就可以认为该点是图像在该尺度下的一个特征点。</p>
<p><img src="https://ooo.0o0.ooo/2017/06/30/59562da5c15d9.png" alt="extremum.png" title="DoG尺度空间极值检测"></p>
<p>&emsp;&emsp;在极值比较的过程中，每一组差分图像的首末两层是无法比较的，为了在每组中检测$S$个尺度的极值点，则DoG金字塔每组须有$S+2$层图像，高斯金字塔每组须有$S+3$层图像。另外，在降采样时，高斯金字塔中一组(Octive)的底层图像是由前一组图像的倒数第3张图像(S+1层)隔点采样得到。这样也保证了尺度变化的连续性，如下图所示：</p>
<p><img src="https://ooo.0o0.ooo/2017/06/30/59562da5c399e.png" alt="parameter.png" title="尺度参数的变化"></p>
<p>假设每组的层数$S=3$，则$k=2^{1/S}=s^{1/3}$，在高斯金字塔中，第一个Octave中第$S+1$层图像尺度为$k^3\sigma=2\sigma$，经降采样后得到第二个Octave的第$1$层图像，尺度仍为$2\sigma$。在DoG尺度空间中，第一组(1st-Octave)图像中间三项的尺度分别为$(k\sigma, k^2\sigma, k^3\sigma)$，下一组中间三项为$(2k\sigma, 2k^2\sigma, 2k^3\sigma)$，其“首项”$2k\sigma=2^{4/3}\sigma$，与上一组“末项”$k^3\sigma=2^{3/3}\sigma$尺度变化连续，变化尺度为$k=2^{1/S}=2^{1/3}$。</p>
<h3 id="关键点定位"><a href="#关键点定位" class="headerlink" title="关键点定位"></a>关键点定位</h3><p>&emsp;&emsp;在DoG尺度空间检测到的极值点是离散的，通过拟合三元二次函数可以精确定位关键点的位置和尺度，达到亚像素精度。同时去除低对比度的检测点和不稳定的边缘点，以增强匹配稳定性，提高抗噪声能力。</p>
<h4 id="关键点精确定位"><a href="#关键点精确定位" class="headerlink" title="关键点精确定位"></a>关键点精确定位</h4><p>&emsp;&emsp;DoG函数$D(X)=D(x,y,\sigma)$在尺度空间的的Taylor展开式为<br>$$D(X)=D+\dfrac{\partial D^T}{\partial X} X + \dfrac{1}{2} X^T \dfrac{\partial^2 D}{\partial X^2}X$$<br>令$D(X)$导数为0，得到极值点的偏移量<br>$$\hat {X} = -(\dfrac{\partial^2 D}{\partial X^2})^{-1} \dfrac{\partial D}{\partial X}$$<br>若$\hat {X}=(x,y,\sigma)^T$在任意一个维度大于$0.5$，说明极值点精确位置距离另一个点更近，应该改变当前关键点的位置，定位到新点后执行相同操作，若迭代5次仍不收敛，则认为该检测点不为关键点。精确关键点处函数值为<br>$$D(\hat {X})=D+\dfrac{1}{2} \dfrac{\partial D^T}{\partial X} \hat {X}$$<br>$|D(\hat {X})|$过小易受噪声点的干扰而变得不稳定，若其小于某个阈值(例如$0.03$或者$0.04/S$)，则将该极值点删除。</p>
<h4 id="消除边缘效应"><a href="#消除边缘效应" class="headerlink" title="消除边缘效应"></a>消除边缘效应</h4><p>&emsp;&emsp;高斯差分函数DoG有较强的边缘响应，需要剔除不稳定的边缘响应点。边缘点的特征表现在某个方向有较大的主曲率，而在与其垂直方向主曲率较小。主曲率可通过一个$2 \times 2$的$Hessian$矩阵求出。<br>$$H=\left( \begin{matrix} D_{xx}&amp; D_{xy} \\ D_{xy} &amp; D_{yy}\end{matrix} \right)$$<br>$D$的主曲率和$H$的特征值成正比，令$\alpha$为较大特征值，$\beta$为较小特征值，且$\alpha / \beta = r$，则<br>$$Tr(H)=D_{xx}+D_{yy}=\alpha + \beta, &emsp;Det(H)=D_{xx}D_{yy}-D_{xy}^2=\alpha \beta$$<br>$$\dfrac{Tr(H)^2}{Det(H)}=\dfrac{(\alpha + \beta)^2}{\alpha \beta} = \dfrac{(r+1)^2}{r}$$<br>$(r+1)^2/r$在两个特征值相等时最小，随着$r$的增大而增大，$r$值越大，说明两个特征值的比值越大，正好对应边缘的情况。因此，设定一个阈值$r_t$，若满足<br>$$\dfrac{Tr(H)^2}{Det(H)} &lt; \dfrac{(r_t+1)^2}{r_t}$$<br>则认为该关键点不是边缘，否则予以剔除。</p>
<h3 id="关键点方向分配"><a href="#关键点方向分配" class="headerlink" title="关键点方向分配"></a>关键点方向分配</h3><p>&emsp;&emsp;为了使特征描述子具有旋转不变性，需要利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数。对于在DoG金字塔中检测出的关键点，在其邻近高斯金字塔图像的$3\sigma$邻域窗口内计算其梯度幅值和方向，公式如下：<br>$$m(x,y)=\sqrt{[L(x+1,y)-L(x-1,y)]^2 + [L(x,y+1)-L(x,y-1)]^2}$$<br>$$\theta(x,y)=tan^{-1}{[L(x,y+1)-L(x,y-1)] / [L(x+1,y)-L(x-1,y)]}$$<br>式中，$L$为关键点所在尺度空间的灰度值，$m(x,y)$为梯度幅值，$\theta(x,y)$为梯度方向。对模值$m(x,y)$按$\sigma=1.5\sigma_{oct}$、邻域窗口为$3\sigma=3 \times 1.5\sigma_{oct}$的高斯分布加权。在完成关键点的梯度计算后，使用直方图统计邻域内像素的梯度和方向，梯度直方图将梯度方向$(0, 360^{\circ })$分为36柱(bins)，如下图所示，直方图的峰值所在的方向代表了该关键点的主方向。</p>
<p><img src="https://ooo.0o0.ooo/2017/06/30/59564f338a59c.png" alt="main direction.png" title="梯度方向直方图"></p>
<p>梯度方向直方图的峰值代表了该特征点处邻域梯度的主方向，为了增强鲁棒性，保留峰值大于主方向峰值80%的方向作为该关键点的辅方向，因此，在相同位置和尺度，将会有多个关键点被创建但方向不同，可以提高特征点匹配的稳定性。</p>
<h3 id="关键点特征描述"><a href="#关键点特征描述" class="headerlink" title="关键点特征描述"></a>关键点特征描述</h3><p>&emsp;&emsp;在经过上述流程后，检测到的每个关键点有三个信息：位置、尺度以及方向，接下来要做的就是抽象出一组特征向量描述每个关键点。这个特征描述子不但包括关键点，还包括其邻域像素的贡献，而且需具备较高的独特性和稳定性，以提高特征点匹配的准确率。SIFT特征描述子是关键点邻域梯度经过高斯加权后统计结果的一种表示。通过对关键点周围图像区域分块，计算块内的梯度直方图，得到表示局部特征点信息的特征向量。例如在尺度空间$4 \times 4$的窗口内统计8个方向的梯度直方图，生成一个$4 \times 4 \times 8 = 128$维的表示向量。</p>
<h4 id="确定特征描述子采样区域"><a href="#确定特征描述子采样区域" class="headerlink" title="确定特征描述子采样区域"></a>确定特征描述子采样区域</h4><p>&emsp;&emsp;特征描述子与特征点所在的尺度空间有关，因此对梯度的求取应该在特征点对应的高斯图像上进行。将关键点附近的邻域划分为$d \times d(d=4)$个子区域，每个子区域的大小与关键点方向分配时相同，即边长为$3\sigma_{oct}$，考虑到实际计算时需要进行三线性插值，邻域窗口边长设为$3\sigma_{oct}(d+1)$，又考虑到旋转因素(坐标轴旋转至关键点主方向)，最后所需的图像区域半径为<br>$$radius=\dfrac{3\sigma_{oct} \times \sqrt{2} \times (d+1)}{2}$$<br><img src="https://ooo.0o0.ooo/2017/06/30/595656c99be22.jpg" alt="radius.jpg"></p>
<h4 id="旋转坐标轴至关键点主方向"><a href="#旋转坐标轴至关键点主方向" class="headerlink" title="旋转坐标轴至关键点主方向"></a>旋转坐标轴至关键点主方向</h4><p>&emsp;&emsp;将坐标轴旋转至关键点主方向，以确保旋转不变性。旋转后采样点的新坐标为<br>$$\left[ \begin{matrix} x’\\ y’\end{matrix} \right] = \left( \begin{matrix} cos\theta &amp; -sin\theta \\ sin\theta &amp; cos\theta \end{matrix} \right) \left[ \begin{matrix} x\\ y \end{matrix} \right], &emsp;x,y \in [-radius, radius]$$</p>
<p><img src="https://ooo.0o0.ooo/2017/06/30/595656c9f07bc.png" alt="rotation.png"></p>
<h4 id="三线性插值计算权值"><a href="#三线性插值计算权值" class="headerlink" title="三线性插值计算权值"></a>三线性插值计算权值</h4><p>&emsp;&emsp;在图像半径区域内对每个像素点求其梯度幅值和方向，并对每个梯度幅值乘以高斯权重参数<br>$$w=m(a+x,b+y) \times e^{-\dfrac{(x’)^2+(y’)^2}{2\sigma_w^2}}$$<br>式中，$x’、y’$分别表示像素点与关键点的行、列距离，$m(x,y)$表示像素梯度幅值，高斯尺度因子为$\sigma_w=3\sigma \times 0.5d$。<br>将旋转后的采样点坐标分配到对应的子区域，计算影响子区域的采样点的梯度和方向，分配到8个方向上。旋转后的采样点$(x’,y’)$落在子区域的下标为<br>$$\left[ \begin{matrix} u\\ v\end{matrix} \right] = \dfrac{1}{3\sigma_{oct}} \left[ \begin{matrix} x’\\ y’\end{matrix} \right] + \dfrac{d}{2}, &emsp;u,v \in [0,d]$$<br>将采样点在子区域的下标进行三线性插值，根据三维坐标计算与周围子区域的距离，按距离远近计算权重，最终累加在相应子区域的相关方向上的权值为<br>$$weight = w \cdot dr^i \cdot (1-dr)^{1-i} \cdot dc^j \cdot (1-dc)^{1-j} \cdot do^k \cdot (1-do)^{1-k}$$<br>式中$i、j、k$取0或者1.</p>
<p><img src="https://ooo.0o0.ooo/2017/06/30/59565c422740a.jpg" alt="grad hist.jpg" title="梯度方向统计直方图"></p>
<h4 id="向量归一化生成描述子"><a href="#向量归一化生成描述子" class="headerlink" title="向量归一化生成描述子"></a>向量归一化生成描述子</h4><p>&emsp;&emsp;得到128维特征向量后，为了去除光照变化的影响，需要对向量进行归一化处理。非线性光照变化仍可能导致梯度幅值的较大变化，但对梯度方向影响较小。因此对于超过阈值0.2的梯度幅值设为0.2，然后再进行一次归一化。最后将特征向量按照对应高斯金字塔的尺度大小排序。至此，SIFT特征描述子形成。</p>
<h3 id="SIFT特征匹配"><a href="#SIFT特征匹配" class="headerlink" title="SIFT特征匹配"></a>SIFT特征匹配</h3><p>&emsp;&emsp;对两幅图像中检测到的特征点，可采用特征向量的欧式距离作为特征点相似性的度量，取图像1中某个关键点，并在图像2中找到与其距离最近的两个关键点，若最近距离与次近距离的比值小于某个阈值，则认为距离最近的这一对关键点为匹配点。降低比例阈值，SIFT匹配点数量会减少，但相对而言会更加稳定。阈值ratio的取值范围一般为0.4~0.6。</p>
<h3 id="SIFT特征的特点"><a href="#SIFT特征的特点" class="headerlink" title="SIFT特征的特点"></a>SIFT特征的特点</h3><p>&emsp;&emsp;SIFT是一种检测、描述、匹配图像局部特征点的算法，通过在尺度空间中检测极值点，提取位置、尺度、旋转不变量，并抽象成特征向量加以描述，最后用于图像特征点的匹配。SIFT特征对灰度、对比度变换、旋转、尺度缩放等保持不变性，对视角变化、仿射变化、噪声也具有一定的鲁棒性。但其实时性不高，对边缘光滑的目标无法准确提取特征点。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf" target="_blank" rel="external">Paper: Distinctive Image Features from Scale-Invariant Keypoints</a></li>
<li><a href="http://cgit.nutn.edu.tw:8080/cgit/PaperDL/iccv99.pdf" target="_blank" rel="external">Paper: Object Recognition from Local Scale-Invariant Features</a></li>
<li><a href="https://people.cs.umass.edu/~elm/Teaching/ppt/SIFT.pdf" target="_blank" rel="external">PPT: Object Recognition from Local Scale-Invariant Features (SIFT)</a></li>
<li><a href="https://github.com/robwhess/opensift" target="_blank" rel="external">Code: RobHess OpenSIFT</a></li>
<li><a href="http://blog.csdn.net/abcjennifer/article/details/7639681" target="_blank" rel="external">http://blog.csdn.net/abcjennifer/article/details/7639681</a></li>
<li><a href="http://blog.csdn.net/zddblog/article/details/7521424" target="_blank" rel="external">http://blog.csdn.net/zddblog/article/details/7521424</a></li>
<li><a href="http://www.sun11.me/blog/2016/sift-implementation-in-matlab/" target="_blank" rel="external">http://www.sun11.me/blog/2016/sift-implementation-in-matlab/</a></li>
<li><a href="http://masikkk.com/article/RobHess-SIFT-Source-Code-Analysis-Overview/" target="_blank" rel="external">http://masikkk.com/article/RobHess-SIFT-Source-Code-Analysis-Overview/</a></li>
<li><a href="http://masikkk.com/article/RANSAC-SIFT-Image-Match/" target="_blank" rel="external">http://masikkk.com/article/RANSAC-SIFT-Image-Match/</a></li>
<li><a href="http://www.cnblogs.com/letben/p/5510976.html" target="_blank" rel="external">http://www.cnblogs.com/letben/p/5510976.html</a></li>
<li><a href="http://blog.csdn.net/fzthao/article/details/62424271" target="_blank" rel="external">http://blog.csdn.net/fzthao/article/details/62424271</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;尺度不变特征变换(Scale-invariant feature transform, SIFT)是计算机视觉中一种检测、描述和匹配图像局部特征点的方法，通过在不同的尺度空间中检测极值点或特征点(Conrner Point, Interest Point)，提取出其位置、尺度和旋转不变量，并生成特征描述子，最后用于图像的特征点匹配。SIFT特征凭借其良好的性能广泛应用于运动跟踪(Motion tracking)、图像拼接(Automatic mosaicing)、3D重建(3D reconstruction)、移动机器人导航(Mobile robot navigation)以及目标识别(Object Recognition)等领域。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征之LoG算子与DoG算子</title>
    <link href="https://senitco.github.io/2017/06/20/image-feature-LoG-DoG/"/>
    <id>https://senitco.github.io/2017/06/20/image-feature-LoG-DoG/</id>
    <published>2017-06-19T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;LoG(Laplacian of Gaussian)算子和DoG(Difference of Gaussian)算子是图像处理中实现极值点检测(Blob Detection)的两种方法。通过利用高斯函数卷积操作进行尺度变换，可以在不同的尺度空间检测到关键点(Key Point)或兴趣点(Interest Point)，实现尺度不变性(Scale invariance)的特征点检测。<br><a id="more"></a></p>
<h3 id="Laplacian-of-Gaussian-LoG"><a href="#Laplacian-of-Gaussian-LoG" class="headerlink" title="Laplacian of Gaussian(LoG)"></a>Laplacian of Gaussian(LoG)</h3><p>&emsp;&emsp;Laplace算子通过对图像求取二阶导数的零交叉点(zero-cross)来进行边缘检测，其计算公式如下：<br>$$\nabla ^2 f(x,y)=\dfrac{\partial^2 f}{\partial x^2} + \dfrac{\partial^2 f}{\partial y^2}$$<br>由于微分运算对噪声比较敏感，可以先对图像进行高斯平滑滤波，再使用Laplace算子进行边缘检测，以降低噪声的影响。由此便形成了用于极值点检测的LoG算子。常用的二维高斯函数如下：<br>$$G_\sigma(x,y)=\dfrac{1}{\sqrt {2\pi \sigma ^{2}}} exp(-\dfrac{x^2+y^2}{2\sigma ^2})$$<br>原图像与高斯核函数卷积后再做laplace运算<br>$$\Delta [G_\sigma(x,y) \ast f(x,y)]=[\Delta G_\sigma(x,y)] \ast f(x,y)$$<br>$$LoG = \Delta G_\sigma(x,y)=\dfrac{\partial^2 G_\sigma(x,y)}{\partial x^2} + \dfrac{\partial^2 G_\sigma(x,y)}{\partial y^2}=\dfrac{x^2+y^2-2\sigma^2}{\sigma^4}e^{-(x^2+y^2)/2\sigma^2}$$<br>所以先对高斯核函数求取二阶导数，再与原图像进行卷积操作。由于高斯函数是圆对称的，因此LoG算子可以有效地实现极值点或局部极值区域的检测。</p>
<h3 id="Difference-of-Gaussian-DoG"><a href="#Difference-of-Gaussian-DoG" class="headerlink" title="Difference of Gaussian(DoG)"></a>Difference of Gaussian(DoG)</h3><p>&emsp;&emsp;DoG算子是高斯函数的差分，具体到图像中，就是将图像在不同参数下的高斯滤波结果相减，得到差分图。DoG算子的表达式如下：<br>$$DoG = G_{\sigma_1} - G_{\sigma_2}=\dfrac{1}{\sqrt{2\pi}} [\dfrac{1}{\sigma_1} e^{-(x^2+y^2)/2\sigma_1^2} - \dfrac{1}{\sigma_2} e^{-(x^2+y^2)/2\sigma_2^2}]$$<br>如果将高斯核函数的形式表示为<br>$$G_\sigma(x,y)=\dfrac{1}{2\pi \sigma ^{2}} exp(-\dfrac{x^2+y^2}{2\sigma ^2})$$<br>则存在以下等式<br>$$\dfrac{\partial G}{\partial \sigma} = \sigma \nabla ^2 G$$<br>$$\dfrac{\partial G}{\partial \sigma} \approx \dfrac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma-\sigma}$$<br>因此有<br>$$G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^2 \nabla ^2 G$$<br>其中$k-1$是个常数，不影响极值点的检测，LoG算子和DoG算子的函数波形对比如下图所示，由于高斯差分的计算更加简单，因此可用DoG算子近似替代LoG算子</p>
<p><img src="https://ooo.0o0.ooo/2017/06/29/5954c1640caeb.jpg" alt="LoG-DoG.jpg" title="LoG与DoG的对比"></p>
<h3 id="边缘检测-Edge-Detection-和极值点检测-Blob-Detection"><a href="#边缘检测-Edge-Detection-和极值点检测-Blob-Detection" class="headerlink" title="边缘检测(Edge Detection)和极值点检测(Blob Detection)"></a>边缘检测(Edge Detection)和极值点检测(Blob Detection)</h3><p>&emsp;&emsp;LoG算子和DoG算子既可以用于检测图像边缘，也可用于检测局部极值点或极值区域，图像边缘在LoG算子下的响应情况如下图所示，二阶微分算子在边缘处为一过零点，而且过零点两边的最大值(正)和最小值(负)的差值较大。</p>
<p><img src="https://ooo.0o0.ooo/2017/06/29/5954d7a6ca776.jpg" alt="edge.jpg" title="LoG算子检测边缘响应"></p>
<p>接下来观察下图，由边缘过渡到极值点，LoG算子的响应变化</p>
<p><img src="https://ooo.0o0.ooo/2017/06/29/5954d7a6cb78d.jpg" alt="edge to blob.jpg" title="边缘到极值点的LoG响应变化"></p>
<p>LoG算子在极值点(Blob)处的响应如下图所示：</p>
<p><img src="https://ooo.0o0.ooo/2017/06/29/5954d7a6c963e.jpg" alt="blob.jpg"></p>
<p>通过定义不同尺寸的高斯核函数，可以实现在不同尺度检测Blob，如下图所示</p>
<p><img src="https://ooo.0o0.ooo/2017/06/29/5954d7a6c08d4.jpg" alt="scale blob.jpg"></p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ul>
<li>对原图像进行LoG或者DoG卷积操作</li>
<li>检测卷积后图像中的过零点(边缘)或者极值点(Blob)</li>
<li>如果是检测边缘，则对过零点进行阈值化(过零点两边的最大值和最小值之间的差值要大于某个阈值)；如果是检测极值点，则极值点的LoG或DoG响应值应该大于某个阈值。</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://fourier.eng.hmc.edu/e161/lectures/gradient/node8.html" target="_blank" rel="external">http://fourier.eng.hmc.edu/e161/lectures/gradient/node8.html</a></li>
<li><a href="http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html" target="_blank" rel="external">http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html</a></li>
<li><a href="http://blog.csdn.net/songzitea/article/details/12851079" target="_blank" rel="external">http://blog.csdn.net/songzitea/article/details/12851079</a></li>
<li><a href="http://blog.csdn.net/kezunhai/article/details/11579785" target="_blank" rel="external">http://blog.csdn.net/kezunhai/article/details/11579785</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;LoG(Laplacian of Gaussian)算子和DoG(Difference of Gaussian)算子是图像处理中实现极值点检测(Blob Detection)的两种方法。通过利用高斯函数卷积操作进行尺度变换，可以在不同的尺度空间检测到关键点(Key Point)或兴趣点(Interest Point)，实现尺度不变性(Scale invariance)的特征点检测。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>图像特征之Harris角点检测</title>
    <link href="https://senitco.github.io/2017/06/18/image-feature-harris/"/>
    <id>https://senitco.github.io/2017/06/18/image-feature-harris/</id>
    <published>2017-06-17T16:00:00.000Z</published>
    <updated>2017-07-30T07:30:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;角点检测(Corner Detection)也称为特征点检测，是图像处理和计算机视觉中用来获取图像局部特征点的一类方法，广泛应用于运动检测、图像匹配、视频跟踪、三维建模以及目标识别等领域中。<br><a id="more"></a></p>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h3 id="局部特征"><a href="#局部特征" class="headerlink" title="局部特征"></a>局部特征</h3><p>&emsp;&emsp;不同于HOG、LBP、Haar等基于区域(Region)的图像局部特征，Harris是基于角点的特征描述子，属于feature detector，主要用于图像特征点的匹配(match)，在SIFT算法中就有用到此类角点特征；而HOG、LBP、Haar等则是通过提取图像的局部纹理特征(feature extraction)，用于目标的检测和识别等领域。无论是HOG、Haar特征还是Harris角点都属于图像的局部特征，满足局部特征的一些特性。主要有以下几点：</p>
<ul>
<li>可重复性(Repeatability)：同一个特征可以出现在不同的图像中，这些图像可以在不同的几何或光学环境下成像。也就是说，同一物体在不同的环境下成像(不同时间、不同角度、不同相机等)，能够检测到同样的特征。</li>
<li>独特性(Saliency)：特征在某一特定目标上表现为独特性，能够与场景中其他物体相区分，能够达到后续匹配或识别的目的。</li>
<li>局部性(Locality)；特征能够刻画图像的局部特性，而且对环境影响因子(光照、噪声等)鲁棒。</li>
<li>紧致性和有效性(Compactness and efficiency)；特征能够有效地表达图像信息，而且在实际应用中运算要尽可能地快。  </li>
</ul>
<p>相比于考虑局部邻域范围的局部特征，全局特征则是从整个图像中抽取特征，较多地运用在图像检索领域，例如图像的颜色直方图。<br>除了以上几点通用的特性外，对于一些图像匹配、检测识别等任务，可能还需进一步考虑图像的局部不变特征。例如尺度不变性(Scale invariance)和旋转不变性(Rotation invariance)，当图像中的物体或目标发生旋转或者尺度发生变换，依然可以有效地检测或识别。此外，也会考虑局部特征对光照、阴影的不变性。</p>
<h3 id="Harris角点检测"><a href="#Harris角点检测" class="headerlink" title="Harris角点检测"></a>Harris角点检测</h3><p>&emsp;&emsp;特征点在图像中一般有具体的坐标，并具有某些数学特征，如局部最大或最小灰度、以及某些梯度特征等。角点可以简单的认为是两条边的交点，比较严格的定义则是在邻域内具有两个主方向的特征点，也就是说在两个方向上灰度变化剧烈。如下图所示，在各个方向上移动小窗口，如果在所有方向上移动，窗口内灰度都发生变化，则认为是角点；如果任何方向都不变化，则是均匀区域；如果灰度只在一个方向上变化，则可能是图像边缘。</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953a445031a0.jpg" alt="corner.jpg"></p>
<p>&emsp;&emsp;对于给定图像$I(x,y)$和固定尺寸的邻域窗口，计算窗口平移前后各个像素差值的平方和，也就是自相关函数<br>$$E(u,v)=\Sigma_x\Sigma_yw(x,y)[I(x+u,y+v)-I(x,y)]^2$$<br>其中，窗口加权函数$w(x,y)$可取均值函数或者高斯函数，如下图所示：</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953a7fa5a172.jpg" alt="weights.jpg" title="窗口加权函数"></p>
<p>根据泰勒展开，可得到窗口平移后图像的一阶近似<br>$$I(x+u,y+v)\approx I(x,y)+I_x(x,y)u+I_y(x,y)v$$<br>因此$E(u, v)$可化为<br>$$E(u,v) \approx \Sigma_{x,y}w(x,y)[I_x(x,y)u+I_y(x,y)v]^2=\left[u,v\right] M(x,y) \left[ \begin{matrix} u\\ v\end{matrix} \right]$$<br>$$M(x,y)=\Sigma_w \left[ \begin{matrix} I_x^2&amp; I_xI_y \\ I_xI_y &amp; I_y^2\end{matrix} \right] = \left[ \begin{matrix} A&amp; C\\ C&amp; B\end{matrix} \right]$$<br>$E(u,v)$可表示为一个二次项函数<br>$$E(u,v)=Au^2+2Cuv+Bv^2$$<br>其中$A=\Sigma_w I_x^2, B = \Sigma_w I_y^2, C=\Sigma_w I_x I_y$<br>二次项函数本质上是一个椭圆函数，椭圆的曲率和尺寸可由$M(x,y)$的特征值$\lambda_1,\lambda_2$决定，椭圆方向由$M(x,y)$的特征向量决定，椭圆方程和其图形分别如下所示：<br>$$\left[u,v\right] M(x,y) \left[ \begin{matrix} u\\ v\end{matrix} \right] = 1$$</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953af7c8c289.png" alt="tuoyuan.png"></p>
<p>考虑角点的边界和坐标轴对齐的情况，如下图所示，在平移窗口内，只有上侧和左侧边缘，上边缘$I_y$很大而$I_x$很小，左边缘$I_x$很大而$I_y$很小，所以矩阵$M$可化简为<br>$$M=\left[ \begin{matrix} \lambda_1&amp; 0\\ 0&amp; \lambda_2\end{matrix} \right]$$</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953b0d8c854c.jpg" alt="axis-aligned.jpg"></p>
<p>当角点边界和坐标轴没有对齐时，可对角点进行旋转变换，将其变换到与坐标轴对齐，这种旋转操作可用矩阵的相似对角化来表示，即<br>$$M=X\Sigma X^T = X \left[ \begin{matrix} \lambda_1&amp; 0\\ 0&amp; \lambda_2\end{matrix} \right] X^T$$<br>$$Mx_i=\lambda_i x_i$$</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953b0d8e3d2d.jpg" alt="not-aligned.jpg"></p>
<p>&emsp;&emsp;对于矩阵$M$，可以将其和协方差矩阵类比，协方差表示多维随机变量之间的相关性，协方差矩阵对角线的元素表示的是各个维度的方差，而非对角线上的元素表示的是各个维度之间的相关性，在PCA(主成分分析)中，将协方差矩阵对角化，使不同维度的相关性尽可能的小，并取特征值较大的维度，来达到降维的目的。类似的，可以将矩阵$M$看成是一个二维随机分布的协方差矩阵，通过将其对角化，求取矩阵的两个特征值，并根据这两个特征值来判断角点。</p>
<p>如下图所示，可根据矩阵$M$的特征值来判断是否为角点，当两个特征值都较大时为角点(corne)，一个特征值较大而另一个较小时则为图像边缘(edge)，两个特征值都较小时为均匀区域(flat)。<br><img src="https://ooo.0o0.ooo/2017/06/28/5953b3774c2f2.png" alt="judge corners.png"></p>
<p>在判断角点时，无需具体计算矩阵$M$的特征值，而使用下式近似计算角点响应值。<br>$$R = detM-\alpha (traceM)^2$$<br>$$detM=\lambda_1 \lambda_2=AB-C^2$$<br>$$traceM=\lambda_1 + \lambda_2 = A+B$$<br>式中，$detM$为矩阵$M$的行列式，$traceM$为矩阵$M$的迹，$\alpha$为一常数，通常取值为0.04~0.06。</p>
<h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><p>Harris角点检测的算法步骤归纳如下：</p>
<ul>
<li>计算图像$I(x,y)$在$X$方向和$Y$方向的梯度<br>$$I_x=\dfrac {\partial I} {\partial x}=I(x,y)\otimes \left( \begin{matrix} -1&amp; 0&amp; 1\end{matrix} \right)$$<br>$$I_y=\dfrac {\partial I} {\partial y}=I(x,y)\otimes \left( \begin{matrix} -1&amp; 0&amp; 1\end{matrix} \right)^T$$</li>
<li>计算图像两个方向梯度的乘积$I_x^2、I_y^2、I_x I_y$</li>
<li>使用窗口高斯函数分别对$I_x^2、I_y^2、I_x I_y$进行高斯加权，生成矩阵$M$。</li>
<li>计算每个像素的Harris响应值$R$，并设定一阈值$T$，对小于阈值$T$的$R$置零。</li>
<li>在一个固定窗口大小的邻域内($5 \times 5$)进行非极大值抑制，局部极大值点即为图像中的角点。</li>
</ul>
<h3 id="Harris角点性质"><a href="#Harris角点性质" class="headerlink" title="Harris角点性质"></a>Harris角点性质</h3><p>1.参数$\alpha$对角点检测的影响：增大$\alpha$的值，将减小角点响应值$R$，减少被检测角点的数量；减小$\alpha$的值，将增大角点响应值$R$，增加被检测角点的数量。<br>2.Harris角点检测对亮度和对比度的变化不敏感。</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953bb60bb814.jpg" alt="Brightness-Contrast.jpg"></p>
<p>3.Harris角点检测具有旋转不变性，但不具备尺度不变性。如下图所示，在小尺度下的角点被放大后可能会被认为是图像边缘。</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953bb317b0b6.png" alt="scale.png"></p>
<p>Harris角点检测的结果示意图：</p>
<p><img src="https://ooo.0o0.ooo/2017/06/28/5953bc1427011.jpg" alt="result.jpg"></p>
<h3 id="多尺度Harris角点检测"><a href="#多尺度Harris角点检测" class="headerlink" title="多尺度Harris角点检测"></a>多尺度Harris角点检测</h3><p>&emsp;&emsp;Harris角点具有灰度不变性和旋转不变性，但不具备尺度不变性，而尺度不变性对于图像的局部特征来说至关重要。将Harris角点检测算子和高斯尺度空间表示相结合，可有效解决这个问题。与Harris角点检测中的二阶矩表示类似，定义一个尺度自适应的二阶矩<br>$$M=\mu (x,y,\sigma_I, \sigma_D)=\sigma_D^2g(\sigma_I) \otimes \left[ \begin{matrix} L_x^2(x,y,\sigma_D)&amp; L_xL_y(x,y,\sigma_D)\\ L_xL_y(x,y,\sigma_D)&amp; L_y^2(x,y,\sigma_D)\end{matrix} \right]$$<br>式中，$g(\sigma_I)$表示尺度为$\sigma_I$的高斯卷积核，$L_x(x,y,\sigma_D)$和$L_y(x,y,\sigma_D)$表示对图像使用高斯函数$g(\sigma_D)$进行平滑后取微分的结果。$\sigma_I$通常称为积分尺度，是决定Harris角点当前尺度的变量，$\sigma_D$为微分尺度，是决定角点附近微分值变化的变量，通常$\sigma_I$应大于$\sigma_D$。<br>算法流程：</p>
<ul>
<li>确定尺度空间的一组取值$\sigma_I=(\sigma_0, \sigma_1, \sigma_2,…, \sigma_n)=(\sigma, k\sigma, k^2\sigma,…, k^n\sigma), \sigma_D=s\sigma_I$</li>
<li>对于给定的尺度空间值$\sigma_D$，进行角点响应值的计算和判断，并做非极大值抑制处理</li>
<li>在位置空间搜索候选角点后，还需在尺度空间上进行搜索，计算候选点的拉普拉斯响应值，并于给定阈值作比较<br>$$F(x,y,\sigma_n)=\sigma_n^2|L_{xx}(x,y,\sigma_n)+L_{yy}(x,y,\sigma_n)| \geq threshold$$</li>
<li>将响应值$F$与邻近的两个尺度空间的拉普拉斯响应值进行比较，使其满足<br>$$F(x,y,\sigma_n) &gt; F(x,y,\sigma_l),&emsp;l=n-1, n+1$$</li>
</ul>
<p>这样既可确定在位置空间和尺度空间均满足条件的Harris角点。</p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a href="http://www.bmva.org/bmvc/1988/avc-88-023.pdf" target="_blank" rel="external">Paper: A COMBINED CORNER AND EDGE DETECTOR</a></li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/mikolajczyk_ijcv2004.pdf" target="_blank" rel="external">Paper: Scale &amp; Affine Invariant Interest Point Detectors</a></li>
<li><a href="https://github.com/ronnyyoung/ImageFeatures" target="_blank" rel="external">Code: Harris Detector</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/4009425.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/4009425.html</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/3886013.html" target="_blank" rel="external">http://www.cnblogs.com/ronny/p/3886013.html</a></li>
<li><a href="https://xmfbit.github.io/2017/01/25/cs131-finding-features/" target="_blank" rel="external">https://xmfbit.github.io/2017/01/25/cs131-finding-features/</a></li>
<li><a href="http://www.voidcn.com/blog/app_12062011/article/p-6071346.html" target="_blank" rel="external">http://www.voidcn.com/blog/app_12062011/article/p-6071346.html</a></li>
<li><a href="http://blog.csdn.net/jwh_bupt/article/details/7628665" target="_blank" rel="external">http://blog.csdn.net/jwh_bupt/article/details/7628665</a></li>
<li><a href="http://www.cnblogs.com/ztfei/archive/2012/05/07/2487123.html" target="_blank" rel="external">http://www.cnblogs.com/ztfei/archive/2012/05/07/2487123.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;角点检测(Corner Detection)也称为特征点检测，是图像处理和计算机视觉中用来获取图像局部特征点的一类方法，广泛应用于运动检测、图像匹配、视频跟踪、三维建模以及目标识别等领域中。&lt;br&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
</feed>
