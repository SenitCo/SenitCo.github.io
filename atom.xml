<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Senit_Co</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://senitco.github.io/"/>
  <updated>2017-05-11T08:38:18.066Z</updated>
  <id>https://senitco.github.io/</id>
  
  <author>
    <name>Senit_Co</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习算法之数据降维</title>
    <link href="https://senitco.github.io/2017/05/10/data-dimensionality-reduction/"/>
    <id>https://senitco.github.io/2017/05/10/data-dimensionality-reduction/</id>
    <published>2017-05-10T13:17:14.930Z</published>
    <updated>2017-05-11T08:38:18.066Z</updated>
    
    <content type="html"><![CDATA[<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p>&emsp;&emsp;数据降维是通过某种数学变换将原始高维属性空间，转变为一个低维子空间，对数据进行降维，可以有效地去除样本中冗余的属性，减少数据容量，缓解维数灾难，加快学习速度。数据降维的常用方法有主成分分析(PCA)、多维缩放(MDS)、线性判别分析(LDA)、等度量映射(Isomap)、局部线性嵌入(LLE)、t分布随机邻域嵌入(t-SNE)、Laplacian Eigenmaps等。</p>
<h3 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析(PCA)"></a>主成分分析(PCA)</h3><p>&emsp;&emsp;主成分分析是一种线性降维方法，将高维空间映射到低维空间，并使得所有样本在低维空间的投影点尽可能分开，也就是低维子空间对样本具有最大可分性，为了实现这种最大可分性，应该使投影后样本的方差最大化。<br><img src="https://ooo.0o0.ooo/2017/05/11/59141de4ab509.png" alt="PCA模型图" width="256" height="240" align="center"><br>&emsp;&emsp;对于一个维数为$D$的高维空间，样本点$x_i=(x_1,x_2,…,x_D)$通过与矩阵$W$相乘映射到低维空间（维数为$d$，$d&lt;D$）中的某个点$z_i=W^T x_i=(z_1,z_2,…,z_d)$，矩阵$W$的大小是$D\ast d$。令数据样本集的大小为$N$，PCA的目标是要让低维子空间中 $z_{i}$ 尽可能地分开，因此投影后样本的方差要尽可能的大。假定数据样本进行了中心化，数据每一维的均值为$0$，即 $\Sigma_{i}x_{i}=0$，乘上矩阵$W^T$得到的降维后的数据每一维均值也为$0$，考虑高维空间中原始样本数据集的协方差矩阵$C=\dfrac{1}{N}\ast XX^T$，协方差矩阵中对角线上的值为某一维的方差，非对角线上的值为两维之间的协方差。降维后低维子空间中相应的协方差矩阵为$B=\dfrac{1}{N}\ast ZZ^T$，如果希望降维后的点具有最大的可分性，那么协方差矩阵$B$对角线上的值也就是每一维的方差应该尽可能的大，同时为了让不同的属性能够更多地表示原始信息，而不包含冗余的信息，可以使不同属性之间正交，这种情况下矩阵$B$非对角线上的值即不同维之间的协方差为$0$。因此降维后的每一维既有足够的区分性，又能代表不同的信息。对于矩阵$B$，可进一步推导出$B=\dfrac{1}{N}\ast ZZ^T=\dfrac{1}{N}\ast W^T X(W^T X)^T=W^T (\dfrac{1}{N}\ast XX^T)W=W^TCW$，这个式子表明，线性变换矩阵$W$实现数据降维的过程是将高维空间中的协方差矩阵$C$对角化，因此可通过求协方差矩阵$C$的特征值以及对应的特征向量来确定投影变换矩阵$W$。PCA的算法流程如下所述： </p>
<ul>
<li><strong>输入</strong>：样本集${x_1,x_2,…,x_N }$，低维空间的维数$d$；  </li>
<li><strong>过程</strong>：  </li>
<li>对所有数据样本进行中心化$x_i=x_i-\dfrac{1}{N} \Sigma_{i}x_{i}$；  </li>
<li>计算样本的协方差矩阵$C=\dfrac{1}{N} XX^T$；  </li>
<li>对协方差矩阵$C$做特征值分解：  </li>
<li>取最大的$d$个特征值对应的特征向量$w_1,w_2,…,w_d$；  </li>
<li><strong>输出</strong>：投影矩阵$W=(w_1,w_2,…,w_d)$，$w_i$为$D$维列向量。  </li>
</ul>
<h3 id="多维缩放-MDS"><a href="#多维缩放-MDS" class="headerlink" title="多维缩放(MDS)"></a>多维缩放(MDS)</h3><p>&emsp;&emsp;多维缩放要求原始高维空间中数据样本之间的距离在低维空间中保持不变，即在降维的过程中保留原始数据的差异性。<br>&emsp;&emsp;假定$N$个样本在$D$维原始空间的距离矩阵为$A\in R^{N×N}$，其第$i$行第$j$列的元素$a_{ij}$为样本$x_i$到$x_j$的距离。多维缩放的目标是获得数据样本在$d$维空间的表示$Z\in R^{d×N}$,$d≤D$，且任意两个样本在低维空间的欧氏距离等于原始空间中的距离，即$\left|z_i-z_j \right|=a_{ij}$。令$B=Z^T Z\in R^{N×N}$,其中$B$为降维后样本的內积矩阵，$b_{ij}=z_i^T z_j$，根据样本在原始空间和低维空间的距离相等有<br>$$a_{ij}^2=‖z_i-z_j ‖^2=‖z_i ‖^2+‖z_j ‖^2-2z_i^T z_j=b_{ii}+b_{jj}-2b_{ij}$$<br>&emsp;&emsp;令降维后的数据样本$Z$中心化，数据每一维的均值为$0$，即$\Sigma_i z_i = 0$。显然，內积矩阵$B$的行与列之和均为$0$，即$\Sigma_{i=1}^N b_{ij} = \Sigma_{j=1}^N b_{ij} = 0$，则有以下式子成立：<br>$$\Sigma_{i=1}^N a_{ij}^2 =tr(B)+Nb_{jj}$$<br>$$\Sigma_{j=1}^N a_{ij}^2 =tr(B)+Nb_{ii}$$<br>$$\Sigma_{i=1}^N \Sigma_{j=1}a_{ij}^2 =2Ntr(B)$$<br>其中 $tr(B)=\Sigma_{i=1}^N ‖z_i‖^2$ 为矩阵$B$的迹，令<br>$$a_{i\cdot}^2=\dfrac{1}{N} \Sigma_{j=1}^N a_{ij}^2$$<br>$$a_{\cdot j}^2=\dfrac{1}{N} \Sigma_{i=1}^N a_{ij}^2$$<br>$$a_{\cdot \cdot}^2=\dfrac{1}{N^2} \Sigma_{i=1}^N \Sigma_{j=1}^N a_{ij}^2$$<br>由以上各式可得<br>$$b_{ij}=-\dfrac{1}{2} (a_{ij}^2-a_{i \cdot}^2-a_{\cdot j}^2+a_{\cdot \cdot}^2)$$<br>由此即可通过降维前后保持不变的距离矩阵$A$求取內积矩阵$B$。对矩阵$B$做特征值分解，$B=VΛV^T$，其中$Λ=diag(λ_1,λ_2,…,λ_d)$为特征值构成的对角矩阵，$λ_1≥λ_2≥\ldots≥λ_d$，$V$为特征向量矩阵，则$Z$可表达为<br>$$Z=Λ^{1/2} V^T∈R^{d×N}$$<br>为了实现有效降维，往往仅需降维后的距离与原始空间中距离尽可能接近，而不必严格相等，因此可取$d(d&lt;&lt;D)$个最大特征值构成对角矩阵。MDS的算法流程如下所述：  </p>
<ul>
<li><strong>输入</strong>：距离矩阵$A∈R^{N×N}$，其元素$a_{ij}$为样本$x_i$到$x_j$的距离，原始空间维数为$D$，低维空间维数为$d$；</li>
<li><strong>过程</strong>：</li>
<li>计算內积矩阵$B$；</li>
<li>对矩阵$B$做特征值分解；</li>
<li>取$d$个最大的特征值构成的对角矩阵$Λ$和对应的特征向量矩阵$V$；</li>
<li><strong>输出</strong>：矩阵$Z=Λ^{1/2} V^T∈R^{d×N}$，每列为一个样本的低维坐标。  </li>
</ul>
<h3 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析(LDA)"></a>线性判别分析(LDA)</h3><p>&emsp;&emsp;线性判别分析是一种典型的线性学习方法，在二分类问题上最早由Fisher提出。LDA的基本思想是给定数据样本集，设法将样本投影到一个低维空间，使得同类样本的投影点尽可能接近，不同类样本的投影点尽可能远离。LDA和PCA的降维思路相似，都是通过矩阵乘法进行线性降维，但两者原理有所不同，PCA是希望所有样本在每一维上尽可能分开。<br>&emsp;&emsp;给定数据样本集$D={(x_i,y_i )},i=1,2,…,N$，样本集的大小为$N$，样本的类别数为$K$，令$X_i$ 、$μ_i$ 、$Σ_i$分别表示第$i(i=1,2,…,K)$类样本的集合、均值向量和协方差矩阵，样本在低维空间的表示为$z_i=W^T x_i$，欲使同类样本的投影点尽可能近，可以让同类样本投影点的协方差尽可能小；欲使不同类样本的投影点尽可能远离，则可以让不同类中心之间的距离尽可能大，定义类内散度矩阵<br>$$S_w=\Sigma_{k=1}^K S_k = \Sigma_{k=1}^K \Sigma_{X∈X_k} (X-μ_k)(X-μ_k)^T$$<br>即$K$个类的原始数据协方差矩阵相加。定义类间散度矩阵<br>$$S_b=\Sigma_{k=1}^K N_k (μ_k-u)(μ_k-u)^T$$<br>其中$μ=1/N \Sigma_{k=1}^K N_k μ_k$($N_k$为第$k$类的样本个数)，同时考虑两者，则LDA算法的最大化目标<br>$$max_W J(W)=tr(W^T S_b W)/tr(W^T S_w W)$$<br>其中，$W∈R^{D×(N-1)}$，$tr(∙)$表示矩阵的迹，$J(W)$对$W$求偏导<br>$$∂J(W)/∂W=(W^T S_w W)2S_b W-(W^T S_b W)2S_w W = 0$$<br>$$S_b W - J(W) S_w W = 0$$<br>上式表明$W$的闭式解是$S_w^{-1} S_b$的$K-1$最大广义个特征值所对应的特征向量组成的矩阵。若将$W$视为一个投影矩阵，则多分类LDA将样本投影到$K-1$维空间，$K-1$通常远小于数据的原有属性数。通过投影可有效减少样本的维数，而且投影过程中使用了类别信息，因此LDA也常视为一种经典的监督降维技术。LDA的算法流程如下：  </p>
<ul>
<li><strong>输入</strong>：数据样本集$D={(x_i,y_i)},i=1,2,…,N$，数据可以分为$K$个类；  </li>
<li><strong>过程</strong>：<br>  计算类内散度矩阵$S_w$和类间散度矩阵$S_b$；<br>  对矩阵$S_w$做奇异值分解，$S_w=UΣV^T, S_w^{-1}=VΣ^{-1} U^T$，$Σ$是一个实对称矩阵；<br>  对矩阵$S_w^{-1} S_b$进行特征值分解；<br>  取最大的d个特征值对应的特征向量；  </li>
<li><strong>输出</strong>：$W=(w_1,w_2,…,w_d)$  </li>
</ul>
<h3 id="等度量映射-Isomap"><a href="#等度量映射-Isomap" class="headerlink" title="等度量映射(Isomap)"></a>等度量映射(Isomap)</h3><p>&emsp;&emsp;等度量映射是一种基于流形学习的数据降维方法，流形是在局部与欧式空间同胚的空间，也就是说在局部具有欧式空间的性质，可以利用欧氏距离进行距离计算。低维流形嵌入到高维空间之后，直接在高维空间中计算直线距离具有误导性，因为高维空间中直线距离在低维嵌入流形上是不可达的。利用流形在局部上与欧式空间同胚的性质，对每个点基于欧式距离找出其近邻点，然后建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接。于是计算两点之间测地线距离的问题，就转变为计算近邻连接图上两点之间的最短路径问题，基于近邻逼近能获得低维流形上测地线很好的近似。<br>&emsp;&emsp;多维缩放(MDS)对数据降维，需要已知高维空间中样本之间的距离关系，等度量映射(Isomap)结合流形学习的思想和多维缩放的方法来进行降维，在高维空间的局部空间，计算邻近点的欧式距离，并构建近邻连接图。然后利用Dijkstra算法或Floyd算法在近邻连接图上计算两点间的最短路径，得到任意两点间的距离之后，即可作为MDS算法的输入距离矩阵，通过MDS算法输出样本在低维空间的投影。Isomap算法描述如下：</p>
<ul>
<li><strong>输入</strong>：数据样本集${x_1,x_2,…,x_N }$，近邻参数$k$，低维空间维数$d$；</li>
<li><strong>过程</strong>：</li>
<li>利用$k$近邻算法计算$x_i$与$k$个近邻样本点的欧式距离，与其他点的距离设为无穷大；  </li>
<li>根据最短路径算法计算任意两个样本点之间的距离，得到距离矩阵$A$；  </li>
<li>将距离矩阵$A$作为MDS算法的输入，得到输出；  </li>
<li><strong>输出</strong>：样本集在低维空间的投影$Z=(z_1,z_2,…,z_N)$，$z_i$为低维空间的向量。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;!-- mathjax config similar to math.stackexchange --&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
    jax: [&quot;input/TeX&quot;, &quot;out
    
    </summary>
    
      <category term="Algorithm" scheme="https://senitco.github.io/categories/Algorithm/"/>
    
    
      <category term="ML" scheme="https://senitco.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Image Analysis and Understand</title>
    <link href="https://senitco.github.io/2017/03/03/image-analysis-understand/"/>
    <id>https://senitco.github.io/2017/03/03/image-analysis-understand/</id>
    <published>2017-03-03T06:45:21.196Z</published>
    <updated>2017-05-10T13:22:58.547Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Participants"><a href="#Participants" class="headerlink" title="Participants"></a>Participants</h2><h2 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h2><h3 id="Chinese-Text-Detection-and-Recognition"><a href="#Chinese-Text-Detection-and-Recognition" class="headerlink" title="Chinese Text Detection and Recognition"></a>Chinese Text Detection and Recognition</h3><h2 id="Deadline"><a href="#Deadline" class="headerlink" title="Deadline"></a>Deadline</h2><h3 id="April-11-Tuesday-2017"><a href="#April-11-Tuesday-2017" class="headerlink" title="April 11, Tuesday, 2017"></a>April 11, Tuesday, 2017</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Participants&quot;&gt;&lt;a href=&quot;#Participants&quot; class=&quot;headerlink&quot; title=&quot;Participants&quot;&gt;&lt;/a&gt;Participants&lt;/h2&gt;&lt;h2 id=&quot;Project&quot;&gt;&lt;a href=&quot;#Projec
    
    </summary>
    
      <category term="Projects" scheme="https://senitco.github.io/categories/Projects/"/>
    
    
      <category term="Image" scheme="https://senitco.github.io/tags/Image/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://senitco.github.io/2017/03/02/hello-world/"/>
    <id>https://senitco.github.io/2017/03/02/hello-world/</id>
    <published>2017-03-02T08:19:37.778Z</published>
    <updated>2017-03-03T12:09:50.569Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
